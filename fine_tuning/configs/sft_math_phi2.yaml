# configs/sft_math_phi2.yaml
inherit: _template_sft.yaml

# — core settings —
backbone: phi2              # 2.7 B params
dataset:  mathmix           

output_dir: "/content/drive/MyDrive/RL_Practice_Files/outputs/phi2_mathmix_lora"


# — LoRA & precision —
quantized: true             # 4-bit QLoRA to fit larger batches
lora_r:     96
lora_alpha: 192
lora_dropout: 0.05

# — training size & speed trade-offs —
batch_size: 16               # per-device batch; safe on A100 in 4-bit
grad_accum: 2               # effective batch = batch_size × grad_accum
epochs:     2              
packing: false 

# — optimizer & logging —
lr:        3e-4              # tuned for LoRA‐SFT on similar models
save_steps: 500
log_steps:  1000
max_seq_len: 512            # Math problems & CoT usually <1 k tokens

lr_scheduler_type: linear
warmup_ratio:    0.05      # ~5 %
