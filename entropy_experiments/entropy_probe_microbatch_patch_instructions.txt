
MICROBATCHED ENTROPY PROBE PATCH — PRECISE IMPLEMENTATION INSTRUCTIONS
=====================================================================

Goal
----
Patch the offline entropy probe to:
1) Compute X̄·Ȳ̄ **without** ever materializing a computation graph spanning all prompts;
2) Keep peak VRAM bounded by the **microbatch** size (as in normal gradient accumulation);
3) Remove any dependency on precomputed logprobs in the block path (compute S with grad when needed).

We will modify `ProbeComponents._compute_delta_h1_exact` and the helpers it calls, and adjust the block path accordingly.


Core concepts (recap)
---------------------
Let prompts be the i.i.d. units (U = B for exact, or U = M blocks for block mode). For each unit u:
- Build scalar probe losses
    L_X(u) = mean_g( (S_u,g − LOO_mean_u,g) * S_u,g )
    L_Y(u) = mean_g( A_u,g / Lmax(u) * S_u,g )
- Precondition with Adam: P^{1/2} = 1 / (sqrt(v̂) + eps).

Target estimator (diagonals excluded):
    U^cross = (U/(U−1)) * (X̄·Ȳ̄) − (1/(U(U−1))) * Σ_u (X̃_u · Ỹ_u),
where X̄ = (1/U) Σ_u X̃_u, and similarly for Ȳ̄.


High-level change list
----------------------
A) **Never** build `L_X_total` or `L_Y_total` by summing over the *entire* batch before calling `backward()` / `autograd.grad`.
B) Introduce two **microbatched** reduction passes:
   - Pass X: accumulate ΣX = Σ_u X̃_u
   - Pass Y: accumulate ΣY = Σ_u Ỹ_u  (or compute bars_dot on-the-fly against ΣX)
C) Add a **diagonal loop** over units to compute Σ_u (X̃_u·Ỹ_u) and the row-means r_u required for variance.
D) Keep ΣX and (optionally) ΣY as **single param-sized buffers** (recommend fp16/bf16); no per-unit vectors are stored.
E) In block mode, treat each microbatch as a block (unit) and use the identical algebra at the block level.
F) Ensure S (logprobs) used in probe passes are computed with **grad enabled** (no inference_mode).


Patch details — exact path
--------------------------

### 0) Preliminaries
- Add two helpers (signatures):
    _iter_prompt_microbatches(self, batch_data, microbatch_size) -> iterator of PromptBatch
    _teacher_force_logprobs(self, prompt_batch) -> dict with S_u,g (requires_grad=True), masks, etc.
- Ensure Adam preconditioner exposes `precond[id(param)]` (1/(sqrt(v̂)+eps)) and is available in the module.
- Ensure `allow_unused=True` for `autograd.grad` and skip `None` grads.

### 1) X-pass (microbatched accumulation of ΣX)
```
# Allocate ΣX once (param-sized), same dtype/device as params; initialize to 0.
X_sum = {id(p): torch.zeros_like(p, memory_format=torch.preserve_format) for p in params}

for prompt_batch in _iter_prompt_microbatches(batch_data, microbatch_size):
    # 1a. Forward with grad to get S for this microbatch (teacher forcing)
    S_dict = _teacher_force_logprobs(prompt_batch)   # S requires grad
    L_X_mb  = _build_probe_loss_X_from_S(S_dict)     # scalar
    
    # 1b. Backward immediately (microbatch)
    zero_grad(params)
    L_X_mb.backward()    # param.grad now holds un-preconditioned Σ_{u in mb} (S - S̄_loo)∇S
    
    # 1c. Apply P^{1/2} and accumulate into ΣX
    for p in params:
        g = p.grad
        if g is None: continue
        g.mul_(precond[id(p)])            # in-place: X̃ contribution
        X_sum[id(p)].add_(g)              # accumulate
    zero_grad(params)                     # free activations of this microbatch
```
Notes:
- No `retain_graph=True`; each microbatch’s graph is freed after its backward.
- Peak memory is that of one microbatch.

### 2) Y-pass (option A: also accumulate ΣY)
```
Y_sum = {id(p): torch.zeros_like(p) for p in params}

for prompt_batch in _iter_prompt_microbatches(batch_data, microbatch_size):
    S_dict  = _teacher_force_logprobs(prompt_batch)  # fresh forward with grad
    L_Y_mb  = _build_probe_loss_Y_from_S(S_dict)     # scalar
    
    y_grads = torch.autograd.grad(L_Y_mb, params, allow_unused=True)
    
    for p, gy in zip(params, y_grads):
        if gy is None: continue
        gy.mul_(precond[id(p)])
        Y_sum[id(p)].add_(gy)
        
# bars_dot = (ΣX · ΣY) / U^2
bars_dot = 0.0
for p in params:
    bars_dot += (X_sum[id(p)] * Y_sum[id(p)]).sum()
bars_dot = bars_dot / (U * U)
```
### 2') Y-pass (option B: no ΣY buffer; compute bars_dot on-the-fly)
- If you want to avoid an entire extra param-sized buffer for ΣY:
```
bars_dot = 0.0
for prompt_batch in _iter_prompt_microbatches(...):
    S_dict = _teacher_force_logprobs(prompt_batch)
    L_Y_mb = _build_probe_loss_Y_from_S(S_dict)
    y_grads = torch.autograd.grad(L_Y_mb, params, allow_unused=True)
    # dot with ΣX directly
    partial = 0.0
    for p, gy in zip(params, y_grads):
        if gy is None: continue
        gy.mul_(precond[id(p)])
        partial += (gy * X_sum[id(p)]).sum()
    bars_dot += partial
bars_dot = bars_dot / (U * U)
```
- This keeps only ΣX in memory.


### 3) Diagonal loop (compute Σ diag and row-means r_u)
We need, for each unit u:
- diag_u = X̃_u · Ỹ_u
- r_u = 0.5 * [ X̃_u · Ȳ̄_{−u} + Ỹ_u · X̄_{−u} ]
  where Ȳ̄_{−u} = (ΣY − Ỹ_u)/(U−1), X̄_{−u} = (ΣX − X̃_u)/(U−1)

Implementation (microbatch == 1 unit for clarity; if you batch a few units together, iterate within the mb):
```
diag_sum = 0.0
r_values = []

for unit in _iter_prompts_as_units(batch_data):
    # X_u
    zero_grad(params)
    S_u = _teacher_force_logprobs(unit)          # grad-enabled
    L_X_u = _build_probe_loss_X_from_S(S_u)
    L_X_u.backward()                             # param.grad == X_u (unpreconditioned); apply P^{1/2}
    
    # Save a *copy* of X_u for the scalar dots we need (or just keep in param.grad temporarily)
    # Y_u
    y_grads_u = torch.autograd.grad(_build_probe_loss_Y_from_S(S_u), params, allow_unused=True)
    
    # Scalars we need:
    dot_Xu_Yu = 0.0
    dot_Xu_SumY = 0.0
    dot_Yu_SumX = 0.0
    
    for p, gy in zip(params, y_grads_u):
        Xu = p.grad
        if Xu is None or gy is None: continue
        Xu.mul_(precond[id(p)])                  # X̃_u
        gy.mul_(precond[id(p)])                  # Ỹ_u
        
        dot_Xu_Yu   += (Xu * gy).sum()
        if HAVE_Y_SUM:                           # Option A above
            dot_Xu_SumY += (Xu * Y_sum[id(p)]).sum()
        dot_Yu_SumX += (gy * X_sum[id(p)]).sum()
    
    diag_sum += dot_Xu_Yu
    if HAVE_Y_SUM:
        XdotYbar_minus = (dot_Xu_SumY - dot_Xu_Yu) / (U - 1)
    else:
        # If you did not store ΣY, you cannot form X_u·Ȳ̄_{−u}. Prefer Option A when you want r_u.
        raise NotImplementedError("Row-means need ΣY; enable Option A.")
    YdotXbar_minus = (dot_Yu_SumX - dot_Xu_Yu) / (U - 1)
    
    r_u = 0.5 * (XdotYbar_minus + YdotXbar_minus)
    r_values.append(r_u.item())
    
    zero_grad(params)                             # free activations
```
- If you choose Option B (no ΣY), **skip** computing plug‑in/jackknife SEs, or run a second temporary Y-sum pass to get ΣY solely for the r_u loop.

### 4) Assemble U^cross and δH₁
```
U_cross = (U / (U - 1)) * bars_dot - (1.0 / (U * (U - 1))) * diag_sum
deltaH1 = - lr * U_cross
```

### 5) Variance estimators from r_values
- Plug‑in:
```
r = torch.tensor(r_values, device=device, dtype=torch.float32)
rbar = r.mean()
zeta1_plugin = (U / (4.0 * (U - 1.0))) * ((r - rbar)**2).sum()
se_plugin    = torch.sqrt(4.0 * zeta1_plugin / U)
```
- Jackknife:
```
U_minus = (U * U_cross - 2.0 * r) / (U - 2.0)
Ubar_minus = U_minus.mean()
var_jack = ((U - 1.0) / U) * ((U_minus - Ubar_minus)**2).sum()
se_jack  = torch.sqrt(var_jack)
zeta1_jack = (U / 4.0) * var_jack
```


Patch details — block path
--------------------------
Treat each **microbatch** as one block (unit). Replace any use of precomputed `logprobs` with **fresh** teacher-forced S under grad.

A) Build ΣX_blocks and ΣY_blocks exactly as in the exact path, but iterating over blocks (microbatches).  
B) Compute diag_blocks = Σ_b (X̃_b · Ỹ_b) by doing two small passes per block.  
C) Assemble
```
bars_dot_blocks = (Σ_b X̃_b · Σ_b Ỹ_b) / M^2
U_cross_blocks  = (M/(M-1)) * bars_dot_blocks - (1/(M*(M-1))) * diag_blocks
deltaH1_blocks  = - lr * U_cross_blocks
```
D) Row-means for blocks (needed for SEs) use ΣX_blocks and ΣY_blocks exactly as above (replace U→M, u→b).


Interface & small utilities to add
----------------------------------
- `_iter_prompt_microbatches(batch_data, microbatch_size)` → yields slices {prompts, responses, masks} of size ≤ microbatch_size prompts.
- `_iter_prompts_as_units(batch_data)` → yields one-prompt “units” for the diagonal loop (or mini-units if you prefer to amortize).
- `_teacher_force_logprobs(unit_or_batch)` → returns per-token/per-sequence logprobs `S` with requires_grad=True (no inference_mode), plus any masks needed.
- `_build_probe_loss_X_from_S(S_dict)` and `_build_probe_loss_Y_from_S(S_dict)` → construct the scalar L_X and L_Y for the provided unit/batch (per the definitions above).


Memory knobs & recommendations
------------------------------
- Keep ΣX (and ΣY if you want SEs) in **bf16/fp16** to halve memory—safe because both are only used inside dot-products.
- If ΣY is too heavy, use Option B for bars_dot and **skip SEs** (or run a one-off ΣY pass only on steps where you log SEs).
- Always `zero_grad(params)` between microbatches to free activations immediately.
- Do not use `retain_graph=True` in these passes; it defeats the memory savings.
- Ensure probe forwards use the same autocast dtype as training to avoid cast explosions.


Distributed notes (unchanged)
-----------------------------
- After local ΣX, ΣY, and diag are computed, **all-reduce** the scalars needed to assemble global `bars_dot` and `diag_sum` (or all-reduce ΣX/ΣY if you elect to keep them as vectors, then compute dots locally). Prefer scalar reductions; avoid vector all-gathers.


Unit test you should add
------------------------
- With a small LM and synthetic data, run both the old “global loss then backward” and the new microbatched pass at modest B; verify numeric equality of `bars_dot` within tolerance and measure a ~linear drop in peak memory with microbatch size.


Drop-in locations
-----------------
- Replace the body of `ProbeComponents._compute_delta_h1_exact` with the three-phase microbatched logic (X-sum, Y-sum, diagonal + r_u), and remove `_build_total_probe_loss` (or keep it only for unit/microbatch-sized chunks).
- In the block path, remove any consumption of `batch_data['logprobs']`; call `_teacher_force_logprobs` to obtain S for each block with grad. Assemble `U_cross_blocks` from bars/diagonals as shown.

Once these changes are in, the probe will have the same peak VRAM behavior as ordinary gradient accumulation while producing the exact (diagonals-excluded) estimator and its variance from a single batch.
