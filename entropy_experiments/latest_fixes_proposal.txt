TITLE: Importance Sampling (IS/SNIS) Path — Precise Changes and Rationale

SCOPE
=====
This note specifies concrete modifications to `importance_sampling.py` (and minimal call-site updates)
so that the entropy-change measurement is correct, efficient, and DDP-safe.

It covers four required changes:
  (1) Always pass attention masks when computing teacher-forced log-probs
  (2) Avoid redundant recomputation of S_orig (log πθ) by threading a cached tensor
  (3) Snapshot & restore the *optimizer state* as well as parameters
  (4) All-reduce the “original entropy” across ranks in DDP

It also includes optional improvements and a verification checklist.


----------------------------------------------------------------------
A. ALWAYS PASS ATTENTION MASKS FOR TEACHER-FORCED LOG-PROBS
----------------------------------------------------------------------

Problem
-------
In the TF inference helper you call the model with only input_ids; left-padding then
allows attention to pads, and logits for early tokens are wrong under left-padding.

Change
------
In `_compute_logprobs_for_sequences(...)`, ensure the forward includes `attention_mask`.

Before:
    logits = self.model(micro_seqs).logits

After:
    logits = self.model(micro_seqs, attention_mask=micro_masks).logits

Notes
-----
- Keep the rest intact (slice logits to align with labels; mask pads; sum over tokens).
- This makes sequence log-probs consistent with the masked sum used elsewhere.


----------------------------------------------------------------------
B. AVOID RECOMPUTING S_orig — THREAD A CACHED LOGPROB TENSOR
----------------------------------------------------------------------

Problem
-------
`compute_entropy_change(...)` computes `logprobs_original` (S_orig) and later
`_compute_updated_logprobs(...)` recomputes S_orig again. This doubles inference.

Change 1 (caller)
-----------------
In `compute_entropy_change(...)` (or the two-batch analog), compute S_orig once and pass it:

    S_orig = self._compute_logprobs_for_sequences(sequences, prompt_lens, attention_masks)
    S_upd  = self._compute_updated_logprobs(batch_data, optimizer, cached_S_orig=S_orig)

Change 2 (callee)
-----------------
Edit `_compute_updated_logprobs(..., cached_S_orig=None)`:

    def _compute_updated_logprobs(self, batch_data, optimizer, cached_S_orig=None, ...):
        sequences = batch_data['sequences']
        prompt_lens = batch_data['prompt_lens']
        attention_masks = batch_data['attention_masks']

        # 1) If not provided, compute S_orig once here
        if cached_S_orig is None:
            S_orig = self._compute_logprobs_for_sequences(sequences, prompt_lens, attention_masks)
        else:
            S_orig = cached_S_orig

        # 2) Take the optimizer step (see Section C for snapshot/restore)
        # 3) Compute S_upd under θ⁺ (same helper)
        S_upd = self._compute_logprobs_for_sequences(sequences, prompt_lens, attention_masks)

        # 4) Return both S_upd and (optionally) S_orig for callers that want both
        return S_upd, S_orig

Notes
-----
- The two-batch path should pass E-batch tensors as `cached_S_orig` to avoid rework there as well.
- Keep the API backward-compatible by allowing `cached_S_orig=None`.


----------------------------------------------------------------------
C. SNAPSHOT & RESTORE THE OPTIMIZER STATE (NOT JUST PARAMETERS)
----------------------------------------------------------------------

Problem
-------
Currently, parameters are saved/restored but Adam moments (state_dict) are not. If this
probe runs “online,” the optimizer would silently advance its moments by one step.

Change
------
Right before the update, snapshot both model parameters (you already do)
and the optimizer state_dict. After computing S_upd and any IS metrics, restore both.

Sketch:

    # --- snapshot params (existing) ---
    param_snapshots = {id(p): p.detach().cpu().clone() for p in model.parameters()}

    # --- snapshot optimizer state ---
    opt_state_cpu = optimizer.state_dict()              # shallow copy of Python objects
    # deep-copy tensors to CPU to be safe
    for group in opt_state_cpu.get('state', {}).values():
        for k, v in group.items():
            if torch.is_tensor(v):
                group[k] = v.detach().cpu().clone()

    # --- accumulate grads & step ---
    optimizer.zero_grad(set_to_none=True)
    # (your microbatched backward over U; scale/clip as usual)
    optimizer.step()

    # --- compute S_upd (θ⁺) on E or U as configured ---
    S_upd = self._compute_logprobs_for_sequences(...)

    # --- restore params ---
    with torch.no_grad():
        for p in model.parameters():
            p.copy_(param_snapshots[id(p)].to(p.device))

    # --- restore optimizer ---
    optimizer.load_state_dict(opt_state_cpu)
    # move any tensors in optimizer state back to correct device
    for state in optimizer.state.values():
        for k, v in state.items():
            if torch.is_tensor(v):
                state[k] = v.to(next(model.parameters()).device)
    optimizer.zero_grad(set_to_none=True)

Notes
-----
- If running under DDP/FSDP, ensure the state restore occurs on all ranks and
  that parameter shapes and device placements are consistent.
- This keeps training state pristine after the probe path.


----------------------------------------------------------------------
D. ALL-REDUCE THE “ORIGINAL ENTROPY” ACROSS RANKS
----------------------------------------------------------------------

Problem
-------
You compute `original_entropy` as the local mean of −S_orig. In multi-GPU,
this under/over-estimates unless combined across ranks.

Change
------
Replace local mean by global mean:

    # local tensors
    neg_sum_local = (-S_orig).sum()
    cnt_local = torch.tensor(S_orig.numel(), device=self.device, dtype=neg_sum_local.dtype)

    if dist.is_initialized():
        # sum across ranks
        dist.all_reduce(neg_sum_local, op=dist.ReduceOp.SUM)
        dist.all_reduce(cnt_local,    op=dist.ReduceOp.SUM)

    original_entropy = (neg_sum_local / cnt_local).item()

Notes
-----
- Mirror the same pattern for any other reported global means you need (e.g., per-token variants).
- SNIS accumulators already use global reductions; keep those as they are.


----------------------------------------------------------------------
E. OPTIONAL IMPROVEMENTS (RECOMMENDED)
----------------------------------------------------------------------

E1) Log-stable SNIS and optional clipping
    - You already compute log-weights and stabilize weights by subtracting max(log w).
      Keep that; optionally add configurable clipping on *log*-weights:
          log_w = torch.clamp(log_w, max=log_w_max)  # prevents extreme ratios
      or on weights after exponentiation:
          w = torch.clamp(w, max=clip_c)
      Make this a config flag (`is_mode ∈ {"snis", "clip"}`, `clip_c`).

E2) Two-batch evaluation (preferred “truth”)
    - When measuring ΔH_true, prefer:
        • Take one update on U (θ→θ⁺),
        • Evaluate H(θ;E) and H(θ⁺;E) using SNIS on E,
        • ΔH_true = Ĥ(θ⁺;E) − Ĥ(θ;E).
      Keep θ and optimizer states restored afterwards (Section C).

E3) Per-token vs per-sequence entropy
    - If you report per-token entropy, weight −log π by token masks and divide by
      (weighted) token counts. Do the same normalization for both θ and θ⁺.

E4) Microbatching and AMP
    - Reuse your microbatch iterator in `_compute_logprobs_for_sequences` to bound memory.
    - Use the same autocast dtype as training; keep reductions in fp32.

E5) Determinism / seeds
    - If you compare δH₁ and ΔH_true repeatedly, seed the sampling for U and E for reproducibility.


----------------------------------------------------------------------
F. VERIFICATION CHECKLIST
----------------------------------------------------------------------

1) Single GPU
   - Compare original entropy computed as:
       (a) local mean and (b) the same code path with all_reduce disabled.
     They should match (trivial here).
   - Confirm S_upd equals recomputed S_upd if you call `_compute_updated_logprobs(...)`
     twice in a row (ensures snapshot/restore is correct).

2) Multi-GPU
   - Verify original-entropy now matches the explicit gather-then-mean baseline.
   - Verify SNIS sums (Σw, Σw², Σw f) are identical across ranks (they should be by design).
   - Run with different local batch partitions; global results must be invariant.

3) Efficiency
   - Profile calls before/after adding `cached_S_orig`: wall time for IS path should drop ~×2.
   - Ensure no increase in peak VRAM (microbatching intact; no retain_graph).

4) Sanity on signs
   - If you negate advantages for the update step, ΔH_true should flip sign on average.
   - If you set advantages to zero, ΔH_true ≈ 0 (within noise), and IS weights center at 1.

5) No training-state drift
   - Log a hash or checksum of parameter and optimizer state before/after the IS routine.
     They must be identical (bitwise) after restore.


----------------------------------------------------------------------
G. SUMMARY OF EDIT LOCATIONS
----------------------------------------------------------------------

- importance_sampling.py
  • `_compute_logprobs_for_sequences`:
      - Add `attention_mask=micro_masks` to the model forward.
      - Ensure masking in the log-prob sum matches the attention mask.

  • `compute_entropy_change` (and two-batch analog):
      - Compute `S_orig` once, pass `cached_S_orig` to `_compute_updated_logprobs`.
      - Replace local mean of −S_orig by global mean via all-reduce.

  • `_compute_updated_logprobs`:
      - Add arg `cached_S_orig=None` and use it if provided.
      - Snapshot & restore *optimizer state* as well as parameters (Section C).
      - Return `(S_upd, S_orig)` if upstream code benefits; else minimally `S_upd`.

  • Any helper that reports entropies:
      - Ensure DDP all-reduce is used for global means.

- probe_components.py (call sites)
  • If you call the IS routine from here, thread `cached_S_orig` where available
    and adopt the two-batch protocol (E for evaluation, U for update).

This completes the required changes. The code-path remains microbatched and DDP-safe,
redundant compute is removed, and the training state is preserved exactly.
