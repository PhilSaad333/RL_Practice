
OFFLINE ENTROPY PROBE — FIX LIST & IMPLEMENTATION GUIDE (for teammate)
======================================================================
Context
-------
We are implementing an offline “entropy probe” that, given a checkpoint {θ, Adam state}, will:
  1) Estimate the first-order entropy change δH₁ = −η · U_cross (order-2 cross‑prompt U‑statistic).
  2) Estimate the variance/standard error of U_cross using BOTH: (i) plug‑in ζ̂₁ via row‑means; (ii) delete‑1 jackknife.
  3) Compute the realized ΔH on the SAME batch after an actual optimizer step (using SNIS on common samples).

This guide explains exactly what to fix in the current draft and how to implement the correct behavior. It is self-contained — you can code from it directly.


Top-level problems to fix (summary)
-----------------------------------
P0. Probe losses are built from DETACHED log-probs S, so autograd cannot produce gradients (grads come back None).
P1. The cross‑prompt U‑statistic U_cross is incomplete — the (B/(B−1))·(X̄·Ȳ̄) term is missing; only the diagonal term Σ(X̃·Ỹ) is used.
P2. The variance estimators are placeholders; need real plug‑in ζ̂₁ via row‑means and the proper jackknife formula.
P3. Adam preconditioner reads v-state only when p.grad is not None; that silently disables preconditioning at probe time.
P4. Importance sampling path is a stub; ΔH must be computed on the same sequences after a real (or virtual) step to θ⁺.
P5. Tests use a 1.5B model; impractical. Use a small LM so the probe can run on CPU/GPU quickly.

Below are exact changes, module by module, including pseudocode.


Module-by-module fixes and how to implement them
------------------------------------------------

1) probe_components.py  (core of the probe)
-------------------------------------------
Goal: Build differentiable scalar probe losses L_X and L_Y from *fresh* teacher-forced log-probs S (with grad enabled), then compute
  U_cross = (B/(B−1))·(X̄̃·Ȳ̄̃) − (1/(B(B−1)))·Σ_n (X̃_n·Ỹ_n),
and the row-means r_n for the variance estimators. Support two modes: EXACT (per-prompt) and BLOCK (RAM-light).

A. Never use S computed under torch.inference_mode() for the probe passes.
   • In sample_batch(), return tokenized prompt+response sequences (ids, masks). Do NOT compute S there.
   • In the probe passes, re-run the model with grad enabled and teacher forcing to get S for those sequences.

B. Build scalar probe losses (per our derivation):
   For a unit u (prompt n or block b) with G responses:
     L_X(u) = mean_g ( (S_u,g − loo_mean(S_u,· excluding g)) * S_u,g )
     L_Y(u) = mean_g ( A_u,g / Lmax(u) * S_u,g )
   Notes:
     • L_X and L_Y are scalars. Do not call .item() before computing grads.
     • Use per-prompt LOO mean S̄_{u,−g} to avoid the 1/G shrinkage bias.

C. EXACT mode (per-prompt; use less frequently due to cost):
   # Global bars (two passes, microbatch-friendly)
   1) Build L_X_total = Σ_n L_X(n).    zero_grad();  L_X_total.backward()
      → param.grad holds Σ_n (S−S̄_loo)∇S.  Apply P^{1/2} in-place → this is Σ_n X̃_n.
   2) Build L_Y_total = Σ_n L_Y(n).    y_grads = autograd.grad(L_Y_total, params)
      Apply P^{1/2} to y_grads → this is Σ_n Ỹ_n.
      Compute bars_dot = (Σ_n X̃_n) · (Σ_n Ỹ_n) / B^2   # equals X̄̃·Ȳ̄̃.
   # Diagonal term (loop prompts; small passes)
   3) For each prompt n:
        zero_grad(); L_X(n).backward(retain_graph=False)  # → param.grad == X̃_n (after P^{1/2})
        y_grads = autograd.grad(L_Y(n), params)
        diag_sum += Σ_p (X̃_n[p] * Ỹ_n[p])     # apply P^{1/2} to y_grads before dot
   4) U_cross = (B/(B−1)) * bars_dot − (1/(B*(B−1))) * diag_sum
      δH₁ = −η * U_cross

   ALSO collect row-means r_n for variance:
     r_n = 0.5 * ( X̃_n · (Ȳ̄_{−n}) + Ỹ_n · (X̄̃_{−n}) )
   Using global sums, Ȳ̄_{−n} = (Σ_m Ỹ_m − Ỹ_n) / (B−1) etc.
   You already computed Σ X̃ and Σ Ỹ, so compute r_n “on the fly” during the diagonal loop.

D. BLOCK mode (RAM-light; default online/offline):
   Partition global batch into M blocks (microbatches are natural blocks). Treat each block b as a “super-prompt” unit.
   Repeat EXACT logic at the block level:
     # Bars over blocks
     L_X_blocks_total = Σ_b L_X(b); backward → Σ_b X̃_b
     L_Y_blocks_total = Σ_b L_Y(b); autograd.grad → Σ_b Ỹ_b
     bars_dot_blocks = (Σ_b X̃_b · Σ_b Ỹ_b) / M^2
     # Diagonals over blocks
     diag_blocks = Σ_b (X̃_b · Ỹ_b)   # two small passes per block
     U_cross_blocks = (M/(M−1)) * bars_dot_blocks − (1/(M(M−1))) * diag_blocks
     δH₁ ≈ −η * U_cross_blocks
   Row-means for blocks:
     r_b = 0.5 * ( X̃_b · Ȳ̄_{−b} + Ỹ_b · X̄̃_{−b} ), with leave-one-out bars via global block sums.

Implementation details:
   • Teacher forcing helper must run under model.train() | autocast(as in training), NOT inference_mode.
   • For speed, compute S once per unit per pass (you can microbatch through the unit’s sequences).
   • Apply P^{1/2} consistently to both X and Y sides before any dot products.
   • ZERO grads between passes; do not call optimizer.step() during probe.
   • Set allow_unused=True in autograd.grad and skip None tensors (unused params).

E. Distributed (multi-GPU):
   • All-reduce only scalar sums:
       - global Σ X̃ (as a set of parameter-wise sums collapsed into one “bar dot” via running reductions),
       - global Σ Ỹ,
       - global diag_sum (sum of X̃·Ỹ over units),
       - counts (B or M).
     With these, every rank can compute bars_dot and r_u locally (leave-one-out via global sums).
   • Do NOT all-gather parameter-length vectors.
   • If some params are unused by probe losses: enable find_unused_parameters=True or handle static graph carefully.

F. Autograd gotchas to avoid “None” grads:
   • Probe losses MUST be scalars; reduce over tokens/sequences.
   • Do not wrap probe forwards in torch.inference_mode(); that disables needed graph tracking.
   • If you ever need two grads from one forward, the first backward must use retain_graph=True; otherwise, prefer two forwards to reduce RAM.
   • Use allow_unused=True; skip None grads.

G. Output from this module for variance:
   • U_cross (or U_cross_blocks), r_u for each unit u (prompt or block), B (or M), and diag/bars bookkeeping to allow SEs.


2) u_statistics.py  (variance estimators)
-----------------------------------------
Implement the two estimators using only {r_u} and U_cross.

Given units u = 1..U (U=B in EXACT, or U=M in BLOCK):
   r̄ = (1/U) Σ_u r_u

A) Plug‑in ζ̂₁ and SE:
   zeta1_plugin = (U / (4*(U−1))) * Σ_u (r_u − r̄)^2
   se_plugin    = sqrt( 4 * zeta1_plugin / U )

B) Jackknife (delete‑1):
   U_minus_u  = (U*U_cross − 2*r_u) / (U−2)              # exact for order‑2 U‑statistic
   Ubar_minus = (1/U) Σ_u U_minus_u
   var_jack   = ((U−1)/U) * Σ_u (U_minus_u − Ubar_minus)^2
   se_jack    = sqrt(var_jack)
You may also expose zeta1_jack = (U/4) * var_jack for logging.

Return both SEs and ζ̂₁ values.


3) adam_preconditioner.py  (match optimizer geometry)
-----------------------------------------------------
A) Extract v-state (exp_avg_sq) for ALL parameters, not gated on p.grad:
   for group in optimizer.param_groups:
       for p in group['params']:
           state = optimizer.state.get(p, {})
           v = state.get('exp_avg_sq', None)
           step = state.get('step', 0)
           cache[id(p)] = (v, step)

B) Bias correction (if step>0):
   v_hat = v / (1 − beta2**step)
   precond(p) = 1 / (sqrt(v_hat) + eps)
Apply this elementwise to gradients from L_X and to y_grads from L_Y BEFORE forming any dot-products.

C) Map by param object id, not by (group_idx, index), to avoid ordering mismatches.

D) Provide a validator that does a test dot-product with and without preconditioner and checks for NaNs/Infs; raise if any v is missing.


4) importance_sampling.py  (realized ΔH on same batch)
------------------------------------------------------
Implement ΔH using the SAME sequences via SNIS after a real/virtual optimizer step.

A) Make a copy of the model and optimizer state, or do a “virtual step”:
   • Option 1 (clone+step): clone model θ→θ⁺, load optimizer state, run one real optimizer.step() on the training loss using the SAME batch.
   • Option 2 (virtual step): compute Δθ from saved Adam moments and LR, then with torch.no_grad(): for each param p, p.add_(Δθ_p). Revert after probe.

B) Recompute log-probs S⁺ for the cached prompt+response sequences under θ⁺ by teacher forcing (identical tokenization & masks).

C) Compute SNIS weights: w_i = exp(S⁺_i − S_i); ŵ_i = w_i / Σ_j w_j; ESS = (Σ w)^2 / Σ w^2.

D) Entropy estimates:
   Ĥ(θ)   = −(1/N) Σ_i S_i
   Ĥ(θ⁺)  = −Σ_i (ŵ_i * S⁺_i)
   ΔĤ_SNI S = Ĥ(θ⁺) − Ĥ(θ)
   If ESS/(B·G) < threshold (e.g., 0.3–0.5), either: (i) resample at θ⁺ and recompute Ĥ(θ⁺) directly, or (ii) apply PSIS to stabilize weights (optional).

Return ΔĤ_SNI S and ESS (and PSIS diagnostics if implemented).


5) distributed_helpers.py  (collectives layout)
-----------------------------------------------
Provide helpers to all-reduce the minimal set of scalars:
   • counts: U (B or M)
   • bars: sums needed to compute X̄̃·Ȳ̄̃ (do this via two global passes and scalar reductions — avoid storing vectors)
   • diagonal sum: Σ (X̃_u·Ỹ_u)
   • row-means: you do NOT need to all-reduce per‑unit r_u; compute r_u locally using global Σ X̃ and Σ Ỹ, then all-reduce only Σ r_u and Σ r_u² to form plug‑in ζ̂₁ (or all-reduce jackknife aggregates).

No parameter-length all-gathers. Ensure dtype consistency with AMP settings.


6) offline_entropy_probe.py  (orchestrator)
-------------------------------------------
A) Implement checkpoint loading for BOTH model and optimizer; ensure Adam state is present (exp_avg_sq, step).

B) Wire the two probe modes:
   • EXACT mode for validation/sweeps.
   • BLOCK mode as default (RAM-light).

C) Call importance_sampling to compute ΔH on the SAME batch after applying a real/virtual step.

D) Log: U_cross, δH₁, SE_plugin, SE_jack, ΔĤ_SNI S, ESS, timing, config.


7) test_entropy_probe.py  (practical tests)
-------------------------------------------
A) Replace the large model with a genuinely small causal LM (e.g., GPT-2 small-ish or a toy Transformer). The test should run on CPU or a single small GPU.

B) Seed everything for determinism.

C) Unit tests:
   • autograd returns non-None for at least some params in both X and Y passes.
   • U_cross != 0 on a synthetic batch; signs are consistent across runs with small variance.
   • plug‑in SE and jackknife SE are within a factor of ~2–3 on small U.
   • importance_sampling returns ΔĤ_SNI S with reasonable ESS (> 0.3 for small steps).


Implementation snippets (copy/paste starting points)
----------------------------------------------------

Scalar probe losses (per unit u)
--------------------------------
# teacher-force to get S_u,g (grad-enabled); A_u,g and Lmax(u) are tensors (no .item())
L_X_u = ((S_u - loo_mean_per_prompt(S_u)) * S_u).mean()
L_Y_u = ((A_u / Lmax_u) * S_u).mean()

Global bars (two passes)
------------------------
zero_grad()
L_X_total.backward()                         # fills param.grad with Σ X (unpreconditioned)
for p in params: p.grad.mul_(precond[p_id])  # apply P^{1/2}, in-place
X_sum = {p_id: p.grad.clone()}               # or reduce immediately to a scalar via dot with Y_sum later

y_grads = autograd.grad(L_Y_total, params, allow_unused=True)
for p, gy in zip(params, y_grads):
    if gy is None: continue
    gy.mul_(precond[id(p)])                  # apply P^{1/2}
# bars_dot = (Σ X)·(Σ Y) / U^2  (do not materialize vectors if RAM-bound; reduce to a scalar on the fly)

Diagonal (per unit)
-------------------
zero_grad(); L_X_u.backward()
y_grads_u = autograd.grad(L_Y_u, params, allow_unused=True)
diag_u = 0.0
for p, gy in zip(params, y_grads_u):
    if gy is None or p.grad is None: continue
    gy.mul_(precond[id(p)])
    diag_u += (p.grad * gy).sum().item()
diag_sum += diag_u

Row means (per unit) using global sums (X_sum_tot, Y_sum_tot)
-------------------------------------------------------------
# During the diagonal loop you have p.grad == X_u; also compute Y_u via autograd.grad above
# Maintain scalar dots:
XdotYbar_minus_u = dot(X_u, (Y_sum_tot - Y_u)) / (U - 1)
YdotXbar_minus_u = dot(Y_u, (X_sum_tot - X_u)) / (U - 1)
r_u = 0.5 * (XdotYbar_minus_u + YdotXbar_minus_u)

U_cross assembly
----------------
bars_dot = dot(X_sum_tot, Y_sum_tot) / (U*U)
U_cross  = (U/(U-1)) * bars_dot - (1/(U*(U-1))) * diag_sum
deltaH1  = -lr * U_cross


Checklist before you commit
---------------------------
[ ] Probe passes recompute S with grad enabled (no inference_mode); L_X, L_Y are scalars.
[ ] Full U_cross uses both bars_dot and diag_sum (diagonals excluded).
[ ] Row-means r_u computed; plug‑in ζ̂₁ and jackknife SE implemented exactly.
[ ] Adam preconditioner: v-state extracted for all params; bias-corrected; mapped by param id.
[ ] Importance sampling: θ→θ⁺ done on a copy/virtual step; S⁺ recomputed on SAME sequences; SNIS/ESS computed.
[ ] Distributed helpers: only scalar all-reduces; no vector all-gathers.
[ ] Tests: small model; assertions for non-None grads; SEs reasonable; ESS reported.

That’s it — implementing the above changes will make the offline probe numerically correct, memory-safe, and faithful to the derivations.
