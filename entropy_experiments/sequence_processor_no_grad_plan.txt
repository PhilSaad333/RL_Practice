
SequenceProcessor No‑Grad Precision Plan (Point A)
==================================================

Goal
----
Make the *no‑grad* path in `sequence_processor.py` numerically stable and strictly comparable to the proven functional‑call probe in `param_override_test_2.py`. This will ensure that entropy estimated on a fixed E‑batch varies continuously (and linearly for sufficiently small η) when we evaluate the model at parameters θ + η·v using a LoRA model.

Scope for this iteration
------------------------
Only the *no‑grad* path is in scope. We will:
  • unify the forward path for both baseline and overridden parameters,
  • enforce deterministic, non‑autocast precision,
  • construct params‑only mappings in FP32/FP64 before adding η·v,
  • ensure module keyspace consistency under PEFT/LoRA,
  • add η=0 zero‑checks and tiny/main scaling checks (optional debug),
  • make RB/entropy computation use identical precision on both paths.

We will *not* touch the grad path or update‑vector code in this step, except to thread through existing precision knobs.


A. Interfaces & invariants
--------------------------
1) Add a one‑time binding of the *target module identity* used for both mapping and forward:
   - `self._mdl_target = unwrap(self.model)` where `unwrap` mirrors the helper used by the working probe.
   - Never mix `self.model` and `self._mdl_target` when building mappings vs. calling `functional_call`.

2) Unify forward contexts:
   - Always call `eval()` during these probes.
   - Always set `use_cache=False` for forward.
   - Always disable autocast (AMP) for both baseline and overridden forwards.
   - Never touch buffers in the default path (params‑only mapping).

3) Precision policy for parameter arithmetic:
   - Upcast base params and the update vector to *the same compute dtype* (FP32 or FP64) **before** adding `η·v`.
   - This is controlled by the existing precision profile: `precision.func_override.cast_params` and `precision.func_override.dtype`.
   - Disable TF32 in this probe unless explicitly configured (prefer `allow_tf32: false` for determinism).

4) Determinism toggles (probe‑only):
   - `model.eval()` ensures dropout etc. are off.
   - Optionally set `torch.use_deterministic_algorithms(True)` behind a config flag (can be slower).
   - Avoid any stochastic layers in the path; ensure tokenization/masks are fixed.

5) LoRA/PEFT keyspace consistency:
   - Build the mapping against `self._mdl_target` (unwrapped) and pass that *same* object to `functional_call`.
   - Ensure the update vector `v_named` keys match `self._mdl_target.named_parameters()` exactly.
   - Do not merge LoRA into base; use adapters in FP32/FP64 as loaded.


B. Concrete additions to sequence_processor.py
----------------------------------------------
Add the following *private* utilities and refactor callers to use them. Pseudocode below is deliberately minimal; adapt names to the file’s conventions.

B1) Module binding and precision profile
----------------------------------------
At initialization (or `_ensure_initialized()`), bind the target module and read precision knobs.

    from entropy_experiments.utils.precision_utils import str_to_dtype, apply_global_precision, maybe_cast_logits_fp32
    from entropy_experiments.utils.param_overrides import build_functional_params_named
    from entropy_experiments.utils.param_overrides import _unwrap_module as _unwrap  # if exported; else define a local unwrap

    class SequenceProcessor:
        def __init__(self, model, config, logger):
            self.model = model
            self.config = config
            self.logger = logger
            # One canonical module for mappings *and* forward
            self._mdl_target = _unwrap(model) if callable(_unwrap) else model

            # Precision / determinism profiles
            prec_cfg = (config or {}).get("precision", {})
            self._fo_cfg = prec_cfg.get("func_override", {})   # {"autocast": False, "cast_params": True, "dtype": "float32"}
            self._tf_cfg = prec_cfg.get("tf_nograd", {})       # logits/logprobs precision for TF no‑grad
            self._allow_tf32 = bool(prec_cfg.get("allow_tf32", False))
            self._matmul_precision = prec_cfg.get("matmul_precision", "high")  # or "highest" for FP64 probes
            apply_global_precision(self._allow_tf32, self._matmul_precision)

            # Optional determinism (probe‑only)
            det = bool(prec_cfg.get("deterministic_probe", False))
            if det:
                try: torch.use_deterministic_algorithms(True)
                except Exception: pass

            # Cache for η=0 mapping (avoids rebuilding per call)
            self._params_zero = None

B2) Unified helper: params‑only mapping (FP32/FP64) and forward
---------------------------------------------------------------
    import contextlib
    import torch

    def _build_params_override(self, v_named: dict|None, eta: float) -> dict[str, torch.Tensor]:
        """
        Build a *parameters‑only* mapping for self._mdl_target with upcast before base + η·v.
        Buffers are detached and *not* overridden. This mirrors the working probe.
        """
        cast_params = bool(self._fo_cfg.get("cast_params", True))
        force_dtype = str_to_dtype(self._fo_cfg.get("dtype", "float32")) if cast_params else None

        params_dict, _ = build_functional_params_named(
            self._mdl_target, v_named, eta,
            detach_params=True,          # do not backprop through base params
            detach_buffers=True,         # snapshot buffers but do not override in default path
            force_param_dtype=force_dtype,
            force_buffer_dtype=None
        )
        return params_dict

    @torch.no_grad()
    def _fc_logits_noautocast(self, input_ids, attention_mask, params_mapping: dict[str, torch.Tensor]):
        """
        Run a forward on self._mdl_target using torch.func.functional_call with autocast disabled,
        eval mode, and use_cache=False. Returns logits tensor in compute precision.
        """
        m = self._mdl_target
        was_training = m.training
        m.eval()
        try:
            with torch.autocast(device_type="cuda", enabled=False):
                out = torch.func.functional_call(
                    m, params_mapping, (input_ids,),
                    {"attention_mask": attention_mask, "use_cache": False}
                )
            logits = out.logits if hasattr(out, "logits") else out[0]
            return logits
        finally:
            if was_training:
                m.train()

B3) Public no‑grad TF API that both baseline and override use
-------------------------------------------------------------
Refactor the existing teacher‑forced no‑grad path to call the *same* helper for both θ and θ+η·v.

    @torch.no_grad()
    def teacher_force_logprobs_nograd(
        self,
        batch,                    # tokenized E‑batch: dict with input_ids [B,G,L], attention_mask [B,G,L], labels or prefix lengths
        params_override=None,     # optional params mapping dict (params‑only); if None, use η=0 mapping
        return_logits=False,
    ) -> dict:
        """
        Compute teacher‑forced log‑probs and RB payloads in a single precision‑stable path.
        - Builds (or reuses) a params‑only mapping keyed to self._mdl_target.
        - For η=0, we still go through the *same* functional_call path to ensure exact comparability.
        - Cast logits to FP32 before log‑softmax for numerical stability.
        """
        input_ids = batch["input_ids"]          # [B,G,L]
        attention_mask = batch["attention_mask"]# [B,G,L]
        # Collapse (B,G) to a batch dimension if current code expects [N,L]
        B, G, L = input_ids.shape
        ids   = input_ids.view(B*G, L)
        masks = attention_mask.view(B*G, L)

        # Build or reuse η=0 mapping
        mapping = params_override
        if mapping is None:
            if self._params_zero is None:
                self._params_zero = self._build_params_override(v_named=None, eta=0.0)
            mapping = self._params_zero

        logits = self._fc_logits_noautocast(ids, masks, mapping)
        logits_fp32 = maybe_cast_logits_fp32(logits)  # avoids bf16 rounding in log‑softmax
        logprobs = torch.nn.functional.log_softmax(logits_fp32, dim=-1)

        # Compute per‑sequence S, and RB payloads exactly as in your current code
        # (Implement calls to existing helpers that sum over label positions and form RB statistics.)
        # Example placeholders (replace with project‑local functions):
        #   S = sum_logprobs_over_teacher_forcing(logprobs, labels_or_mask, ...)
        #   RB = rb_payload_from_logits_or_logprobs(...)

        out = {
            "logprobs": logprobs,
            # "S": S,
            # "RB": RB,
        }
        if return_logits:
            out["logits"] = logits_fp32
        return out


C. Debug/validation hooks (no‑grad)
-----------------------------------
C1) η=0 zero‑check
    Add a small method to assert that the live forward and `functional_call` with η=0 match bitwise (within a tight tolerance) on a *single (b,g)* item. This should mirror the check in `param_override_test.py`.

    @torch.no_grad()
    def debug_zero_check(self, ids, mask, atol=1e-7):
        # direct forward (same precision context)
        with torch.autocast(device_type="cuda", enabled=False):
            out_dir = self._mdl_target(ids, attention_mask=mask, use_cache=False)
        logits_dir = out_dir.logits if hasattr(out_dir, "logits") else out_dir[0]

        # functional_call with η=0 mapping
        m0 = self._params_zero or self._build_params_override(v_named=None, eta=0.0)
        logits_fc = self._fc_logits_noautocast(ids, mask, m0)

        delta = (logits_dir - logits_fc).abs().max().item()
        self.logger.info(f"[SP-η=0] max|direct - fc(η=0)| = {delta:.3e}")
        if delta > atol:
            raise RuntimeError("Baseline vs functional_call(η=0) mismatch; check precision or module identity.")

C2) Tiny‑η scaling check (optional)
    Provide a helper that takes a `v_named` update and two η values (η_main, η_tiny), builds mappings via `_build_params_override`, runs `_fc_logits_noautocast`, and prints:
      • ||Δθ||₂(tiny)/||Δθ||₂(main)  and  ||Δlogits||₂(tiny)/||Δlogits||₂(main)
    to confirm proportionality. This mirrors the printed diagnostics in your working script.


D. Call‑site refactor (no‑grad only)
------------------------------------
Replace any existing baseline (θ) teacher‑forced evaluation that directly calls `self._mdl_target(...)` with:
   (i) build or reuse `self._params_zero` via `_build_params_override(None, 0.0)`
  (ii) call `_fc_logits_noautocast(..., params_mapping=self._params_zero)`
  (iii) feed logits into the same downstream TF/RB code path

Similarly, any overridden‑parameter evaluation should:
   (i) build mapping via `_build_params_override(v_named, eta)`
  (ii) call `_fc_logits_noautocast(..., params_mapping=that_mapping)`
  (iii) feed logits into the same downstream TF/RB code path

This guarantees that both baselines and overrides share *identical* execution contexts.


E. Configuration keys (read but do not invent new ones)
-------------------------------------------------------
Assume the following entries already exist (per your tests); use them rather than introducing new names. Provide sane defaults if missing.

precision:
  allow_tf32: false              # prefer strict FP32/FP64 math
  matmul_precision: "high"       # or "highest" for FP64 probes
  deterministic_probe: true      # optional
  tf_nograd:
    autocast: false              # ensure TF no‑grad runs without AMP
    cast_params: true            # if a TF re‑mapping is required (rare)
    dtype: "float32"
  func_override:
    autocast: false
    cast_params: true
    dtype: "float32"             # set "float64" for the most demanding tiny‑η checks

(If `tf_nograd` is not used anywhere else, you can drop it and rely solely on `func_override` for this probe.)


F. Acceptance criteria for this step
------------------------------------
1) η=0 zero‑check: `max|direct - fc(η=0)| ≤ 1e‑7` on the test (b,g) item. If violated, fail fast with an actionable error.
2) Tiny‑η scaling (params‑only): For η_main = 1e‑5 and η_tiny = 1e‑10 (or FP64 with even smaller), the measured ratios satisfy
       ||Δθ||₂(tiny)/||Δθ||₂(main)  ≈  η_tiny/η_main  within a factor of 2,
       ||Δlogits||₂(tiny)/||Δlogits||₂(main)  ≈  η_tiny/η_main  within a factor of 3.
3) Entropy continuity: Running `param_override_test.py`’s SP checks reports near‑zero differences at η=0 and monotone linear growth for small η in the params‑only mode.
4) No buffers overridden in default path; a separate diagnostic path may exist but is not used for Part 1 of the probe.


G. Common pitfalls to avoid
---------------------------
• Mixing `self.model` and `self._mdl_target` when building mappings vs. calling `functional_call` (PEFT keyspace mismatch).
• Enabling AMP in either the baseline or overridden forward. Autocast must be disabled in both.
• Performing `base + η·v` in bf16/fp16; always upcast before the addition.
• Accidentally using `use_cache=True` in one path and `False` in another.
• Accidentally overriding buffers by merging params+buffers unless deliberately in a diagnostic mode.
• Computing log‑softmax in bf16; always cast logits to FP32 first (for this probe).


H. Minimal patch sketch (to guide the edit)
-------------------------------------------
Below is a compact sketch of the exact pieces to introduce. Adapt names to your file layout:

    # At top of sequence_processor.py
    from entropy_experiments.utils.param_overrides import build_functional_params_named
    from entropy_experiments.utils.param_overrides import _unwrap_module as _unwrap
    from entropy_experiments.utils.precision_utils import str_to_dtype, maybe_cast_logits_fp32, apply_global_precision

    class SequenceProcessor:
        def __init__(self, model, config, logger):
            self.model = model; self.config = config; self.logger = logger
            self._mdl_target = _unwrap(model) if callable(_unwrap) else model
            prec = (config or {}).get("precision", {})
            self._fo_cfg = prec.get("func_override", {})
            apply_global_precision(prec.get("allow_tf32", False), prec.get("matmul_precision", "high"))
            self._params_zero = None

        def _build_params_override(self, v_named, eta):
            cast_params = bool(self._fo_cfg.get("cast_params", True))
            force_dtype = str_to_dtype(self._fo_cfg.get("dtype", "float32")) if cast_params else None
            params, _ = build_functional_params_named(self._mdl_target, v_named, eta,
                                                      detach_params=True, detach_buffers=True,
                                                      force_param_dtype=force_dtype, force_buffer_dtype=None)
            return params

        @torch.no_grad()
        def _fc_logits_noautocast(self, ids, mask, mapping):
            m = self._mdl_target; was = m.training; m.eval()
            try:
                with torch.autocast(device_type="cuda", enabled=False):
                    out = torch.func.functional_call(m, mapping, (ids,), {"attention_mask": mask, "use_cache": False})
                return out.logits if hasattr(out, "logits") else out[0]
            finally:
                if was: m.train()

        @torch.no_grad()
        def teacher_force_logprobs_nograd(self, batch, params_override=None, return_logits=False):
            ids = batch["input_ids"]; mask = batch["attention_mask"]
            B,G,L = ids.shape; ids = ids.view(B*G, L); mask = mask.view(B*G, L)
            mapping = params_override or (self._params_zero or self._build_params_override(None, 0.0))
            logits = self._fc_logits_noautocast(ids, mask, mapping)
            logprobs = torch.nn.functional.log_softmax(maybe_cast_logits_fp32(logits), dim=-1)
            # Compute S/RB as in current implementation...
            return {"logprobs": logprobs}

With these changes in place, Part 1 of your probe (true ΔH via no‑grad entropy on the E‑batch) should become continuous in η and agree with the fixed‑sequence sanity probe.
