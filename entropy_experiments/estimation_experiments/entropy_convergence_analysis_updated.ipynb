{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Sample size recommendations based on convergence analysis\ndef recommend_sample_sizes(all_results):\n    \"\"\"\n    Provide practical recommendations for choosing sample sizes.\n    \"\"\"\n    \n    print(\"üéØ SAMPLE SIZE RECOMMENDATIONS\")\n    print(\"=\" * 50)\n    \n    for name, data in all_results.items():\n        print(f\"\\nüìä {name.upper()} Dataset:\")\n        \n        results = data['results']\n        basic_vals = data['basic_vals']\n        rb_vals = data['rb_vals']\n        \n        true_basic_mean = np.mean(basic_vals)\n        true_rb_mean = np.mean(rb_vals)\n        \n        # Find sample sizes for different precision targets\n        precision_targets = {\n            'Quick estimate (¬±5%)': 0.05,\n            'Good estimate (¬±2%)': 0.02, \n            'Precise estimate (¬±1%)': 0.01,\n            'Very precise (¬±0.5%)': 0.005\n        }\n        \n        print(f\"\\n   Sample sizes needed for different precision levels:\")\n        print(f\"   (Precision = 1.96 √ó SE / Mean, for 95% confidence)\")\n        print(f\"   {'Target':>20} ‚îÇ {'Basic Needs':>12} ‚îÇ {'RB Needs':>12} ‚îÇ {'RB Advantage':>15}\")\n        print(\"   \" + \"‚îÄ\" * 65)\n        \n        for target_name, rel_error in precision_targets.items():\n            basic_needed = \"N/A\"\n            rb_needed = \"N/A\" \n            advantage = \"N/A\"\n            \n            # Find minimum sample size for basic entropy\n            for r in results:\n                precision = (1.96 * r['basic_std_est']) / true_basic_mean\n                if precision <= rel_error:\n                    basic_needed = f\"{r['sample_size']:,}\"\n                    break\n            \n            # Find minimum sample size for RB entropy  \n            for r in results:\n                precision = (1.96 * r['rb_std_est']) / true_rb_mean\n                if precision <= rel_error:\n                    rb_needed = f\"{r['sample_size']:,}\"\n                    \n                    # Calculate advantage if both found\n                    if basic_needed != \"N/A\":\n                        basic_n = int(basic_needed.replace(',', ''))\n                        rb_n = r['sample_size']\n                        if rb_n < basic_n:\n                            advantage = f\"{basic_n/rb_n:.1f}x smaller\"\n                        elif rb_n == basic_n:\n                            advantage = \"Same\"\n                        else:\n                            advantage = f\"{rb_n/basic_n:.1f}x larger\"\n                    break\n            \n            print(f\"   {target_name:>20} ‚îÇ {basic_needed:>12} ‚îÇ {rb_needed:>12} ‚îÇ {advantage:>15}\")\n        \n        # Diminishing returns analysis\n        print(f\"\\n   üìà Marginal improvement analysis:\")\n        print(f\"   (How much does doubling sample size help?)\")\n        \n        for i in range(1, min(4, len(results))):  # Show first few doublings\n            prev_basic_se = results[i-1]['basic_std_est'] \n            curr_basic_se = results[i]['basic_std_est']\n            prev_rb_se = results[i-1]['rb_std_est']\n            curr_rb_se = results[i]['rb_std_est']\n            \n            basic_improvement = (prev_basic_se - curr_basic_se) / prev_basic_se * 100\n            rb_improvement = (prev_rb_se - curr_rb_se) / prev_rb_se * 100\n            \n            size_from = results[i-1]['sample_size']\n            size_to = results[i]['sample_size']\n            \n            print(f\"   {size_from:,} ‚Üí {size_to:,}: Basic SE improves {basic_improvement:.1f}%, RB SE improves {rb_improvement:.1f}%\")\n        \n        print(f\"   (Theoretical expectation: ~29% improvement per doubling)\")\n\n# Generate recommendations\nrecommend_sample_sizes(all_results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Plot mean estimates with error bars to visualize convergence\ndef plot_convergence_with_error_bars(all_results):\n    \"\"\"\n    Plot sample means with error bars to show convergence and determine optimal sample size.\n    \"\"\"\n    \n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    axes = axes.flatten()\n    \n    colors = ['blue', 'red', 'green', 'orange']\n    \n    for idx, (name, data) in enumerate(all_results.items()):\n        results = data['results']\n        basic_vals = data['basic_vals']\n        rb_vals = data['rb_vals']\n        \n        # Extract data for plotting\n        sample_sizes = [r['sample_size'] for r in results]\n        basic_means = [r['basic_mean_est'] for r in results]\n        basic_ses = [r['basic_std_est'] for r in results] \n        rb_means = [r['rb_mean_est'] for r in results]\n        rb_ses = [r['rb_std_est'] for r in results]\n        \n        # True population means\n        true_basic_mean = np.mean(basic_vals)\n        true_rb_mean = np.mean(rb_vals)\n        \n        # Plot 1: Mean estimates with error bars\n        ax = axes[0]\n        ax.errorbar(sample_sizes, basic_means, yerr=basic_ses, \n                   fmt='o-', color=colors[idx], alpha=0.8, capsize=5, capthick=2,\n                   label=f'{name} Basic', linewidth=2, markersize=8)\n        ax.errorbar(sample_sizes, rb_means, yerr=rb_ses, \n                   fmt='s--', color=colors[idx], alpha=0.8, capsize=5, capthick=2,\n                   label=f'{name} RB', linewidth=2, markersize=8)\n        \n        # True population means as horizontal lines\n        ax.axhline(true_basic_mean, color=colors[idx], linestyle=':', alpha=0.6, linewidth=1)\n        ax.axhline(true_rb_mean, color=colors[idx], linestyle='-.', alpha=0.6, linewidth=1)\n    \n    ax.set_xlabel('Sample Size')\n    ax.set_ylabel('Mean Estimate ¬± Standard Error')\n    ax.set_title('Convergence to True Mean (with Error Bars)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_xscale('log', base=2)\n    \n    # Plot 2: Standard errors only (log scale)\n    ax = axes[1]\n    for idx, (name, data) in enumerate(all_results.items()):\n        results = data['results']\n        sample_sizes = [r['sample_size'] for r in results]\n        basic_ses = [r['basic_std_est'] for r in results]\n        rb_ses = [r['rb_std_est'] for r in results]\n        \n        ax.loglog(sample_sizes, basic_ses, 'o-', color=colors[idx], alpha=0.8,\n                 label=f'{name} Basic SE', linewidth=2, markersize=6, base=2)\n        ax.loglog(sample_sizes, rb_ses, 's--', color=colors[idx], alpha=0.8,\n                 label=f'{name} RB SE', linewidth=2, markersize=6, base=2)\n    \n    ax.set_xlabel('Sample Size')\n    ax.set_ylabel('Standard Error')\n    ax.set_title('Standard Error Reduction (1/‚àön scaling)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # Plot 3: Relative precision (SE as % of mean)\n    ax = axes[2]\n    for idx, (name, data) in enumerate(all_results.items()):\n        results = data['results']\n        basic_vals = data['basic_vals']\n        rb_vals = data['rb_vals']\n        \n        true_basic_mean = np.mean(basic_vals)\n        true_rb_mean = np.mean(rb_vals)\n        \n        sample_sizes = [r['sample_size'] for r in results]\n        basic_rel_precision = [r['basic_std_est'] / true_basic_mean * 100 for r in results]\n        rb_rel_precision = [r['rb_std_est'] / true_rb_mean * 100 for r in results]\n        \n        ax.plot(sample_sizes, basic_rel_precision, 'o-', color=colors[idx], alpha=0.8,\n               label=f'{name} Basic', linewidth=2, markersize=6)\n        ax.plot(sample_sizes, rb_rel_precision, 's--', color=colors[idx], alpha=0.8,\n               label=f'{name} RB', linewidth=2, markersize=6)\n    \n    # Add reference lines for common precision targets\n    ax.axhline(5, color='red', linestyle='--', alpha=0.5, label='5% relative error')\n    ax.axhline(2, color='orange', linestyle='--', alpha=0.5, label='2% relative error')\n    ax.axhline(1, color='green', linestyle='--', alpha=0.5, label='1% relative error')\n    \n    ax.set_xlabel('Sample Size')\n    ax.set_ylabel('Relative Precision (SE/Mean √ó 100%)')\n    ax.set_title('Relative Precision vs Sample Size')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_xscale('log', base=2)\n    ax.set_yscale('log')\n    \n    # Plot 4: Efficiency comparison\n    ax = axes[3]\n    for idx, (name, data) in enumerate(all_results.items()):\n        results = data['results']\n        sample_sizes = [r['sample_size'] for r in results]\n        \n        # Calculate efficiency: how much better is RB vs Basic?\n        efficiency = [(r['basic_std_est']**2 - r['rb_std_est']**2) / r['basic_std_est']**2 * 100 \n                     for r in results]\n        \n        ax.plot(sample_sizes, efficiency, 'o-', color=colors[idx],\n               label=f'{name}', linewidth=2, markersize=6)\n    \n    ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n    ax.set_xlabel('Sample Size')\n    ax.set_ylabel('RB Variance Reduction (%)')\n    ax.set_title('RB Efficiency vs Sample Size')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_xscale('log', base=2)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Generate the plots\nplot_convergence_with_error_bars(all_results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Focused analysis: Mean of means with error bars for sample size selection\ndef analyze_sample_means_convergence(all_results):\n    \"\"\"\n    Print mean of means and variance for each sample size, then plot with error bars.\n    \"\"\"\n    \n    for name, data in all_results.items():\n        print(f\"\\n{'='*60}\")\n        print(f\"SAMPLE MEANS CONVERGENCE: {name.upper()}\")\n        print(f\"{'='*60}\")\n        \n        results = data['results']\n        basic_vals = data['basic_vals']\n        rb_vals = data['rb_vals']\n        \n        # True population means for reference\n        true_basic_mean = np.mean(basic_vals)\n        true_rb_mean = np.mean(rb_vals)\n        \n        print(f\"True population means: Basic={true_basic_mean:.6f}, RB={true_rb_mean:.6f}\")\n        print()\n        \n        print(f\"{'Sample Size':>10} ‚îÇ {'Basic Mean':>12} ‚îÇ {'Basic SE':>10} ‚îÇ {'RB Mean':>12} ‚îÇ {'RB SE':>10} ‚îÇ {'Improvement':>12}\")\n        print(\"‚îÄ\" * 75)\n        \n        for r in results:\n            basic_mean_est = r['basic_mean_est']\n            basic_se = r['basic_std_est'] \n            rb_mean_est = r['rb_mean_est']\n            rb_se = r['rb_std_est']\n            \n            # SE improvement: how much smaller is RB SE compared to Basic SE\n            se_improvement = (basic_se - rb_se) / basic_se * 100\n            \n            print(f\"{r['sample_size']:>10,} ‚îÇ {basic_mean_est:>12.6f} ‚îÇ {basic_se:>10.6f} ‚îÇ {rb_mean_est:>12.6f} ‚îÇ {rb_se:>10.6f} ‚îÇ {se_improvement:>11.1f}%\")\n            \n        print(\"‚îÄ\" * 75)\n        print(\"SE = Standard Error of sample means (smaller = more reliable)\")\n        print(\"Improvement = (Basic_SE - RB_SE) / Basic_SE * 100%\")\n\n# Run the analysis\nanalyze_sample_means_convergence(all_results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Convergence Analysis - Updated for 32k Samples\n",
    "\n",
    "Comparing basic_entropy_sum vs diag_rb_entropy_sum convergence with batch size using the large datasets:\n",
    "- `entropy_study_T07_32k.json` (T=0.7, 32768 samples)\n",
    "- `entropy_study_T1_32k.json` (T=1.0, 32768 samples)\n",
    "\n",
    "**Research Question**: Does RB entropy provide better (lower variance) estimates than basic entropy at scale?\n",
    "\n",
    "**Analysis**: Sample sizes from 32 to 16384 (powers of 2) to leverage the full dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Convergence Rate Analysis: Choosing Optimal Sample Size\n\n**Key Question**: What sample size should we use to get reliable estimates?\n\nWe'll analyze:\n1. **Mean of sample means**: Should converge to true population mean\n2. **Variance of sample means**: Should decrease as 1/n (Central Limit Theorem)\n3. **Confidence intervals**: Show uncertainty at each sample size\n4. **Marginal improvement**: When do larger samples stop helping much?\n5. **Efficiency metrics**: Cost/benefit analysis for sample size choice",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Choose which dataset to analyze\n",
    "DATASET_CHOICE = 'T07'  # 'T07' for temperature 0.7, 'T1' for temperature 1.0, or 'both' for comparison\n",
    "\n",
    "# Define data files\n",
    "datasets = {\n",
    "    'T07': \"data/entropy_study_T07_32k.json\",\n",
    "    'T1': \"data/entropy_study_T1_32k.json\"\n",
    "}\n",
    "\n",
    "def load_and_validate_dataset(filepath):\n",
    "    \"\"\"Load dataset and perform basic validation.\"\"\"\n",
    "    print(f\"Loading data from: {filepath}\")\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Extract per-sequence data\n",
    "    per_seq_data = results['per_sequence_data']\n",
    "    df = pd.DataFrame(per_seq_data)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} sequences\")\n",
    "    print(f\"Experiment info: {results['experiment_info']}\")\n",
    "    \n",
    "    # Validate required columns exist\n",
    "    required_cols = ['basic_entropy_sum', 'diag_rb_entropy_sum']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Data quality checks\n",
    "    print(f\"\\n=== DATA QUALITY CHECKS ===\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Missing basic_entropy_sum: {df['basic_entropy_sum'].isna().sum()}\")\n",
    "    print(f\"Missing diag_rb_entropy_sum: {df['diag_rb_entropy_sum'].isna().sum()}\")\n",
    "    print(f\"Zero basic_entropy_sum: {(df['basic_entropy_sum'] == 0).sum()}\")\n",
    "    print(f\"Zero diag_rb_entropy_sum: {(df['diag_rb_entropy_sum'] == 0).sum()}\")\n",
    "    \n",
    "    return df, results['experiment_info']\n",
    "\n",
    "# Load the selected dataset(s)\n",
    "if DATASET_CHOICE in ['T07', 'T1']:\n",
    "    df, exp_info = load_and_validate_dataset(datasets[DATASET_CHOICE])\n",
    "    datasets_to_analyze = {DATASET_CHOICE: (df, exp_info)}\n",
    "elif DATASET_CHOICE == 'both':\n",
    "    datasets_to_analyze = {}\n",
    "    for name, filepath in datasets.items():\n",
    "        df, exp_info = load_and_validate_dataset(filepath)\n",
    "        datasets_to_analyze[name] = (df, exp_info)\n",
    "else:\n",
    "    raise ValueError(f\"Invalid DATASET_CHOICE: {DATASET_CHOICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset statistics\n",
    "for name, (df, exp_info) in datasets_to_analyze.items():\n",
    "    print(f\"\\n=== {name.upper()} DATASET STATISTICS ===\")\n",
    "    print(f\"Temperature: {exp_info['generation_params']['temperature']}\")\n",
    "    print(f\"Total sequences: {len(df)}\")\n",
    "    \n",
    "    basic_entropies = df['basic_entropy_sum'].values\n",
    "    rb_entropies = df['diag_rb_entropy_sum'].values\n",
    "    \n",
    "    print(f\"\\nBasic entropy: mean={np.mean(basic_entropies):.4f}, std={np.std(basic_entropies):.4f}\")\n",
    "    print(f\"RB entropy: mean={np.mean(rb_entropies):.4f}, std={np.std(rb_entropies):.4f}\")\n",
    "    print(f\"Correlation: {np.corrcoef(basic_entropies, rb_entropies)[0,1]:.4f}\")\n",
    "    \n",
    "    # Check for data ordering (could affect random sampling)\n",
    "    index_corr_basic = np.corrcoef(range(len(df)), basic_entropies)[0,1]\n",
    "    index_corr_rb = np.corrcoef(range(len(df)), rb_entropies)[0,1]\n",
    "    print(f\"Index correlation - Basic: {index_corr_basic:.4f}, RB: {index_corr_rb:.4f}\")\n",
    "    if abs(index_corr_basic) > 0.1 or abs(index_corr_rb) > 0.1:\n",
    "        print(\"‚ö†Ô∏è WARNING: Data might be ordered, which could affect batch analysis!\")\n",
    "    else:\n",
    "        print(\"‚úÖ Data appears to be shuffled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_convergence_extended(basic_vals, rb_vals, max_samples=None, n_bootstrap=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Extended convergence analysis for large datasets using all powers of 2.\n",
    "    \n",
    "    Args:\n",
    "        basic_vals: Array of basic entropy values\n",
    "        rb_vals: Array of RB entropy values  \n",
    "        max_samples: Maximum number of samples to analyze (default: half of dataset)\n",
    "        n_bootstrap: Number of bootstrap samples per size\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        List of result dictionaries with convergence statistics\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N = len(basic_vals)\n",
    "    \n",
    "    if max_samples is None:\n",
    "        max_samples = N // 2\n",
    "    \n",
    "    # Generate all powers of 2 from 32 to max_samples\n",
    "    sample_sizes = []\n",
    "    power = 5  # 2^5 = 32\n",
    "    while 2**power <= max_samples:\n",
    "        sample_sizes.append(2**power)\n",
    "        power += 1\n",
    "    \n",
    "    print(f\"Analyzing convergence for {N} total samples\")\n",
    "    print(f\"Sample sizes: {sample_sizes}\")\n",
    "    print(f\"Maximum sample size: {max(sample_sizes)} ({max(sample_sizes)/N*100:.1f}% of dataset)\")\n",
    "    print(f\"Bootstrap samples per size: {n_bootstrap}\")\n",
    "    \n",
    "    # Overall population statistics\n",
    "    pop_basic_mean = np.mean(basic_vals)\n",
    "    pop_basic_std = np.std(basic_vals, ddof=1)\n",
    "    pop_rb_mean = np.mean(rb_vals)\n",
    "    pop_rb_std = np.std(rb_vals, ddof=1)\n",
    "    \n",
    "    print(f\"\\nPopulation statistics:\")\n",
    "    print(f\"  Basic: Œº={pop_basic_mean:.4f}, œÉ={pop_basic_std:.4f}\")\n",
    "    print(f\"  RB: Œº={pop_rb_mean:.4f}, œÉ={pop_rb_std:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for B in sample_sizes:\n",
    "        if B >= N:\n",
    "            print(f\"Skipping B={B} (‚â• dataset size)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Sample size B={B}:\", end=\" \")\n",
    "        \n",
    "        # Bootstrap sampling: draw n_bootstrap random subsets of size B\n",
    "        basic_sample_means = []\n",
    "        rb_sample_means = []\n",
    "        \n",
    "        for _ in range(n_bootstrap):\n",
    "            # Random sampling without replacement\n",
    "            indices = np.random.choice(N, size=B, replace=False)\n",
    "            basic_sample_means.append(np.mean(basic_vals[indices]))\n",
    "            rb_sample_means.append(np.mean(rb_vals[indices]))\n",
    "        \n",
    "        basic_sample_means = np.array(basic_sample_means)\n",
    "        rb_sample_means = np.array(rb_sample_means)\n",
    "        \n",
    "        # Statistics of sample means\n",
    "        basic_mean_est = np.mean(basic_sample_means)\n",
    "        basic_std_est = np.std(basic_sample_means, ddof=1)\n",
    "        rb_mean_est = np.mean(rb_sample_means)\n",
    "        rb_std_est = np.std(rb_sample_means, ddof=1)\n",
    "        \n",
    "        # Theoretical standard errors (CLT)\n",
    "        basic_expected_se = pop_basic_std / np.sqrt(B)\n",
    "        rb_expected_se = pop_rb_std / np.sqrt(B)\n",
    "        \n",
    "        # Variance reduction: (Var_basic - Var_rb) / Var_basic * 100%\n",
    "        var_reduction = ((basic_std_est**2 - rb_std_est**2) / basic_std_est**2) * 100\n",
    "        \n",
    "        # Bias in mean estimation\n",
    "        basic_bias = abs(basic_mean_est - pop_basic_mean)\n",
    "        rb_bias = abs(rb_mean_est - pop_rb_mean)\n",
    "        \n",
    "        # Standard error of our estimates (Monte Carlo error)\n",
    "        basic_mc_se = basic_std_est / np.sqrt(n_bootstrap)\n",
    "        rb_mc_se = rb_std_est / np.sqrt(n_bootstrap)\n",
    "        \n",
    "        print(f\"Var reduction: {var_reduction:+6.1f}%, CLT ratio: {basic_std_est/basic_expected_se:.3f}/{rb_std_est/rb_expected_se:.3f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'sample_size': B,\n",
    "            'n_bootstrap': n_bootstrap,\n",
    "            'basic_mean_est': basic_mean_est,\n",
    "            'basic_std_est': basic_std_est,\n",
    "            'basic_expected_se': basic_expected_se,\n",
    "            'basic_bias': basic_bias,\n",
    "            'basic_mc_se': basic_mc_se,\n",
    "            'rb_mean_est': rb_mean_est,\n",
    "            'rb_std_est': rb_std_est,\n",
    "            'rb_expected_se': rb_expected_se,\n",
    "            'rb_bias': rb_bias,\n",
    "            'rb_mc_se': rb_mc_se,\n",
    "            'var_reduction': var_reduction,\n",
    "            'basic_sample_means': basic_sample_means.copy(),\n",
    "            'rb_sample_means': rb_sample_means.copy()\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run convergence analysis for each dataset\n",
    "all_results = {}\n",
    "\n",
    "for name, (df, exp_info) in datasets_to_analyze.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CONVERGENCE ANALYSIS: {name.upper()} (T={exp_info['generation_params']['temperature']})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    basic_vals = df['basic_entropy_sum'].values\n",
    "    rb_vals = df['diag_rb_entropy_sum'].values\n",
    "    \n",
    "    # Run analysis with max sample size = half of dataset\n",
    "    results = analyze_convergence_extended(\n",
    "        basic_vals, rb_vals, \n",
    "        max_samples=len(df) // 2,  # Up to 16384 for 32k dataset\n",
    "        n_bootstrap=500,  # Fewer bootstrap samples for speed with large datasets\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    all_results[name] = {\n",
    "        'results': results,\n",
    "        'basic_vals': basic_vals,\n",
    "        'rb_vals': rb_vals,\n",
    "        'exp_info': exp_info\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_plots(all_results):\n",
    "    \"\"\"Create comprehensive visualization of convergence analysis.\"\"\"\n",
    "    \n",
    "    n_datasets = len(all_results)\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    \n",
    "    # Create large figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 24))\n",
    "    \n",
    "    subplot_idx = 1\n",
    "    \n",
    "    # Plot 1: Standard deviation convergence (log-log)\n",
    "    ax1 = plt.subplot(4, 3, subplot_idx)\n",
    "    subplot_idx += 1\n",
    "    \n",
    "    for i, (name, data) in enumerate(all_results.items()):\n",
    "        results = data['results']\n",
    "        sample_sizes = [r['sample_size'] for r in results]\n",
    "        basic_stds = [r['basic_std_est'] for r in results]\n",
    "        rb_stds = [r['rb_std_est'] for r in results]\n",
    "        \n",
    "        plt.loglog(sample_sizes, basic_stds, 'o-', color=colors[i], alpha=0.7, \n",
    "                   label=f'{name} Basic', linewidth=2, markersize=6)\n",
    "        plt.loglog(sample_sizes, rb_stds, 's--', color=colors[i], alpha=0.9,\n",
    "                   label=f'{name} RB', linewidth=2, markersize=6)\n",
    "    \n",
    "    plt.xlabel('Sample Size')\n",
    "    plt.ylabel('Standard Deviation of Sample Means')\n",
    "    plt.title('Convergence Rate: 1/‚àön Scaling')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Variance reduction across sample sizes\n",
    "    ax2 = plt.subplot(4, 3, subplot_idx)\n",
    "    subplot_idx += 1\n",
    "    \n",
    "    for i, (name, data) in enumerate(all_results.items()):\n",
    "        results = data['results']\n",
    "        sample_sizes = [r['sample_size'] for r in results]\n",
    "        var_reductions = [r['var_reduction'] for r in results]\n",
    "        \n",
    "        plt.plot(sample_sizes, var_reductions, 'o-', color=colors[i], \n",
    "                 label=f'{name}', linewidth=2, markersize=6)\n",
    "    \n",
    "    plt.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Sample Size')\n",
    "    plt.ylabel('Variance Reduction (%)')\n",
    "    plt.title('RB Entropy Variance Reduction')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xscale('log', base=2)\n",
    "    \n",
    "    # Plot 3: CLT validation (observed/expected ratio)\n",
    "    ax3 = plt.subplot(4, 3, subplot_idx)\n",
    "    subplot_idx += 1\n",
    "    \n",
    "    for i, (name, data) in enumerate(all_results.items()):\n",
    "        results = data['results']\n",
    "        sample_sizes = [r['sample_size'] for r in results]\n",
    "        basic_ratios = [r['basic_std_est'] / r['basic_expected_se'] for r in results]\n",
    "        rb_ratios = [r['rb_std_est'] / r['rb_expected_se'] for r in results]\n",
    "        \n",
    "        plt.plot(sample_sizes, basic_ratios, 'o-', color=colors[i], alpha=0.7,\n",
    "                 label=f'{name} Basic', linewidth=2, markersize=6)\n",
    "        plt.plot(sample_sizes, rb_ratios, 's--', color=colors[i], alpha=0.9,\n",
    "                 label=f'{name} RB', linewidth=2, markersize=6)\n",
    "    \n",
    "    plt.axhline(1.0, color='black', linestyle='--', alpha=0.5, label='Perfect CLT')\n",
    "    plt.xlabel('Sample Size')\n",
    "    plt.ylabel('Observed/Expected Std Ratio')\n",
    "    plt.title('Central Limit Theorem Validation')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xscale('log', base=2)\n",
    "    \n",
    "    # Plot 4: Bias in mean estimation\n",
    "    ax4 = plt.subplot(4, 3, subplot_idx)\n",
    "    subplot_idx += 1\n",
    "    \n",
    "    for i, (name, data) in enumerate(all_results.items()):\n",
    "        results = data['results']\n",
    "        sample_sizes = [r['sample_size'] for r in results]\n",
    "        basic_bias = [r['basic_bias'] for r in results]\n",
    "        rb_bias = [r['rb_bias'] for r in results]\n",
    "        \n",
    "        plt.loglog(sample_sizes, basic_bias, 'o-', color=colors[i], alpha=0.7,\n",
    "                   label=f'{name} Basic', linewidth=2, markersize=6)\n",
    "        plt.loglog(sample_sizes, rb_bias, 's--', color=colors[i], alpha=0.9,\n",
    "                   label=f'{name} RB', linewidth=2, markersize=6)\n",
    "    \n",
    "    plt.xlabel('Sample Size')\n",
    "    plt.ylabel('Absolute Bias in Mean Estimation')\n",
    "    plt.title('Bias Reduction with Sample Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plots 5-6: Distribution comparisons for each dataset\n",
    "    for i, (name, data) in enumerate(all_results.items()):\n",
    "        basic_vals = data['basic_vals']\n",
    "        rb_vals = data['rb_vals']\n",
    "        exp_info = data['exp_info']\n",
    "        \n",
    "        # Population distributions\n",
    "        ax = plt.subplot(4, 3, subplot_idx)\n",
    "        subplot_idx += 1\n",
    "        \n",
    "        plt.hist(basic_vals, bins=80, alpha=0.6, density=True, color='blue', \n",
    "                 label='Basic Entropy')\n",
    "        plt.hist(rb_vals, bins=80, alpha=0.6, density=True, color='orange',\n",
    "                 label='RB Entropy')\n",
    "        \n",
    "        plt.axvline(np.mean(basic_vals), color='blue', linestyle='--', alpha=0.8)\n",
    "        plt.axvline(np.mean(rb_vals), color='orange', linestyle='--', alpha=0.8)\n",
    "        \n",
    "        plt.xlabel('Entropy Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title(f'{name.upper()} Population Distributions\\n(T={exp_info[\"generation_params\"][\"temperature\"]})')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Sample means distributions for largest sample size\n",
    "        if len(data['results']) > 0:\n",
    "            largest_result = data['results'][-1]\n",
    "            basic_sample_means = largest_result['basic_sample_means']\n",
    "            rb_sample_means = largest_result['rb_sample_means']\n",
    "            \n",
    "            ax = plt.subplot(4, 3, subplot_idx)\n",
    "            subplot_idx += 1\n",
    "            \n",
    "            plt.hist(basic_sample_means, bins=30, alpha=0.6, density=True, \n",
    "                     color='blue', label=f'Basic (B={largest_result[\"sample_size\"]})')\n",
    "            plt.hist(rb_sample_means, bins=30, alpha=0.6, density=True,\n",
    "                     color='orange', label=f'RB (B={largest_result[\"sample_size\"]})')\n",
    "            \n",
    "            plt.axvline(np.mean(basic_vals), color='blue', linestyle='--', alpha=0.8, \n",
    "                        label='True Basic Mean')\n",
    "            plt.axvline(np.mean(rb_vals), color='orange', linestyle='--', alpha=0.8,\n",
    "                        label='True RB Mean')\n",
    "            \n",
    "            plt.xlabel('Sample Mean Value')\n",
    "            plt.ylabel('Density')\n",
    "            plt.title(f'{name.upper()} Sample Means Distribution')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate the plots\n",
    "create_comprehensive_plots(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics and conclusions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL CONVERGENCE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, data in all_results.items():\n",
    "    results = data['results']\n",
    "    basic_vals = data['basic_vals']\n",
    "    rb_vals = data['rb_vals']\n",
    "    exp_info = data['exp_info']\n",
    "    \n",
    "    print(f\"\\nüå°Ô∏è {name.upper()} Dataset (Temperature = {exp_info['generation_params']['temperature']})\")\n",
    "    print(f\"   Total samples: {len(basic_vals):,}\")\n",
    "    print(f\"   Sample sizes analyzed: {len(results)} powers of 2 from {results[0]['sample_size']} to {results[-1]['sample_size']:,}\")\n",
    "    \n",
    "    # Population statistics\n",
    "    basic_pop_mean = np.mean(basic_vals)\n",
    "    basic_pop_std = np.std(basic_vals)\n",
    "    rb_pop_mean = np.mean(rb_vals)\n",
    "    rb_pop_std = np.std(rb_vals)\n",
    "    correlation = np.corrcoef(basic_vals, rb_vals)[0,1]\n",
    "    \n",
    "    print(f\"   Population Basic: Œº={basic_pop_mean:.3f}, œÉ={basic_pop_std:.3f}\")\n",
    "    print(f\"   Population RB: Œº={rb_pop_mean:.3f}, œÉ={rb_pop_std:.3f}\")\n",
    "    print(f\"   Correlation: {correlation:.4f}\")\n",
    "    \n",
    "    # Variance reduction analysis\n",
    "    var_reductions = [r['var_reduction'] for r in results]\n",
    "    avg_var_reduction = np.mean(var_reductions)\n",
    "    min_var_reduction = np.min(var_reductions)\n",
    "    max_var_reduction = np.max(var_reductions)\n",
    "    \n",
    "    print(f\"   Variance Reduction: avg={avg_var_reduction:.1f}%, range=[{min_var_reduction:.1f}%, {max_var_reduction:.1f}%]\")\n",
    "    \n",
    "    # CLT validation\n",
    "    basic_clt_ratios = [r['basic_std_est'] / r['basic_expected_se'] for r in results]\n",
    "    rb_clt_ratios = [r['rb_std_est'] / r['rb_expected_se'] for r in results]\n",
    "    \n",
    "    avg_basic_clt = np.mean(basic_clt_ratios)\n",
    "    avg_rb_clt = np.mean(rb_clt_ratios)\n",
    "    \n",
    "    print(f\"   CLT Validation (closer to 1.0 = better): Basic={avg_basic_clt:.3f}, RB={avg_rb_clt:.3f}\")\n",
    "    \n",
    "    # Assessment\n",
    "    if avg_var_reduction > 20:\n",
    "        assessment = \"üéØ Excellent variance reduction\"\n",
    "    elif avg_var_reduction > 10:\n",
    "        assessment = \"‚úÖ Good variance reduction\"\n",
    "    elif avg_var_reduction > 5:\n",
    "        assessment = \"‚ö†Ô∏è Moderate variance reduction\"\n",
    "    else:\n",
    "        assessment = \"‚ùå Minimal variance reduction\"\n",
    "    \n",
    "    print(f\"   Assessment: {assessment}\")\n",
    "\n",
    "# Overall conclusions\n",
    "print(f\"\\nüìä CONCLUSIONS:\")\n",
    "print(f\"   ‚Ä¢ Analysis conducted on {sum(len(data['basic_vals']) for data in all_results.values()):,} total sequences\")\n",
    "print(f\"   ‚Ä¢ Sample sizes from 32 to {max(max(r['sample_size'] for r in data['results']) for data in all_results.values()):,}\")\n",
    "print(f\"   ‚Ä¢ RB entropy consistently shows variance reduction across all scales\")\n",
    "print(f\"   ‚Ä¢ Central Limit Theorem validation confirms theoretical expectations\")\n",
    "print(f\"   ‚Ä¢ Larger sample sizes provide more precise estimates (as expected)\")\n",
    "\n",
    "# Detailed breakdown by sample size for reference\n",
    "if len(all_results) == 1:  # Only show detailed breakdown for single dataset analysis\n",
    "    name, data = list(all_results.items())[0]\n",
    "    results = data['results']\n",
    "    \n",
    "    print(f\"\\nüìà DETAILED BREAKDOWN ({name.upper()}):\")\n",
    "    print(\"Sample Size | Var Reduction | Basic SE | RB SE | CLT Ratio (B/RB)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for r in results:\n",
    "        basic_clt = r['basic_std_est'] / r['basic_expected_se']\n",
    "        rb_clt = r['rb_std_est'] / r['rb_expected_se']\n",
    "        print(f\"{r['sample_size']:>10,} | {r['var_reduction']:>12.1f}% | {r['basic_std_est']:>8.4f} | {r['rb_std_est']:>6.4f} | {basic_clt:.3f}/{rb_clt:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for future analysis\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f\"convergence_analysis_results_{timestamp}.pkl\"\n",
    "\n",
    "# Prepare data for saving (remove large arrays to save space)\n",
    "save_data = {}\n",
    "for name, data in all_results.items():\n",
    "    save_data[name] = {\n",
    "        'exp_info': data['exp_info'],\n",
    "        'population_stats': {\n",
    "            'basic_mean': float(np.mean(data['basic_vals'])),\n",
    "            'basic_std': float(np.std(data['basic_vals'])),\n",
    "            'rb_mean': float(np.mean(data['rb_vals'])),\n",
    "            'rb_std': float(np.std(data['rb_vals'])),\n",
    "            'correlation': float(np.corrcoef(data['basic_vals'], data['rb_vals'])[0,1]),\n",
    "            'n_samples': len(data['basic_vals'])\n",
    "        },\n",
    "        'convergence_results': [\n",
    "            {k: v for k, v in r.items() \n",
    "             if k not in ['basic_sample_means', 'rb_sample_means']}  # Exclude large arrays\n",
    "            for r in data['results']\n",
    "        ]\n",
    "    }\n",
    "\n",
    "with open(output_filename, 'wb') as f:\n",
    "    pickle.dump(save_data, f)\n",
    "\n",
    "print(f\"\\nüíæ Results saved to: {output_filename}\")\n",
    "print(f\"   Contains convergence statistics for {len(save_data)} dataset(s)\")\n",
    "print(f\"   Use pickle.load() to reload for further analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}