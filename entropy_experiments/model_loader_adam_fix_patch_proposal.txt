Summary
Fix optimizer-state loading alignment for Adam preconditioning by constructing the probe optimizer over LoRA trainables only (in the same stable order as training) before loading `optimizer.pt`. This prevents remapped states from attaching to frozen base weights and eliminates missing `exp_avg_sq` on LoRA params that inflate bars_dot.

Rationale
- Current bug: `load_adam_optimizer_from_path()` builds `AdamW(model.parameters(), ...)`, which includes many frozen base weights. The saved optimizer state from LoRA/QLoRA training typically contains only LoRA trainables (plus possibly `lm_head`). When we remap by position/id, states attach to the wrong tensors; LoRA params end up with no `exp_avg_sq` and the preconditioner computes g/(sqrt(v)+eps) with v~0, scaling ~1/eps (~1e8), inflating bars_dot.
- Restricting the optimizer to the exact trainables (LoRA + optional head) realigns membership and order, so loaded states land on the right tensors. This matches B.1/B.2 of `entropy_experiments/adam_fix_plan.txt`.

Minimal Diffs
File: entropy_experiments/model_loader.py
Function: load_adam_optimizer_from_path

Patch A: build AdamW over LoRA trainables only, not all params
--- BEFORE (excerpt) ---
from torch.optim import AdamW
...
opt = AdamW(model.parameters(), lr=(lr or 1e-4), weight_decay=weight_decay, betas=betas, eps=eps)
...
state_dict = torch.load(str(optimizer_path), map_location="cpu")
remapped = _remap_optimizer_state_ids(state_dict, opt)
opt.load_state_dict(remapped)
return opt

--- AFTER (exact replacement) ---
from torch.optim import AdamW
...
# Build optimizer over LoRA trainables only (stable order via named_parameters)
peft_mod = model.module if hasattr(model, "module") else model
trainable_params = [p for _, p in peft_mod.named_parameters() if p.requires_grad]
if not trainable_params:
    raise RuntimeError("No trainable (LoRA) parameters found; cannot build optimizer for probe.")

opt = AdamW(trainable_params, lr=(lr or 1e-4), weight_decay=weight_decay, betas=betas, eps=eps)

try:
    state_dict = torch.load(str(optimizer_path), map_location="cpu")
    remapped = _remap_optimizer_state_ids(state_dict, opt)
    opt.load_state_dict(remapped)

    # Coverage summary (diagnostic): fraction of params with exp_avg_sq and step
    have_v = 0; have_step = 0; total = 0
    for group in opt.param_groups:
        for p in group["params"]:
            total += 1
            st = opt.state.get(p, {})
            if isinstance(st.get("exp_avg_sq", None), torch.Tensor):
                have_v += 1
            s = st.get("step", None)
            if isinstance(s, int) or isinstance(s, torch.Tensor):
                have_step += 1
    coverage_v = have_v / max(total, 1)
    coverage_step = have_step / max(total, 1)
    print(f"[optimizer-state] coverage: v(exp_avg_sq) {have_v}/{total} = {coverage_v:.1%}, step {have_step}/{total} = {coverage_step:.1%}")

    return opt
except Exception:
    return opt  # Fall back to fresh optimizer

Notes:
- Using `named_parameters()` of the PEFT-wrapped module yields a stable order consistent with training. `requires_grad` filters to LoRA trainables (+ optional head if unfrozen).
- Remapping still runs to translate saved param IDs to current param object IDs by position within this restricted list.
- Coverage print lets us catch misalignment early (expect ~100% coverage on v and step for trainables).

Implementation Steps
1) Apply Patch A to `entropy_experiments/model_loader.py` in `load_adam_optimizer_from_path`.
2) Do not change `_remap_optimizer_state_ids` for now; its positional mapping works once membership is correct.
3) Keep Adam hyperparams (betas, eps, weight_decay) consistent; `opt.load_state_dict` will overwrite group hparams from the checkpoint anyway.
4) Re-run probe initialization paths that call `load_adam_optimizer_from_path` (e.g., `OfflineEntropyProbe.run_mixed_probe`).

Validation
- Local log check: After loading optimizer, confirm log prints
  `[optimizer-state] coverage: v(exp_avg_sq) 392/392 = 100.0%, step 392/392 = 100.0%` (counts are illustrative; expect near 100%).
- Sanity magnitude: With the same config/checkpoint used previously, confirm `bars_dot` returns to a reasonable scale (no 1e6–1e8 blow-ups). Optionally run a single microbatch with identity preconditioner to verify the ratio `bars_dot_precond / bars_dot_identity < 1e3`.
- Lambda test: Run `python entropy_experiments/run_lambda_h100_test.py` on the H100 box. Acceptance: script completes without warnings about poor coverage; `deltaH1` finite; timing printed; no `[Y] Adam preconditioning complete` anomalies (e.g., many zero/tiny exp_avg_sq).

Acceptance Criteria
- v/step coverage ≥ 90% on LoRA trainables; ideally ~100%.
- `bars_dot` within expected historical range (no orders-of-magnitude inflation).
- AdamPreconditioner `validate_preconditioner()` passes; no NaN/inf detected.
- Lambda H100 test script runs to completion and prints sensible metrics.

Rollback
- Revert the changes in `load_adam_optimizer_from_path` to use `model.parameters()` if needed.
- Remove/disable the coverage print if it becomes noisy.

Risk Notes
- If the training checkpoint used multiple optimizer param groups with different hparams, our current remap flattens into the first group’s hparams. This is typical for LoRA setups (uniform hparams), but if your training used heterogeneous groups, consider extending remap to preserve grouping per-trainable.

