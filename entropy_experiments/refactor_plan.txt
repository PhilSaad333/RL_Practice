# Entropy Experiments Refactor Plan: Integration with SequenceProcessor

## Executive Summary

This document outlines the comprehensive plan to refactor the entropy experiments codebase to leverage the new `SequenceProcessor` infrastructure, with a focus on implementing replacement sampling for E batches and modernizing the architecture.

## Current Architecture Analysis

### Key Files and Their Roles

1. **offline_entropy_probe.py** (Main Orchestrator - 1,139 lines)
   - Coordinates the entire entropy analysis pipeline
   - Loads checkpoints and manages model/optimizer state
   - Orchestrates E/U batch sampling and δH₁ computation
   - Manages distributed execution and result compilation

2. **probe_components.py** (Core Logic)
   - `sample_batch(B, G, indices)` - samples B prompts, G responses each
   - Currently uses custom generation logic (similar to collect_rollouts.py)
   - Implements teacher forcing for logprob computation
   - Handles gradient computation for δH₁ estimation

3. **importance_sampling.py** (Ground Truth Computation)
   - Computes actual entropy change ΔH via importance sampling
   - Takes real optimizer steps on U batches
   - Estimates H(π_θ) and H(π_{θ+δθ}) on same E batch

4. **conditional_variance.py** (Variance Analysis)
   - Computes variance estimates for δH₁ uncertainty quantification
   - Uses U-statistics and conditional variance techniques

### Current E/U Batch Sampling Pattern

**Current Approach:**
- E batch: B_E distinct prompts × G responses = B_E × G sequences (for entropy estimation)
- U batch: B_U distinct prompts × G responses = B_U × G sequences (for gradient updates)
- Both use `sample_batch()` which samples distinct prompts without replacement

**Limitations:**
1. No prompt replacement - limits statistical properties
2. Custom generation logic duplicates SequenceProcessor functionality
3. No integration with modern batching optimizations
4. Cannot easily scale to large N (e.g., 32K sequences) from limited prompt pools

## Target Architecture: Integration with SequenceProcessor

### New E Batch Sampling Strategy

**Replacement Sampling for E Batch:**
- Sample B_E prompts WITH replacement from dataset
- Generate 1 response per sampled prompt (G=1 for E batch)
- If prompt appears N times → N different responses to that prompt
- Uses `SequenceProcessor.generate_with_replacement_sampling()`

**Rationale:**
1. True independent sampling from D(p)π(t|p) distribution
2. Can generate large numbers of sequences (32K+) from smaller prompt pools
3. Better statistical properties for entropy estimation
4. Leverages optimized SequenceProcessor infrastructure

**U Batch Strategy (Unchanged):**
- Keep existing approach: B_U distinct prompts × G responses
- Used for gradient updates, doesn't need replacement sampling
- Will use `SequenceProcessor.generate_with_logprobs()` with explicit prompts

## Implementation Plan

### Phase 1: SequenceProcessor Integration Foundation

#### Step 1.1: Update probe_components.py
**Goal:** Replace custom generation with SequenceProcessor calls

**Changes:**
```python
class ProbeComponents:
    def __init__(self, model, tokenizer, config, logger):
        # Add SequenceProcessor initialization
        from sequence_processing import SequenceProcessor, GenerationConfig
        
        self.sp_config = GenerationConfig(
            temperature=config['generation']['temperature'],
            top_p=config['generation']['top_p'],
            max_new_tokens=config['generation']['max_new_tokens'],
            do_sample=config['generation']['do_sample'],
            gen_batch_size=config.get('generation', {}).get('gen_batch_size', 32),
            tf_batch_size=config.get('generation', {}).get('tf_batch_size', 64)
        )
        self.sequence_processor = SequenceProcessor(model, tokenizer, self.sp_config)
    
    def sample_E_batch_with_replacement(self, total_sequences: int, seed: Optional[int] = None):
        """New method for E batch with replacement sampling"""
        return self.sequence_processor.generate_with_replacement_sampling(
            total_sequences=total_sequences,
            dataset_name=self.config['batch_config']['dataset_name'],
            split=self.config['batch_config']['split'],
            seed=seed,
            with_grad=False,  # E batch for entropy estimation only
            compute_rb=True   # Enable Rao-Blackwell entropy estimates
        )
    
    def sample_U_batch_distinct(self, B_U: int, G: int):
        """Keep existing approach for U batch"""
        # Load dataset and sample B_U distinct prompts
        dataset = DATASET_REGISTRY[self.config['batch_config']['dataset_name']]
        examples = dataset(self.config['batch_config']['split'])
        sampled_examples = random.sample(list(examples), B_U)
        prompts = [ex.question for ex in sampled_examples]
        
        return self.sequence_processor.generate_with_logprobs(
            prompts=prompts,
            G=G,
            with_grad=True,  # U batch needs gradients for updates
            compute_rb=False  # Not needed for gradient computation
        )
```

#### Step 1.2: Update offline_entropy_probe.py Integration Points
**Goal:** Modify main orchestrator to use new sampling methods

**Key Changes:**
- Update `_sample_batch()` to use replacement sampling for E batch
- Add configuration options for replacement vs distinct sampling
- Maintain backward compatibility during transition

### Phase 2: Replacement Sampling Implementation

#### Step 2.1: Configuration Updates
**Add to probe config:**
```yaml
batch_config:
  # E batch configuration (new)
  E_sampling_mode: "replacement"  # or "distinct" for backward compatibility
  E_total_sequences: 4096         # Total sequences for replacement sampling
  E_batch_seed: 42                # Seed for reproducible E batch sampling
  
  # U batch configuration (existing)
  B_U: 64                         # Number of distinct prompts for U batch  
  G: 8                            # Responses per prompt (applies to U batch only)
  
  # Legacy support
  B_E_values: [512]               # Kept for backward compatibility
```

#### Step 2.2: Mixed Sampling Pipeline
**New workflow in `run_mixed_probe()`:**

```python
def run_mixed_probe(self, checkpoint_path: str = None) -> Dict[str, Any]:
    # Phase 0: Sample batches with new strategy
    if self.config['batch_config']['E_sampling_mode'] == 'replacement':
        # NEW: E batch with replacement sampling
        E_batch_data = self.probe_components.sample_E_batch_with_replacement(
            total_sequences=self.config['batch_config']['E_total_sequences'],
            seed=self.config['batch_config'].get('E_batch_seed')
        )
        # Convert SequenceProcessor output to probe_components format
        E_batch = self._convert_sp_output_to_probe_format(E_batch_data)
    else:
        # LEGACY: E batch with distinct sampling
        E_batch = self.probe_components.sample_batch(B_E, G, indices)
    
    # U batch unchanged - still uses distinct sampling
    U_batch = self.probe_components.sample_U_batch_distinct(B_U, G)
    
    # Phases 1-3: Existing δH₁ computation pipeline (unchanged)
    # ...
```

#### Step 2.3: Format Conversion Layer
**Bridge between SequenceProcessor and existing probe logic:**

```python
def _convert_sp_output_to_probe_format(self, sp_data: Tuple[BatchedSequences, LogprobResults, DiagnosticsResults]):
    """Convert SequenceProcessor output to format expected by probe components"""
    sequences, logprob_results, diagnostics = sp_data
    
    # Extract data in format expected by existing probe logic
    batch_data = {
        'sequences': sequences.sequences,              # [B, G, max_len]
        'prompt_lens': sequences.prompt_lens,          # [B] 
        'gen_lens': sequences.gen_lens,                # [B][G]
        'attention_masks': sequences.attention_masks,  # [B, G, max_len]
        'responses_text': sequences.responses_text,    # [B][G]
        'advantages': self._compute_advantages_from_logprobs(logprob_results),
        'max_lengths': self._compute_max_lengths(sequences),
        'prompt_ids': list(range(len(sequences.prompt_lens)))  # Placeholder for replacement sampling
    }
    return batch_data
```

### Phase 3: Distributed Training Considerations

#### Step 3.1: SequenceProcessor DDP Compatibility
**Assessment:** SequenceProcessor supports DDP models but currently runs on single GPU

**Required Changes:**
1. **Multi-GPU Generation:** SequenceProcessor needs distributed generation support
2. **Deterministic Sampling:** Ensure replacement sampling is consistent across ranks
3. **Memory Distribution:** Balance generation load across GPUs

**Implementation:**
```python
# In SequenceProcessor
def generate_with_replacement_sampling_distributed(self, total_sequences, rank, world_size, **kwargs):
    """Distributed version of replacement sampling"""
    # Rank-specific subset of total sequences
    local_sequences = total_sequences // world_size
    if rank < total_sequences % world_size:
        local_sequences += 1
    
    # Deterministic sampling with rank offset
    seed_offset = self.base_seed + rank if self.base_seed else None
    
    return self.generate_with_replacement_sampling(
        total_sequences=local_sequences,
        seed=seed_offset,
        **kwargs
    )
```

#### Step 3.2: Probe Distributed Coordination
**Update offline_entropy_probe.py for distributed replacement sampling:**

```python
def _sample_E_batch_distributed(self, total_sequences: int):
    """Coordinate E batch sampling across multiple GPUs"""
    if self.distributed:
        # Each rank generates subset of total sequences
        local_sequences = total_sequences // self.world_size
        if self.rank < total_sequences % self.world_size:
            local_sequences += 1
            
        # Generate local batch
        local_E_batch = self.probe_components.sample_E_batch_with_replacement(
            total_sequences=local_sequences,
            seed=self.config['batch_config']['E_batch_seed'] + self.rank
        )
        
        # All-gather results if needed for global entropy computation
        # (Most probe computations work with local data + all-reduce)
        return local_E_batch
    else:
        # Single GPU: generate full batch locally
        return self.probe_components.sample_E_batch_with_replacement(
            total_sequences=total_sequences,
            seed=self.config['batch_config']['E_batch_seed']
        )
```

### Phase 4: Testing and Validation

#### Step 4.1: Backward Compatibility Testing
1. **Legacy Mode:** All existing configs should work with `E_sampling_mode: "distinct"`
2. **Result Consistency:** δH₁ values should match between old and new implementations
3. **Distributed Parity:** Results should be identical between single-GPU and multi-GPU runs

#### Step 4.2: Replacement Sampling Validation
1. **Statistical Properties:** Verify replacement sampling gives expected statistics
2. **Large Scale Testing:** Test with 4K-32K sequences from smaller prompt pools
3. **Memory Efficiency:** Ensure large batches don't cause OOM errors

#### Step 4.3: Performance Benchmarking
**Metrics to Track:**
- Generation throughput (sequences/sec)
- Memory usage (peak GPU memory)
- E2E runtime for full probe analysis
- Scaling behavior with batch size

## Migration Strategy

### Stage 1: Foundation (Week 1)
- [ ] Integrate SequenceProcessor into probe_components.py
- [ ] Implement format conversion layer
- [ ] Add configuration options for sampling modes
- [ ] Basic testing with small batches

### Stage 2: Core Functionality (Week 2)  
- [ ] Implement replacement sampling for E batches
- [ ] Update offline_entropy_probe.py integration points
- [ ] Comprehensive testing against existing baselines
- [ ] Performance optimization

### Stage 3: Distributed Support (Week 3)
- [ ] Add distributed generation support to SequenceProcessor
- [ ] Update probe distributed coordination
- [ ] Multi-GPU testing and validation
- [ ] Memory optimization for large batches

### Stage 4: Production Ready (Week 4)
- [ ] Complete test coverage
- [ ] Documentation updates
- [ ] Performance benchmarking
- [ ] Migration of existing experiment configs

## Benefits of This Refactor

### Immediate Benefits
1. **Code Consolidation:** Eliminate duplicate generation logic across probe_components.py
2. **Modern Infrastructure:** Leverage battle-tested SequenceProcessor optimizations  
3. **Memory Efficiency:** Better batch size management and GPU utilization
4. **Replacement Sampling:** Enable true independent sampling for better statistics

### Long-Term Benefits
1. **Scalability:** Support for large-scale experiments (32K+ sequences)
2. **Maintainability:** Single source of truth for generation logic
3. **Feature Parity:** Automatic access to SequenceProcessor improvements
4. **Flexibility:** Easy to experiment with different sampling strategies

## Risk Mitigation

### Potential Risks
1. **Breaking Changes:** Existing experiment configs may need updates
2. **Performance Regression:** New infrastructure might be slower initially
3. **Distributed Complexity:** Multi-GPU coordination adds complexity
4. **Statistical Changes:** Replacement sampling may change baseline results

### Mitigation Strategies  
1. **Backward Compatibility:** Maintain legacy sampling mode during transition
2. **Gradual Migration:** Phase rollout with extensive testing at each stage
3. **Performance Monitoring:** Benchmark at each step to catch regressions
4. **Comprehensive Testing:** Validate all existing use cases before deprecating old code

## Conclusion

This refactor will modernize the entropy experiments infrastructure while adding powerful new capabilities for large-scale statistical analysis. The phased approach ensures stability while enabling significant improvements in scalability and maintainability.

The key insight is leveraging SequenceProcessor's replacement sampling capability to achieve true independent sampling from D(p)π(t|p), which will enable more robust entropy variance analysis and support for much larger experimental scales.