Executive summary

What your â€œlinearâ€ estimator actually differentiates.
In DeltaEntropyApprox the JVP path forms

ğ‘”
^
â‹…
ğ‘£
â€…â€Š
=
â€…â€Š
1
ğ‘‡
tot
[
âˆ‘
ğ‘–
âˆ‘
ğ‘˜
(
ğº
ğ‘–
,
ğ‘˜
âˆ’
ğ‘
ğ‘–
,
ğ‘˜
)
â€‰
j
l
o
g
p
ğ‘–
,
ğ‘˜
]
â€…â€Š
+
â€…â€Š
1
ğ‘‡
tot
âˆ‘
ğ‘–
j
ğ»
ğ‘–
,
g
^
	â€‹

â‹…v=
T
tot
	â€‹

1
	â€‹

[
i
âˆ‘
	â€‹

k
âˆ‘
	â€‹

(G
i,k
	â€‹

âˆ’b
i,k
	â€‹

)jlogp
i,k
	â€‹

]+
T
tot
	â€‹

1
	â€‹

i
âˆ‘
	â€‹

jH
i
	â€‹

,

i.e. the directional derivative of the numerator 
âˆ‘
ğ‘–
ğ»
ğ‘–
(
ğœƒ
)
âˆ‘
i
	â€‹

H
i
	â€‹

(Î¸) divided by the fixed token count 
ğ‘‡
tot
T
tot
	â€‹

 of the Eâ€‘batch. This is exactly what the code computes: the microbatch contribution is

(w_cat * j_logp_cat).sum() + j_H_sum


and then it is scaled by 1/T_total (perâ€‘token normalization) before accumulation. 

delta_entropy_approx

 

sequence_processor


The audit in your results.json confirms scale = 1/T_total (here, 
1
/
53,966
1/53,966). 

results

What your â€œtrueâ€ Î”H differentiates.
In DeltaEntropyTrue you estimate

ğ»
(
ğœƒ
+
ğœ‚
ğ‘£
)
â€…â€Š
=
â€…â€Š
âˆ‘
ğ‘–
ğ‘¤
ğ‘–
(
ğœ‚
)
â€‰
ğ»
ğ‘–
(
ğœƒ
+
ğœ‚
ğ‘£
)
âˆ‘
ğ‘–
ğ‘¤
ğ‘–
(
ğœ‚
)
â€‰
ğ‘‡
ğ‘–
withÂ SNISÂ weightsÂ 
ğ‘¤
ğ‘–
(
ğœ‚
)
=
exp
â¡
(
â„“
ğ‘–
(
ğœ‚
)
)
,
H(Î¸+Î·v)=
âˆ‘
i
	â€‹

w
i
	â€‹

(Î·)T
i
	â€‹

âˆ‘
i
	â€‹

w
i
	â€‹

(Î·)H
i
	â€‹

(Î¸+Î·v)
	â€‹

withÂ SNISÂ weightsÂ w
i
	â€‹

(Î·)=exp(â„“
i
	â€‹

(Î·)),

i.e., a ratio of means over sequences, with the denominator 
âˆ‘
ğ‘–
ğ‘¤
ğ‘–
(
ğœ‚
)
ğ‘‡
ğ‘–
âˆ‘
i
	â€‹

w
i
	â€‹

(Î·)T
i
	â€‹

 depending on 
ğœ‚
Î·. This is explicit in the SNIS description and the helper notes you included for reconstructing perâ€‘sequence contributions: the estimator uses the denominator 
(
ğ‘‡
â‹…
ğ‘¤
)
(Tâ‹…w). 

delta_entropy_true

 

results_json_guide

The missing term.
Linearizing the ratio at 
ğœ‚
=
0
Î·=0 gives

ğ‘‘
ğ‘‘
ğœ‚
ğ»
âˆ£
ğœ‚
=
0
â€…â€Š
=
â€…â€Š
1
ğ‘‡
tot
[
âˆ‘
ğ‘–
ğ‘‘
â„“
ğ‘–
ğ‘‘
ğœ‚
âŸ
âˆ‘
ğ‘˜
j
l
o
g
p
ğ‘–
,
ğ‘˜
â€‰
ğ»
ğ‘–
â€…â€Š
+
â€…â€Š
âˆ‘
ğ‘–
j
ğ»
ğ‘–
]
âŸ
whatÂ yourÂ JVPÂ computes
â€…â€Š
âˆ’
â€…â€Š
ğ»
base
ğ‘‡
tot
âˆ‘
ğ‘–
ğ‘‡
ğ‘–
ğ‘‘
â„“
ğ‘–
ğ‘‘
ğœ‚
âŸ
**denominatorÂ correction**Â thatÂ isÂ **not**Â inÂ yourÂ JVP
.
dÎ·
d
	â€‹

H
	â€‹

Î·=0
	â€‹

=
whatÂ yourÂ JVPÂ computes
T
tot
	â€‹

1
	â€‹

[
i
âˆ‘
	â€‹

âˆ‘
k
	â€‹

jlogp
i,k
	â€‹

dÎ·
dâ„“
i
	â€‹

	â€‹

	â€‹

	â€‹

H
i
	â€‹

+
i
âˆ‘
	â€‹

jH
i
	â€‹

]
	â€‹

	â€‹

âˆ’
**denominatorÂ correction**Â thatÂ isÂ **not**Â inÂ yourÂ JVP
T
tot
	â€‹

H
base
	â€‹

	â€‹

i
âˆ‘
	â€‹

T
i
	â€‹

dÎ·
dâ„“
i
	â€‹

	â€‹

	â€‹

	â€‹

.

Your JVP implements the first bracketed term (scoreâ€‘function piece with RB returns + pathwise RB derivative) but omits the denominator correction 
âˆ’
ğ»
base
ğ‘‡
tot
âˆ‘
ğ‘–
ğ‘‡
ğ‘–
â€‰
j
s
e
q
_
l
o
g
p
ğ‘–
âˆ’
T
tot
	â€‹

H
base
	â€‹

	â€‹

âˆ‘
i
	â€‹

T
i
	â€‹

jseq_logp
i
	â€‹

. The code never applies this correction anywhere in DeltaEntropyApprox. 

delta_entropy_approx

 

sequence_processor

Because 
ğ»
base
â‰ˆ
0.5
H
base
	â€‹

â‰ˆ0.5 here and because 
ğ‘‡
ğ‘–
T
i
	â€‹

 varies a fair bit across sequences, the correction can be nonâ€‘negligible and typically has the same sign as the measured Î”H, making the true slope larger in magnitude than your linear prediction â€” exactly your observation (ratio 
âˆ£
Î”
ğ»
true
âˆ£
/
âˆ£
Î”
ğ»
lin
âˆ£
âˆ¼
1
â€‰â£
âˆ’
â€‰â£
10
âˆ£Î”H
true
	â€‹

âˆ£/âˆ£Î”H
lin
	â€‹

âˆ£âˆ¼1âˆ’10). Your own run shows 
Î”
ğ»
lin
/
ğœ‚
â‰ˆ
âˆ’
208.85
Î”H
lin
	â€‹

/Î·â‰ˆâˆ’208.85 per token, while 
Î”
ğ»
true
/
ğœ‚
Î”H
true
	â€‹

/Î· at 
8
â‹…
10
âˆ’
7
8â‹…10
âˆ’7
 is about 
âˆ’
386
âˆ’386 per token â€” a factor 
â‰ˆ
1.85
â‰ˆ1.85 gap in the expected direction. 

results

 

results

Why the histograms differ.
The perâ€‘sequence contributions for the true Î”H include the SNIS weighting and, crucially, the Tâ€‘weighted denominator term. This introduces a lengthâ€‘dependent reweighting that (i) sharpens mass near 0 for many sequences (most have small weights) yet (ii) yields heavier tails when 
ğ‘‡
ğ‘–
T
i
	â€‹

 and 
j
s
e
q
_
l
o
g
p
ğ‘–
jseq_logp
i
	â€‹

 correlate â€” matching your â€œsharper center with heavier tailsâ€ description. The guide you attached shows the correct way to compute perâ€‘sequence SNIS contributions that sum exactly to Î”H, and its structure already reveals the two ingredients that are absent from â€œrawâ€ linear terms. 

results_json_guide

Concrete fix: add the denominator term to the JVP estimator

What to add. For perâ€‘token normalization, correct the linear term by

(
âˆ‡
ğ»
â‹…
ğ‘£
)
corr
â€…â€Š
=
â€…â€Š
(
âˆ‡
ğ»
â‹…
ğ‘£
)
current
â€…â€Š
âˆ’
â€…â€Š
ğ»
base
ğ‘‡
tot
âˆ‘
ğ‘–
ğ‘‡
ğ‘–
âˆ‘
ğ‘˜
j
l
o
g
p
ğ‘–
,
ğ‘˜
âŸ
j
s
e
q
_
l
o
g
p
ğ‘–
.
(âˆ‡Hâ‹…v)
corr
	â€‹

=(âˆ‡Hâ‹…v)
current
	â€‹

âˆ’
T
tot
	â€‹

H
base
	â€‹

	â€‹

i
âˆ‘
	â€‹

T
i
	â€‹

jseq_logp
i
	â€‹

k
âˆ‘
	â€‹

jlogp
i,k
	â€‹

	â€‹

	â€‹

.

Where. In DeltaEntropyApprox, inside the microbatch loop where you already have j_logp_cat (per-token) and T_list (perâ€‘sequence), compute perâ€‘sequence sums of j_logp and accumulate the correction. The scaling must use the same derivative scaling constant (1/T_total for the perâ€‘token objective). The base perâ€‘token RB entropy 
ğ»
base
H
base
	â€‹

 can be computed once with a single noâ€‘grad TF pass on the same Eâ€‘batch (exactly what DeltaEntropyTrue._score_batch_base does). 

delta_entropy_true

Minimal patch (illustrative):

# In DeltaEntropyApprox.__init__: cache a base entropy helper
self._H_base_cache = {}

def _compute_H_base_mean(self, E_batch):
    key = id(E_batch)
    if key in self._H_base_cache:
        return self._H_base_cache[key]
    # No-grad TF to get per-seq RB sums and token counts
    seqs = BatchedSequences(
        sequences=E_batch["sequences"],
        prompt_lens=E_batch["prompt_lens"],
        gen_lens=E_batch["gen_lens"],
        attention_masks=E_batch["attention_masks"],
        responses_text=[]
    )
    lp, _ = self.sp.teacher_force_logprobs_with_diagnostics(
        sequences=seqs, with_grad=False, tf_batch_size=1,
        compute_rb=True, return_baseline_features=False,
        params_override=None, buffers_override=None,
    )
    # Flatten and compute per-token mean H_base
    rb_sums = [np.asarray(x).sum() for row in lp.rb_entropies for x in row]
    T_list = [len(x) for row in lp.rb_entropies for x in row]
    H_base = (float(sum(rb_sums)) / max(float(sum(T_list)), 1.0)) if sum(T_list) > 0 else 0.0
    self._H_base_cache[key] = H_base
    return H_base


Then in the JVP microbatch path (the same block that now has mb_contrib = ((w_cat * j_logp_cat).sum() + j_H_sum); cf. 

delta_entropy_approx

), insert:

H_base = self._compute_H_base_mean(E_batch)            # once per run; cached
# compute per-sequence sums of j_logp
offsets, acc = [], 0
for b in range(B_mb):
    if T_list[b] > 0:
        offsets.append((b, acc, acc + T_list[b]))
        acc += T_list[b]
j_seqlogp = []
for b, s, e in offsets:
    j_seqlogp.append(j_logp_cat[s:e].sum())            # scalar tensor

# denominator correction for this microbatch (still unscaled)
denom_corr_mb = - H_base * sum( float(T_list[b]) * float(j_seqlogp[idx].item())
                                for idx, (b, _, _) in enumerate(offsets) )

# apply the same derivative scaling (per-token: 1 / T_total)
scale = self._scale_for_derivative(B_total, T_total)
mb_contrib = float(((w_cat.to(j_logp_cat) * j_logp_cat).sum() + j_H_sum).item())
contribs_mb.append( (mb_contrib + denom_corr_mb) * float(scale) )


With this change, the reported "delta_h_per_lr" becomes the true derivative of the ratio-of-means objective your SNIS target is estimating, so 
ğœ‚
â€‰
(
âˆ‡
ğ»
â‹…
ğ‘£
)
corr
Î·(âˆ‡Hâ‹…v)
corr
	â€‹

 should line up with your measured 
Î”
ğ»
true
(
ğœ‚
)
Î”H
true
	â€‹

(Î·) for small 
ğœ‚
Î·.

Implementation notes

You already have all the ingredients on the JVP path; the only extra work is the one noâ€‘grad TF pass to get 
ğ»
base
H
base
	â€‹

. That pass is cheap (one forward per sequence; no RB grads).

The correction should also be applied in the nested JVP used for curvature if you rely on those diagnostics. The place is analogous (the block that builds gdotv_mb before vHvv_mb). 

delta_entropy_approx

 

delta_entropy_approx