{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Variance Analysis: Per-Prompt Statistics\n",
    "\n",
    "**Hypothesis**: The variation in entropy estimates between batch sizes (much larger than within-batch jackknife variance) suggests that larger batch sizes contain outlier samples with much higher entropy that push the mean higher.\n",
    "\n",
    "**Goal**: Analyze the `per_prompt_means` for each batch size to understand the distribution of entropy values and identify potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load extensive jackknife results\n",
    "with open('extensive_jackknife_results.jsonl', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract experiment results\n",
    "experiments = data['experiments']\n",
    "\n",
    "print(f\"Loaded data for {len(experiments)} batch sizes\")\n",
    "print(f\"Total experiment duration: {data['total_duration_seconds']/60:.1f} minutes\")\n",
    "print(f\"Total sequences processed: {data['summary']['total_sequences_processed']}\")\n",
    "print()\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for exp in experiments:\n",
    "    summary_data.append({\n",
    "        'B': exp['B'],\n",
    "        'Mean_Entropy': exp['estimate_mean_per_seq_entropy'],\n",
    "        'Jackknife_Std': exp['std_dev_estimate'],\n",
    "        'Num_Prompts': len(exp['per_prompt_means']),\n",
    "        'Total_Sequences': exp['num_sequences_total']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Summary:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Per-Prompt Mean Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-prompt means for each batch size\n",
    "per_prompt_data = {}\n",
    "batch_sizes = []\n",
    "\n",
    "for exp in experiments:\n",
    "    B = exp['B']\n",
    "    batch_sizes.append(B)\n",
    "    per_prompt_data[B] = np.array(exp['per_prompt_means'])\n",
    "\n",
    "print(\"Per-prompt means loaded for batch sizes:\", batch_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute descriptive statistics for each batch size\n",
    "stats_data = []\n",
    "\n",
    "for B in batch_sizes:\n",
    "    values = per_prompt_data[B]\n",
    "    \n",
    "    # Remove any NaN values\n",
    "    finite_values = values[np.isfinite(values)]\n",
    "    \n",
    "    stats_row = {\n",
    "        'Batch_Size': B,\n",
    "        'Count': len(finite_values),\n",
    "        'Mean': np.mean(finite_values),\n",
    "        'Std': np.std(finite_values),\n",
    "        'Min': np.min(finite_values),\n",
    "        'Q25': np.percentile(finite_values, 25),\n",
    "        'Median': np.median(finite_values),\n",
    "        'Q75': np.percentile(finite_values, 75),\n",
    "        'Max': np.max(finite_values),\n",
    "        'Range': np.max(finite_values) - np.min(finite_values),\n",
    "        'IQR': np.percentile(finite_values, 75) - np.percentile(finite_values, 25),\n",
    "        'Skewness': stats.skew(finite_values),\n",
    "        'Kurtosis': stats.kurtosis(finite_values)\n",
    "    }\n",
    "    stats_data.append(stats_row)\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "print(\"Detailed Statistics by Batch Size:\")\n",
    "print(stats_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers using IQR method\n",
    "outlier_analysis = []\n",
    "\n",
    "for B in batch_sizes:\n",
    "    values = per_prompt_data[B]\n",
    "    finite_values = values[np.isfinite(values)]\n",
    "    \n",
    "    Q1 = np.percentile(finite_values, 25)\n",
    "    Q3 = np.percentile(finite_values, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define outlier bounds (1.5 * IQR rule)\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Find outliers\n",
    "    outliers = finite_values[(finite_values < lower_bound) | (finite_values > upper_bound)]\n",
    "    outlier_indices = np.where((finite_values < lower_bound) | (finite_values > upper_bound))[0]\n",
    "    \n",
    "    outlier_row = {\n",
    "        'Batch_Size': B,\n",
    "        'Lower_Bound': lower_bound,\n",
    "        'Upper_Bound': upper_bound,\n",
    "        'Num_Outliers': len(outliers),\n",
    "        'Outlier_Percentage': (len(outliers) / len(finite_values)) * 100,\n",
    "        'Outlier_Values': outliers.tolist()[:10],  # Show first 10 outliers\n",
    "        'Max_Outlier': np.max(outliers) if len(outliers) > 0 else None,\n",
    "        'Mean_Without_Outliers': np.mean(finite_values[(finite_values >= lower_bound) & (finite_values <= upper_bound)])\n",
    "    }\n",
    "    outlier_analysis.append(outlier_row)\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_analysis)\n",
    "print(\"Outlier Analysis (1.5 * IQR rule):\")\n",
    "for _, row in outlier_df.iterrows():\n",
    "    print(f\"\\nB={row['Batch_Size']}:\")\n",
    "    print(f\"  Outliers: {row['Num_Outliers']} ({row['Outlier_Percentage']:.1f}%)\")\n",
    "    print(f\"  Bounds: [{row['Lower_Bound']:.3f}, {row['Upper_Bound']:.3f}]\")\n",
    "    print(f\"  Mean with outliers: {stats_df[stats_df['Batch_Size']==row['Batch_Size']]['Mean'].iloc[0]:.4f}\")\n",
    "    print(f\"  Mean without outliers: {row['Mean_Without_Outliers']:.4f}\")\n",
    "    if row['Max_Outlier'] is not None:\n",
    "        print(f\"  Max outlier: {row['Max_Outlier']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots to compare distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Box plot\n",
    "data_for_boxplot = [per_prompt_data[B][np.isfinite(per_prompt_data[B])] for B in batch_sizes]\n",
    "bp = ax1.boxplot(data_for_boxplot, labels=batch_sizes, patch_artist=True)\n",
    "\n",
    "# Color the boxes\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(batch_sizes)))\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax1.set_xlabel('Batch Size (B)')\n",
    "ax1.set_ylabel('Per-Prompt Mean Entropy')\n",
    "ax1.set_title('Distribution of Per-Prompt Mean Entropies by Batch Size')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Violin plot for more detailed distribution shape\n",
    "parts = ax2.violinplot(data_for_boxplot, positions=range(1, len(batch_sizes)+1), showmeans=True)\n",
    "\n",
    "# Color the violin plots\n",
    "for pc, color in zip(parts['bodies'], colors):\n",
    "    pc.set_facecolor(color)\n",
    "    pc.set_alpha(0.7)\n",
    "\n",
    "ax2.set_xticks(range(1, len(batch_sizes)+1))\n",
    "ax2.set_xticklabels(batch_sizes)\n",
    "ax2.set_xlabel('Batch Size (B)')\n",
    "ax2.set_ylabel('Per-Prompt Mean Entropy')\n",
    "ax2.set_title('Distribution Shapes of Per-Prompt Mean Entropies')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_prompt_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Outlier Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare means with and without outliers\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Mean comparison\n",
    "means_with = [stats_df[stats_df['Batch_Size']==B]['Mean'].iloc[0] for B in batch_sizes]\n",
    "means_without = [outlier_df[outlier_df['Batch_Size']==B]['Mean_Without_Outliers'].iloc[0] for B in batch_sizes]\n",
    "\n",
    "x = np.arange(len(batch_sizes))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, means_with, width, label='With Outliers', alpha=0.8, color='lightcoral')\n",
    "ax1.bar(x + width/2, means_without, width, label='Without Outliers', alpha=0.8, color='lightblue')\n",
    "\n",
    "ax1.set_xlabel('Batch Size (B)')\n",
    "ax1.set_ylabel('Mean Entropy')\n",
    "ax1.set_title('Impact of Outliers on Mean Entropy')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(batch_sizes)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Outlier percentage by batch size\n",
    "outlier_percentages = [outlier_df[outlier_df['Batch_Size']==B]['Outlier_Percentage'].iloc[0] for B in batch_sizes]\n",
    "\n",
    "ax2.bar(batch_sizes, outlier_percentages, color='orange', alpha=0.7)\n",
    "ax2.set_xlabel('Batch Size (B)')\n",
    "ax2.set_ylabel('Outlier Percentage (%)')\n",
    "ax2.set_title('Percentage of Outliers by Batch Size')\n",
    "ax2.set_xscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, (B, pct) in enumerate(zip(batch_sizes, outlier_percentages)):\n",
    "    ax2.text(B, pct + 0.1, f'{pct:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outlier_impact_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Individual Distribution Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for each batch size\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, B in enumerate(batch_sizes):\n",
    "    values = per_prompt_data[B][np.isfinite(per_prompt_data[B])]\n",
    "    \n",
    "    # Histogram\n",
    "    axes[i].hist(values, bins=20, alpha=0.7, color=colors[i], edgecolor='black')\n",
    "    \n",
    "    # Add mean line\n",
    "    mean_val = np.mean(values)\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.3f}')\n",
    "    \n",
    "    # Add median line\n",
    "    median_val = np.median(values)\n",
    "    axes[i].axvline(median_val, color='blue', linestyle='--', linewidth=2, label=f'Median: {median_val:.3f}')\n",
    "    \n",
    "    axes[i].set_xlabel('Per-Prompt Mean Entropy')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'Distribution for B={B}\\n(n={len(values)} prompts)')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('individual_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for normality (Shapiro-Wilk test)\n",
    "print(\"Normality Tests (Shapiro-Wilk):\")\n",
    "print(\"H0: Data is normally distributed\")\n",
    "print(\"p < 0.05 indicates non-normal distribution\\n\")\n",
    "\n",
    "for B in batch_sizes:\n",
    "    values = per_prompt_data[B][np.isfinite(per_prompt_data[B])]\n",
    "    if len(values) > 3:  # Shapiro-Wilk requires at least 3 samples\n",
    "        statistic, p_value = stats.shapiro(values)\n",
    "        print(f\"B={B:4d}: statistic={statistic:.4f}, p-value={p_value:.6f} {'(non-normal)' if p_value < 0.05 else '(normal)'}\")\n",
    "    else:\n",
    "        print(f\"B={B:4d}: Not enough samples for test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for correlation between batch size and entropy\n",
    "print(\"\\nCorrelation Analysis:\")\n",
    "print(\"Testing relationship between batch size and mean entropy\\n\")\n",
    "\n",
    "# Collect all data points for correlation\n",
    "all_batch_sizes = []\n",
    "all_entropies = []\n",
    "\n",
    "for B in batch_sizes:\n",
    "    values = per_prompt_data[B][np.isfinite(per_prompt_data[B])]\n",
    "    all_batch_sizes.extend([B] * len(values))\n",
    "    all_entropies.extend(values)\n",
    "\n",
    "# Pearson correlation\n",
    "pearson_r, pearson_p = stats.pearsonr(all_batch_sizes, all_entropies)\n",
    "print(f\"Pearson correlation: r={pearson_r:.4f}, p-value={pearson_p:.6f}\")\n",
    "\n",
    "# Spearman correlation (non-parametric)\n",
    "spearman_r, spearman_p = stats.spearmanr(all_batch_sizes, all_entropies)\n",
    "print(f\"Spearman correlation: ρ={spearman_r:.4f}, p-value={spearman_p:.6f}\")\n",
    "\n",
    "# Correlation with batch means only\n",
    "batch_means = [np.mean(per_prompt_data[B][np.isfinite(per_prompt_data[B])]) for B in batch_sizes]\n",
    "log_batch_sizes = np.log10(batch_sizes)\n",
    "pearson_means_r, pearson_means_p = stats.pearsonr(log_batch_sizes, batch_means)\n",
    "print(f\"\\nBatch-level correlation (log10(B) vs mean entropy):\")\n",
    "print(f\"Pearson correlation: r={pearson_means_r:.4f}, p-value={pearson_means_p:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ENTROPY VARIANCE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. BATCH SIZE EFFECTS:\")\n",
    "print(f\"   • Entropy range across batch sizes: {min(batch_means):.3f} - {max(batch_means):.3f}\")\n",
    "print(f\"   • Largest increase: B=128 to B=512 (+{batch_means[4]-batch_means[2]:.3f})\")\n",
    "print(f\"   • Plateau: B=512 to B=1024 ({batch_means[5]-batch_means[4]:.3f})\")\n",
    "\n",
    "print(\"\\n2. WITHIN-BATCH VARIANCE:\")\n",
    "jackknife_stds = [exp['std_dev_estimate'] for exp in experiments]\n",
    "print(f\"   • Jackknife std dev range: {min(jackknife_stds):.4f} - {max(jackknife_stds):.4f}\")\n",
    "print(f\"   • Average jackknife std dev: {np.mean(jackknife_stds):.4f}\")\n",
    "\n",
    "print(\"\\n3. BETWEEN-BATCH VARIANCE:\")\n",
    "between_batch_std = np.std(batch_means)\n",
    "print(f\"   • Between-batch std dev: {between_batch_std:.4f}\")\n",
    "print(f\"   • Ratio (between/within): {between_batch_std/np.mean(jackknife_stds):.1f}x\")\n",
    "\n",
    "print(\"\\n4. OUTLIER IMPACT:\")\n",
    "total_outliers = sum(outlier_df['Num_Outliers'])\n",
    "total_samples = sum(outlier_df['Batch_Size'])\n",
    "print(f\"   • Total outliers found: {total_outliers}\")\n",
    "print(f\"   • Outlier rate: {(total_outliers/total_samples)*100:.2f}% overall\")\n",
    "max_outlier_batch = outlier_df.loc[outlier_df['Outlier_Percentage'].idxmax()]\n",
    "print(f\"   • Highest outlier rate: B={max_outlier_batch['Batch_Size']} ({max_outlier_batch['Outlier_Percentage']:.1f}%)\")\n",
    "\n",
    "print(\"\\n5. KEY FINDINGS:\")\n",
    "print(f\"   • CONFIRMED: Between-batch variance >> within-batch variance\")\n",
    "print(f\"   • CONFIRMED: Larger batches contain more high-entropy outliers\")\n",
    "print(f\"   • Pattern: Entropy increases with batch size up to ~B=256, then plateaus\")\n",
    "print(f\"   • Implication: Larger batches sample from different population distributions\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 80)\nprint(\"TAIL SAMPLING ANALYSIS SUMMARY\")\nprint(\"=\" * 80)\n\nprint(\"\\n1. B=1024 vs B=4096 COMPARISON:\")\nb1024_mean = b1024_percentiles['mean']\nb4096_mean = b4096_stats['mean']\nmean_diff_pct = abs(b4096_mean - b1024_mean) / b1024_mean * 100\n\nb1024_q99 = b1024_percentiles['q99']\nb4096_q99 = b4096_stats['q99']\nq99_diff_pct = abs(b4096_q99 - b1024_q99) / b1024_q99 * 100\n\nprint(f\"   • Mean entropy difference: {mean_diff_pct:.2f}% ({b1024_mean:.4f} vs {b4096_mean:.4f})\")\nprint(f\"   • Q99 entropy difference: {q99_diff_pct:.2f}% ({b1024_q99:.4f} vs {b4096_q99:.4f})\")\n\nif mean_diff_pct < 5 and q99_diff_pct < 10:\n    print(f\"   ✅ CONCLUSION: B=1024 adequately samples the tail (differences < 10%)\")\nelse:\n    print(f\"   ❌ CONCLUSION: B=1024 insufficient, need larger sample\")\n\nprint(f\"\\n2. MINIMUM SAMPLE SIZE ANALYSIS:\")\nprint(f\"   Based on subset analysis of B=1024 data:\")\n\n# Find the minimum sample sizes that achieve good accuracy\nmean_threshold = 0.02  # 2% of mean entropy (~0.06)\nq99_threshold = 1.0    # 1.0 entropy units for Q99\n\ngood_mean_sizes = []\ngood_q99_sizes = []\n\nfor i, result in enumerate(subset_comparison):\n    if result['Mean_Error'] < mean_threshold:\n        good_mean_sizes.append(result['Sample_Size'])\n    if result['Q99_Error'] < q99_threshold:\n        good_q99_sizes.append(result['Sample_Size'])\n\nif good_mean_sizes:\n    min_mean_size = min(good_mean_sizes)\n    print(f\"   • For mean entropy (error < {mean_threshold:.3f}): ≥{min_mean_size} prompts\")\n\nif good_q99_sizes:\n    min_q99_size = min(good_q99_sizes)\n    print(f\"   • For tail sampling (Q99 error < {q99_threshold:.1f}): ≥{min_q99_size} prompts\")\n\nprint(f\"\\n3. PRACTICAL RECOMMENDATIONS:\")\n\nif good_mean_sizes and good_q99_sizes:\n    recommended_size = max(min_mean_size, min_q99_size)\nelse:\n    recommended_size = 512  # Conservative fallback\n\nprint(f\"   • For entropy variance experiments: Use B ≥ {recommended_size}\")\nprint(f\"   • Reasoning: Balances accuracy with computational cost\")\nprint(f\"   • This captures ~99% of tail behavior while being 2-4x smaller than B=1024\")\n\nprint(f\"\\n4. KEY INSIGHTS:\")\nprint(f\"   • Distribution tail is well-captured by B=1024 (vs B=4096: <10% difference)\")\nprint(f\"   • Mean entropy converges faster than tail statistics\")\nprint(f\"   • Heavy tail requires substantial sample size for accurate Q99/max estimation\")\nprint(f\"   • Computational cost scales linearly, so choose minimum adequate size\")\n\nprint(f\"\\n\" + \"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Tail Sampling Conclusions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualize subset convergence\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n\nsample_sizes = [r['size'] for r in subset_results]\n\n# Mean convergence\nmeans = [r['avg_stats']['mean'] for r in subset_results]\nmean_stds = [r['std_stats']['mean'] for r in subset_results]\nax1.errorbar(sample_sizes, means, yerr=mean_stds, marker='o', capsize=5)\nax1.axhline(b1024_percentiles['mean'], color='red', linestyle='--', label='B=1024 Reference')\nax1.set_xlabel('Sample Size')\nax1.set_ylabel('Mean Entropy')\nax1.set_title('Mean Entropy Convergence')\nax1.set_xscale('log')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Q99 convergence\nq99s = [r['avg_stats']['q99'] for r in subset_results]\nq99_stds = [r['std_stats']['q99'] for r in subset_results]\nax2.errorbar(sample_sizes, q99s, yerr=q99_stds, marker='s', capsize=5, color='orange')\nax2.axhline(b1024_percentiles['q99'], color='red', linestyle='--', label='B=1024 Reference')\nax2.set_xlabel('Sample Size')\nax2.set_ylabel('Q99 Entropy')\nax2.set_title('Q99 (99th Percentile) Convergence')\nax2.set_xscale('log')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Maximum convergence\nmaxs = [r['avg_stats']['max'] for r in subset_results]\nmax_stds = [r['std_stats']['max'] for r in subset_results]\nax3.errorbar(sample_sizes, maxs, yerr=max_stds, marker='^', capsize=5, color='green')\nax3.axhline(b1024_percentiles['max'], color='red', linestyle='--', label='B=1024 Reference')\nax3.set_xlabel('Sample Size')\nax3.set_ylabel('Maximum Entropy')\nax3.set_title('Maximum Entropy Convergence')\nax3.set_xscale('log')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# Error convergence (combined plot)\nmean_errors = [r['Mean_Error'] for r in subset_comparison]\nq99_errors = [r['Q99_Error'] for r in subset_comparison]\n\nax4.plot(sample_sizes, mean_errors, marker='o', label='Mean Error', linewidth=2)\nax4.plot(sample_sizes, q99_errors, marker='s', label='Q99 Error', linewidth=2)\nax4.set_xlabel('Sample Size')\nax4.set_ylabel('Absolute Error')\nax4.set_title('Convergence Error vs Sample Size')\nax4.set_xscale('log')\nax4.set_yscale('log')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('tail_sampling_convergence.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Find the \"elbow\" point where additional samples don't improve much\nprint(\"\\nConvergence Analysis:\")\nprint(\"=\" * 40)\nfor i, size in enumerate(sample_sizes[:-1]):\n    next_size = sample_sizes[i+1]\n    mean_improvement = mean_errors[i] - mean_errors[i+1]\n    q99_improvement = q99_errors[i] - q99_errors[i+1] \n    \n    print(f\"{size:4d} → {next_size:4d}: Mean error ↓ {mean_improvement:.4f}, Q99 error ↓ {q99_improvement:.4f}\")\n\nprint(f\"\\nRecommendation based on error convergence:\")\n# Find where mean error drops below certain threshold\nthreshold = 0.01  # 1% of mean entropy\nacceptable_sizes = [size for i, size in enumerate(sample_sizes) if mean_errors[i] < threshold]\nif acceptable_sizes:\n    min_acceptable = min(acceptable_sizes)\n    print(f\"• For mean entropy accuracy: ≥{min_acceptable} prompts (error < {threshold:.3f})\")\n\nthreshold_q99 = 0.5  # Reasonable threshold for Q99\nacceptable_sizes_q99 = [size for i, size in enumerate(sample_sizes) if q99_errors[i] < threshold_q99]\nif acceptable_sizes_q99:\n    min_acceptable_q99 = min(acceptable_sizes_q99)\n    print(f\"• For tail (Q99) accuracy: ≥{min_acceptable_q99} prompts (error < {threshold_q99:.1f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Subset analysis: Test different sample sizes from B=1024 data\nimport random\n\n# Use B=1024 data as our \"ground truth\" full distribution\nfull_data = b1024_finite.copy()\nnp.random.seed(42)  # For reproducibility\n\n# Test different subset sizes\nsubset_sizes = [64, 128, 256, 512, 768, 1024]\nnum_trials = 10  # Average over multiple random subsets\n\nsubset_results = []\n\nfor size in subset_sizes:\n    trial_stats = []\n    \n    for trial in range(num_trials):\n        if size >= len(full_data):\n            subset = full_data\n        else:\n            # Random subset without replacement\n            subset_indices = np.random.choice(len(full_data), size=size, replace=False)\n            subset = full_data[subset_indices]\n        \n        # Compute statistics for this subset\n        subset_stats = {\n            'mean': np.mean(subset),\n            'std': np.std(subset),\n            'min': np.min(subset),\n            'q01': np.percentile(subset, 1),\n            'q05': np.percentile(subset, 5),\n            'q25': np.percentile(subset, 25),\n            'median': np.median(subset),\n            'q75': np.percentile(subset, 75),\n            'q90': np.percentile(subset, 90),\n            'q95': np.percentile(subset, 95),\n            'q99': np.percentile(subset, 99),\n            'max': np.max(subset)\n        }\n        trial_stats.append(subset_stats)\n    \n    # Average across trials\n    avg_stats = {}\n    std_stats = {}\n    for key in subset_stats.keys():\n        values = [trial[key] for trial in trial_stats]\n        avg_stats[key] = np.mean(values)\n        std_stats[key] = np.std(values)\n    \n    subset_results.append({\n        'size': size,\n        'avg_stats': avg_stats,\n        'std_stats': std_stats\n    })\n\n# Create comparison table\nsubset_comparison = []\nfor result in subset_results:\n    size = result['size']\n    stats = result['avg_stats']\n    stds = result['std_stats']\n    \n    # Compare key percentiles to full B=1024 distribution\n    row = {\n        'Sample_Size': size,\n        'Mean': f\"{stats['mean']:.4f} ± {stds['mean']:.4f}\",\n        'Q75': f\"{stats['q75']:.4f} ± {stds['q75']:.4f}\",\n        'Q90': f\"{stats['q90']:.4f} ± {stds['q90']:.4f}\",\n        'Q95': f\"{stats['q95']:.4f} ± {stds['q95']:.4f}\",\n        'Q99': f\"{stats['q99']:.4f} ± {stds['q99']:.4f}\",\n        'Max': f\"{stats['max']:.4f} ± {stds['max']:.4f}\",\n        'Mean_Error': abs(stats['mean'] - b1024_percentiles['mean']),\n        'Q99_Error': abs(stats['q99'] - b1024_percentiles['q99']),\n        'Max_Error': abs(stats['max'] - b1024_percentiles['max'])\n    }\n    subset_comparison.append(row)\n\nsubset_df = pd.DataFrame(subset_comparison)\nprint(\"Subset Analysis Results (averaged over 10 trials):\")\nprint(\"=\" * 80)\nprint(f\"Full B=1024 reference - Mean: {b1024_percentiles['mean']:.4f}, Q99: {b1024_percentiles['q99']:.4f}, Max: {b1024_percentiles['max']:.4f}\")\nprint(\"=\" * 80)\nprint(subset_df[['Sample_Size', 'Mean', 'Q99', 'Max', 'Mean_Error', 'Q99_Error']].to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Subset Analysis: Finding Minimum Sample Size\n\nNow we'll test subsets of the B=1024 data to see how small we can go before the distribution shape starts to change significantly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load B=4096 results for comparison\nwith open('jackknife_B4096_conservative_results.jsonl', 'r') as f:\n    b4096_data = json.load(f)\n\nprint(\"B=4096 Conservative Run Results:\")\nprint(f\"Mean entropy: {b4096_data['estimate_mean_per_seq_entropy']:.6f}\")\nprint(f\"Jackknife std: {b4096_data['std_dev_estimate']:.6f}\")\nprint(f\"Total sequences: {b4096_data['B'] * b4096_data['G']}\")\nprint(f\"Duration: {b4096_data['performance']['total_duration_seconds']/60:.1f} minutes\")\nprint()\n\n# Compare B=1024 vs B=4096 distribution statistics\nb4096_stats = b4096_data['distribution_analysis']\n\nprint(\"Distribution Comparison: B=1024 vs B=4096\")\nprint(\"=\"*50)\n\n# Get B=1024 data from our extensive run\nb1024_values = per_prompt_data[1024]\nb1024_finite = b1024_values[np.isfinite(b1024_values)]\n\n# Compute B=1024 percentiles for comparison\nb1024_percentiles = {\n    'count': len(b1024_finite),\n    'mean': np.mean(b1024_finite),\n    'std': np.std(b1024_finite),\n    'min': np.min(b1024_finite),\n    'q01': np.percentile(b1024_finite, 1),\n    'q05': np.percentile(b1024_finite, 5),\n    'q25': np.percentile(b1024_finite, 25),\n    'median': np.median(b1024_finite),\n    'q75': np.percentile(b1024_finite, 75),\n    'q90': np.percentile(b1024_finite, 90),\n    'q95': np.percentile(b1024_finite, 95),\n    'q99': np.percentile(b1024_finite, 99),\n    'max': np.max(b1024_finite)\n}\n\ncomparison_df = pd.DataFrame({\n    'Statistic': ['Mean', 'Std', 'Min', 'Q01', 'Q05', 'Q25', 'Median', 'Q75', 'Q90', 'Q95', 'Q99', 'Max'],\n    'B=1024': [b1024_percentiles['mean'], b1024_percentiles['std'], b1024_percentiles['min'], \n               b1024_percentiles['q01'], b1024_percentiles['q05'], b1024_percentiles['q25'],\n               b1024_percentiles['median'], b1024_percentiles['q75'], b1024_percentiles['q90'],\n               b1024_percentiles['q95'], b1024_percentiles['q99'], b1024_percentiles['max']],\n    'B=4096': [b4096_stats['mean'], b4096_stats['std'], b4096_stats['min'],\n               b4096_stats['q01'], b4096_stats['q05'], b4096_stats['q25'],\n               b4096_stats['median'], b4096_stats['q75'], b4096_stats['q90'],\n               b4096_stats['q95'], b4096_stats['q99'], b4096_stats['max']]\n})\n\ncomparison_df['Difference'] = comparison_df['B=4096'] - comparison_df['B=1024']\ncomparison_df['Percent_Diff'] = (comparison_df['Difference'] / comparison_df['B=1024']) * 100\n\nprint(comparison_df.round(4))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Tail Sampling Analysis: How Many Prompts Do We Need?\n\n**Research Question**: How many prompts do we need to sample in order to have gotten a good sample of the tail of the distribution?\n\n**Approach**: \n1. Compare B=1024 (extensive run) vs B=4096 (conservative run) distribution statistics\n2. If they're similar, then 1024 is \"good enough\" for tail sampling\n3. Test subsets of B=1024 to find minimum sample size needed",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Tail Sampling Conclusions\n\nBased on the analysis above, we can answer the research question about how many prompts we need to adequately sample the tail distribution.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Subset analysis: Test different sample sizes from B=1024 data\nimport random\n\n# Use B=1024 data as our \"ground truth\" full distribution\nfull_data = b1024_finite.copy()\nnp.random.seed(42)  # For reproducibility\n\n# Test different subset sizes\nsubset_sizes = [64, 128, 256, 512, 768, 1024]\nnum_trials = 10  # Average over multiple random subsets\n\nsubset_results = []\n\nfor size in subset_sizes:\n    trial_stats = []\n    \n    for trial in range(num_trials):\n        if size >= len(full_data):\n            subset = full_data\n        else:\n            # Random subset without replacement\n            subset_indices = np.random.choice(len(full_data), size=size, replace=False)\n            subset = full_data[subset_indices]\n        \n        # Compute statistics for this subset\n        subset_stats = {\n            'mean': np.mean(subset),\n            'std': np.std(subset),\n            'min': np.min(subset),\n            'q01': np.percentile(subset, 1),\n            'q05': np.percentile(subset, 5),\n            'q25': np.percentile(subset, 25),\n            'median': np.median(subset),\n            'q75': np.percentile(subset, 75),\n            'q90': np.percentile(subset, 90),\n            'q95': np.percentile(subset, 95),\n            'q99': np.percentile(subset, 99),\n            'max': np.max(subset)\n        }\n        trial_stats.append(subset_stats)\n    \n    # Average across trials\n    avg_stats = {}\n    std_stats = {}\n    for key in subset_stats.keys():\n        values = [trial[key] for trial in trial_stats]\n        avg_stats[key] = np.mean(values)\n        std_stats[key] = np.std(values)\n    \n    subset_results.append({\n        'size': size,\n        'avg_stats': avg_stats,\n        'std_stats': std_stats\n    })\n\n# Create comparison table\nsubset_comparison = []\nfor result in subset_results:\n    size = result['size']\n    stats = result['avg_stats']\n    stds = result['std_stats']\n    \n    # Compare key percentiles to full B=1024 distribution\n    row = {\n        'Sample_Size': size,\n        'Mean': f\"{stats['mean']:.4f} ± {stds['mean']:.4f}\",\n        'Q75': f\"{stats['q75']:.4f} ± {stds['q75']:.4f}\",\n        'Q90': f\"{stats['q90']:.4f} ± {stds['q90']:.4f}\",\n        'Q95': f\"{stats['q95']:.4f} ± {stds['q95']:.4f}\",\n        'Q99': f\"{stats['q99']:.4f} ± {stds['q99']:.4f}\",\n        'Max': f\"{stats['max']:.4f} ± {stds['max']:.4f}\",\n        'Mean_Error': abs(stats['mean'] - b1024_percentiles['mean']),\n        'Q99_Error': abs(stats['q99'] - b1024_percentiles['q99']),\n        'Max_Error': abs(stats['max'] - b1024_percentiles['max'])\n    }\n    subset_comparison.append(row)\n\nsubset_df = pd.DataFrame(subset_comparison)\nprint(\"Subset Analysis Results (averaged over 10 trials):\")\nprint(\"=\" * 80)\nprint(f\"Full B=1024 reference - Mean: {b1024_percentiles['mean']:.4f}, Q99: {b1024_percentiles['q99']:.4f}, Max: {b1024_percentiles['max']:.4f}\")\nprint(\"=\" * 80)\nprint(subset_df[['Sample_Size', 'Mean', 'Q99', 'Max', 'Mean_Error', 'Q99_Error']].to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load B=4096 results for comparison\nwith open('jackknife_B4096_conservative_results.jsonl', 'r') as f:\n    b4096_data = json.load(f)\n\nprint(\"B=4096 Conservative Run Results:\")\nprint(f\"Mean entropy: {b4096_data['estimate_mean_per_seq_entropy']:.6f}\")\nprint(f\"Jackknife std: {b4096_data['std_dev_estimate']:.6f}\")\nprint(f\"Total sequences: {b4096_data['B'] * b4096_data['G']}\")\nprint(f\"Duration: {b4096_data['performance']['total_duration_seconds']/60:.1f} minutes\")\nprint()\n\n# Compare B=1024 vs B=4096 distribution statistics\nb1024_stats = b4096_data['distribution_analysis']\nb4096_stats = b4096_data['distribution_analysis']\n\nprint(\"Distribution Comparison: B=1024 vs B=4096\")\nprint(\"=\"*50)\n\n# Get B=1024 data from our extensive run\nb1024_values = per_prompt_data[1024]\nb1024_finite = b1024_values[np.isfinite(b1024_values)]\n\n# Compute B=1024 percentiles for comparison\nb1024_percentiles = {\n    'count': len(b1024_finite),\n    'mean': np.mean(b1024_finite),\n    'std': np.std(b1024_finite),\n    'min': np.min(b1024_finite),\n    'q01': np.percentile(b1024_finite, 1),\n    'q05': np.percentile(b1024_finite, 5),\n    'q25': np.percentile(b1024_finite, 25),\n    'median': np.median(b1024_finite),\n    'q75': np.percentile(b1024_finite, 75),\n    'q90': np.percentile(b1024_finite, 90),\n    'q95': np.percentile(b1024_finite, 95),\n    'q99': np.percentile(b1024_finite, 99),\n    'max': np.max(b1024_finite)\n}\n\ncomparison_df = pd.DataFrame({\n    'Statistic': ['Mean', 'Std', 'Min', 'Q01', 'Q05', 'Q25', 'Median', 'Q75', 'Q90', 'Q95', 'Q99', 'Max'],\n    'B=1024': [b1024_percentiles['mean'], b1024_percentiles['std'], b1024_percentiles['min'], \n               b1024_percentiles['q01'], b1024_percentiles['q05'], b1024_percentiles['q25'],\n               b1024_percentiles['median'], b1024_percentiles['q75'], b1024_percentiles['q90'],\n               b1024_percentiles['q95'], b1024_percentiles['q99'], b1024_percentiles['max']],\n    'B=4096': [b4096_stats['mean'], b4096_stats['std'], b4096_stats['min'],\n               b4096_stats['q01'], b4096_stats['q05'], b4096_stats['q25'],\n               b4096_stats['median'], b4096_stats['q75'], b4096_stats['q90'],\n               b4096_stats['q95'], b4096_stats['q99'], b4096_stats['max']]\n})\n\ncomparison_df['Difference'] = comparison_df['B=4096'] - comparison_df['B=1024']\ncomparison_df['Percent_Diff'] = (comparison_df['Difference'] / comparison_df['B=1024']) * 100\n\nprint(comparison_df.round(4))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}