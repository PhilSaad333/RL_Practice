Importance Sampling Rework: Align Update With Y‑Loss (GRPO, 1 Epoch)

Goal
- Redesign `importance_sampling.py` so the one‑step update on U uses the same RL objective as the probe’s Y‑loss and as `rl_training/algs/dr_grpo.py` with a single epoch (ratios≈1, no clipping). Include proper per‑prompt normalisation (divide by G·L_max(prompt)), use the same advantages A used in Y, and support gradient accumulation with strict microbatching. Ensure multi‑GPU correctness with DDP.

High‑level design
- Two‑batch ground‑truth ΔH on E:
  1) Snapshot model and optimizer state.
  2) Compute S_orig(E) (no_grad, eval, attention_mask) and optionally lengths for per‑token metrics.
  3) Perform exactly one optimizer step on U using the RL objective aligned with Y‑loss:
     - Loss_mb = − mean_over_prompts [ sum_{g,t} (A_b,g · gen_mask_b,g,t · new_logp_b,g,t) / (G · L_max_b) ]
     - Backward() per microbatch (free graph), zero_grad before each, accumulate grads across microbatches and ranks (DDP sync ON), and call optimizer.step() once.
  4) Compute S_upd(E) under updated model (no_grad, eval, attention_mask).
  5) Compute ΔH_true on E via SNIS (or PSIS later) using logw = S_upd − S_orig; reduce across ranks.
  6) Restore model (and optimizer) to pre‑update snapshot.

Config additions/changes
- Under `importance` (used by ImportanceSampler):
  - training_loss: "rl" (default) or "nll" (keep NLL for experiments only)
  - rl_grad_accum: int (default 1)  # number of microbatches to accumulate before step; we still do a single optimizer.step() per call
  - importance_microbatch_size: int (default 1)  # sequences per micro forward (existing)
  - report_per_token: bool (default false)
  - is_mode: {"snis","clip"}  # unchanged
  - clip_c: float  # unchanged

API changes in importance_sampling.py
- Replace or supplement `compute_entropy_change` with an RL‑aligned entry:
  - def entropy_change_two_batch_rl(self, E_batch: Dict[str,Any], U_batch: Dict[str,Any], optimizer: torch.optim.Optimizer, cfg: Dict[str,Any]) -> Dict[str, Any]
    - Returns: { H_orig, H_upd, deltaH_true, ESS, diagnostics, timing }
  - Keep current `entropy_change_two_batch(...)` for backward compatibility but route to `_rl_update_on_U(...)` when cfg.training_loss=="rl".

Required batch fields
- Both E_batch and U_batch must contain (already present in ProbeComponents.sample_batch):
  - sequences:   Tensor[B, G, T_total]
  - attention_masks: Tensor[B, G, T_total] (0 on padding)
  - prompt_lens: List[int] length B (or Tensor[B])
  - advantages:  Tensor[B, G] (A_b,g)
  - max_lengths: List[int] length B (L_max_b = max token count over g)
  - prompt_ids:  optional for debugging

Implementation details
1) Snapshot/restore
   - Model params: `cpu_snaps = {name: p.detach().to('cpu').clone()}` (or snapshot_device configurable)
   - Optimizer state: deep copy exp_avg, exp_avg_sq, and step to CPU (optional but recommended). If state is large, at minimum snapshot param.data and reset grads to None after restore.

2) Original entropy on E
   - Use `_eval_logprobs_on_batch(E_batch)` with no_grad, eval mode, AMP bf16.
   - Always pass `attention_mask` to model forward; compute S as sum of token logprobs over generated region.
   - Optionally compute lengths for per‑token metrics.

3) RL‑aligned update on U (one step, gradient accumulation)
   - DDP sync: ON (do NOT no_sync). We want a true distributed step.
   - Zero grads: `optimizer.zero_grad(set_to_none=True)`
   - Iterate U prompts in microbatches of size `mb_size_prompts` (same as Stage 1/2):
     - Build flat inputs for the microbatch:
       - micro_seqs: shape [mb_prompts*G, T_total]
       - micro_masks: same shape; keep as long tensor for attention_mask
       - micro_targets_tok = micro_seqs[:, 1:]
       - Compute `B_mb = mb_prompts` and gather `A_mb` and `Lmax_mb` per prompt; expand to token level later.
     - Forward: with autocast bf16, `logits = model(micro_seqs, attention_mask=micro_masks).logits`
       - Convert to float32; compute `logp_all = log_softmax(logits.float(), dim=-1)`
       - Gather next‑token `new_logp = logp_all[:, :-1].gather(-1, micro_targets_tok.unsqueeze(-1)).squeeze(-1)`
       - Reshape to [B_mb, G, T_total−1], then slice generated region: last T_g tokens via prompt_len (same as probe):
         `new_logp_gen = new_logp.view(B_mb, G, -1)[..., prompt_len-1:prompt_len-1+T_g]`
         `gen_mask = micro_masks.view(B_mb, G, -1)[..., prompt_len:]` cast to float
       - Clamp/log sanitization same as DRGRPO if desired (e.g., clamp min=-80).
     - RL loss per prompt (token‑level, GRPO normalisation):
       - For each prompt b: `loss_b = - sum_{g,t}(A_b,g * new_logp_gen[b,g,t] * gen_mask[b,g,t]) / (G * max(Lmax_b,1))`
       - Reduce to scalar loss_mb = mean_b(loss_b)
     - Gradient accumulation scaling:
       - If you want exact gradient of global mean across all B_U prompts, compute `scale = B_mb / B_U_global` and call `loss_mb * scale` before backward.
       - With DDP, B_U_global = all‑reduce sum of local B_U (or simply infer from config if deterministic partitioning in Stage 3 is used).
     - Backward now (no retain_graph), free memory per microbatch.
   - After loop: optional grad clipping (match DRGRPO), then `optimizer.step()`; zero_grad for cleanliness.

4) Updated entropy on E
   - Recompute S_upd(E) with the same `_eval_logprobs_on_batch` helper.
   - Compute log‑weights: `logw = S_upd − S_orig`.
   - SNIS: weights = exp(logw − max(logw)); H_upd = −Σ w·S_upd / Σ w; ESS = (Σ w)^2 / Σ w^2.
   - Multi‑GPU: reduce partial sums across ranks via all‑reduce.

5) Restore
   - Copy cpu_snaps back to model parameters; restore optimizer state if saved; `optimizer.zero_grad(set_to_none=True)`.

ProbeComponents compatibility and reuse
- You do not need to share forward activations between Y‑accumulation and the RL update; keep phases separate for clarity and to avoid interfering with `.grad` semantics.
- Reuse data already produced by `sample_batch`: advantages and L_max are identical to those used by Y‑loss — pass U_batch directly to the importance sampler; no recomputation needed.
- Optionally add a small helper in ProbeComponents to compute per‑prompt L_max (if not already present) and to ensure advantages are tensor on CPU; but avoid writing to .grad in ProbeComponents during this phase.

Helpers to add in importance_sampling.py
- _eval_logprobs_on_batch(batch) -> (S: Tensor[B,G], lengths: optional Tensor[B,G])
  - no_grad, eval mode, pass attention_mask, AMP bf16, compute S by summing token logp over generated region.
- _rl_update_streaming(U_batch, optimizer, grad_accum_cfg, ddp_ctx) -> None
  - Implements step 3 above: per‑micro backward with scaling to match global mean; DDP sync enabled; optional grad clip.
- _snapshot/ _restore helpers for model and optimizer state.

Multi‑GPU notes
- Use deterministic E/U partitioning from Stage 3 multi‑GPU doc or all‑reduce counts on the fly to compute `B_U_global`.
- For IS sums on E, accumulate local (sum_w, sum_wS[, sum_wL]) and reduce via all‑reduce SUM.
- Keep probe backward passes (X/Y/variance) in no_sync; but for the RL update step on U, allow DDP to sync grads (no no_sync).

Diagnostics & logging
- Log: B_U_local/global, grad_accum config, whether RL objective was used, and the normalisation.
- Entropy: report H_orig, H_upd, deltaH_true; SNIS diagnostics (ESS, weight stats). Optional per‑token versions.
- Time per phase; peak VRAM around RL update.

Acceptance criteria
- Using the same E and U as the probe, the new two‑batch ΔH_true correlates strongly with ΔH₁ (same sign typically, magnitude plausible) on small steps.
- No OOM on A100‑40GB with B_E,B_U ∈ {16,32}, G=8, mb_size_prompts=2, rl_grad_accum≥1.
- Multi‑GPU: results identical across ranks; final metrics only logged by rank 0.

Claude Implementation Checklist (copy‑paste guide)
1) importance_sampling.py: add `_eval_logprobs_on_batch`, `_snapshot_state`, `_restore_state`, `_rl_update_streaming`.
2) importance_sampling.py: implement `entropy_change_two_batch_rl(...)` that calls the helpers in order A→F.
3) Wire `entropy_change_two_batch(...)` to dispatch to RL path when `cfg_importance["training_loss"] == "rl"`.
4) Ensure all model forwards in IS paths pass `attention_mask` and use AMP bf16 + float32 log_softmax as in dr_grpo.py.
5) ProbeComponents: verify `sample_batch` populates `advantages` and `max_lengths`; if not, add them. No changes to `.grad` flows.
6) Multi‑GPU: use Stage 3 reductions for counts and SNIS sums; leave RL step with normal DDP sync.

Optional future enhancement
- Add PSIS mode after RL alignment (reuse Stage 3 PSIS doc) and per‑token reporting with lengths.

