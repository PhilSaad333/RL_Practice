PHASE 3 REFACTOR PLAN (TXT)

Goal:

Replae the current naive estimator of the entropy gradient ("X") with a Rao-Blackwellized estimator with a certain baseline


\grad_\theta H(\theta) = \underbrace{\sum_j (G_j-b_j) \grad_\thea \log \pi_\theta(Y_j|s_j)}_{score term} + \underbrace{\sum_k \grad_\tehta H_k(\theta;s_k)}_{pathwise term} 

where H_k = H(\pi(\cdot|s_k)), G_j = \sum_{k\geq j}H_k and b_j = H_j +\mu_j where \mu_j is a positionwise EMA of G_j-H_j

we keep all the Y logic and the \delta H_1 = \grad H \cdot \delta \theta dot product unchanged


0) Config additions (non-breaking)

Append to your YAML (or defaults in code):

estimator:
  x_estimator_mode: "rb_residual"         # {"naive","rb_residual"}
  baseline:
    mode: "residual_mu"                   # {"none","constant","residual_mu","regression"}
    ema_decay: 0.9                        # EMA for residual Œº_j
  rb_normalize_by_length: true            # optional, keep behavior consistent with your current normalization

generation:
  rb_requires_grad: true                  # Phase 2 enabled ‚àÇH; keep true here


Notes:

If you want to A/B with the old implementation, set x_estimator_mode: "naive".

1) sequence_processor.py ‚Äî No code changes in Phase 3

We rely on Phase-2 capabilities:

teacher_force_logprobs(..., with_grad=True, compute_rb=True)
returns token_logprobs (torch) and rb_entropies_torch (torch) aligned to generated tokens (length 
ùëá
T per response), using the same top-p/temperature policy.

2) probe_components.py ‚Äî New RB-X path, baseline state, and switch
2.1 Imports

Add at the top:

from .sequence_processor import SequenceProcessor, GenerationConfig, BatchedSequences, LogprobResults
import torch.nn.functional as F


(If these already exist from Phase 1/2, skip.)

2.2 Baseline manager (EMA residual Œº)

INSERT the following lightweight helper inside this module (top-level or as an inner class):

class BaselineState:
    """
    Maintains per-position residual means Œº_j for the baseline b_j = H_j + Œº_j.
    G_j - H_j is the target; we track an EMA per absolute step index j (0-based).
    """
    def __init__(self, ema_decay: float = 0.9, device: torch.device = torch.device("cpu")):
        self.ema_decay = float(ema_decay)
        self.mu = torch.zeros(0, device=device)  # shape [J_max]; grows as needed

    def ensure_len(self, J_needed: int):
        if self.mu.numel() < J_needed:
            pad = torch.zeros(J_needed - self.mu.numel(), device=self.mu.device)
            self.mu = torch.cat([self.mu, pad], dim=0)

    @torch.no_grad()
    def update_from_batch(self, residuals: torch.Tensor, lengths: torch.Tensor):
        """
        residuals: (B_total, T_max) padded with 0 where j >= length
        lengths:   (B_total,) number of valid steps per row
        """
        J_max = residuals.size(1)
        self.ensure_len(J_max)

        # Compute per-position batch means of residuals over valid rows
        # mask: 1 for j < length
        idx = torch.arange(J_max, device=lengths.device).unsqueeze(0)  # [1, J_max]
        mask = (idx < lengths.unsqueeze(1)).float()                    # [B, J_max]
        denom = mask.sum(dim=0).clamp_min(1.0)                         # [J_max]
        mean_resid = (residuals * mask).sum(dim=0) / denom             # [J_max]

        # EMA update Œº_j ‚Üê (1-Œ±) Œº_j + Œ± mean_resid_j
        Œ± = self.ema_decay
        self.mu[:J_max].mul_(1.0 - Œ±).add_(Œ± * mean_resid)

    def get_mu_vector(self, J: int) -> torch.Tensor:
        self.ensure_len(J)
        return self.mu[:J].detach()  # [J], no grad


Where to store it: add self._baseline_state_x to ProbeComponents.__init__, after you create the SP:

bl_cfg = self.config.get('estimator', {}).get('baseline', {})
self._baseline_state_x = BaselineState(
    ema_decay=bl_cfg.get('ema_decay', 0.9),
    device=next(self.model.parameters()).device
)


(We only maintain one baseline state for X; you can later add separate states if you want per-dataset/per-model tracking.)

2.3 Utility: build a BatchedSequences from the probe batch

SP‚Äôs TF expects a BatchedSequences object. In Phase 1 we did not preserve gen_lens; reconstruct them from attention masks and prompt_lens.

INSERT:

def _to_batched_sequences_from_probe(self, batch: Dict[str, Any]) -> BatchedSequences:
    """
    Convert probe batch dict -> BatchedSequences required by SequenceProcessor.teacher_force_logprobs.
    """
    sequences = batch['sequences']            # [B, G, T]
    attention_masks = batch['attention_masks']# [B, G, T]
    prompt_lens = batch['prompt_lens']        # List[B], left-padded prompt length
    B, G, T = sequences.shape

    # gen_len[b][g] = sum(attn_mask[b,g]) - prompt_len[b]
    sums = attention_masks.long().sum(dim=-1)              # [B, G]
    gen_lens: List[List[int]] = []
    for b in range(B):
        gl = (sums[b].cpu().tolist())
        pl = int(prompt_lens[b])
        gen_lens.append([max(0, int(x) - pl) for x in gl])

    return BatchedSequences(
        sequences=sequences,
        attention_masks=attention_masks,
        prompt_lens=prompt_lens,
        gen_lens=gen_lens,
        responses_text=None  # not needed for TF
    )

2.4 New builder for RB-X loss

INSERT:

def _build_X_loss_rb_residual(
    self,
    logprob_results: LogprobResults,
    prompt_lens: List[int],
    normalize_by_length: bool = True,
) -> torch.Tensor:
    """
    Construct the scalar loss whose gradient equals the RB entropy gradient:
        sum_j (G_j - b_j) * ‚àá log œÄ(Y_j|prefix) + sum_k ‚àá H_k(prefix)
    where b_j = H_j + Œº_j (Œº_j from BaselineState).
    Uses:
      - logprob_results.logprobs[b][g]: torch [T]
      - logprob_results.rb_entropies_torch[b][g]: torch [T]
    """
    assert logprob_results.rb_entropies_torch is not None, "rb_requires_grad=True + compute_rb=True required"

    B = len(logprob_results.logprobs)
    total_loss = torch.zeros((), device=next(self.model.parameters()).device)

    # Determine max per-seq gen length to pad positionwise residuals for EMA update
    lengths = []
    rb_list, lp_list = [], []
    for b in range(B):
        for g in range(len(logprob_results.logprobs[b])):
            lp = logprob_results.logprobs[b][g]              # [T]
            rb = logprob_results.rb_entropies_torch[b][g]    # [T]
            if lp is None or rb is None or lp.numel() == 0:
                continue
            L = rb.numel()
            lengths.append(L)
            rb_list.append(rb)
            lp_list.append(lp)
    if len(lengths) == 0:
        return total_loss
    T_max = int(max(lengths))

    # Pad per-sample tensors to T_max for baseline update bookkeeping
    padded_residuals = []
    length_tensor = []
    for rb in rb_list:
        L = rb.numel()
        G = torch.cumsum(torch.flip(rb, dims=[0]), dim=0)    # [L], reverse cumsum
        G = torch.flip(G, dims=[0])
        resid = G - rb                                       # (G_j - H_j)
        if L < T_max:
            pad = torch.zeros(T_max - L, device=rb.device)
            resid = torch.cat([resid, pad], dim=0)
        padded_residuals.append(resid.unsqueeze(0))
        length_tensor.append(L)
    padded_residuals = torch.cat(padded_residuals, dim=0)    # [N_seq, T_max]
    length_tensor = torch.tensor(length_tensor, device=padded_residuals.device, dtype=torch.long)  # [N_seq]

    # Update EMA baseline Œº_j using current batch residuals
    self._baseline_state_x.update_from_batch(padded_residuals, length_tensor)
    mu_vec = self._baseline_state_x.get_mu_vector(T_max)     # [T_max], detached

    # Assemble losses per sequence
    idx_seq = 0
    for b in range(B):
        for g in range(len(logprob_results.logprobs[b])):
            token_lp = logprob_results.logprobs[b][g]             # [T]
            Hk = logprob_results.rb_entropies_torch[b][g]         # [T]
            if token_lp is None or Hk is None or token_lp.numel() == 0:
                continue
            L = Hk.numel()

            # Returns-to-go G_j from RB entropies
            G = torch.cumsum(torch.flip(Hk, dims=[0]), dim=0)
            G = torch.flip(G, dims=[0])                           # [L]

            # Baseline b_j = H_j + Œº_j (positionwise; slice Œº to length L)
            mu = mu_vec[:L]                                       # [L]
            b_j = Hk.detach() + mu                                # detach baseline

            # Advantages for the score term
            adv = (G.detach() - b_j).detach()                     # [L]

            # --- Score term with correct sign: grad = Œ£ adv * ‚àá log œÄ ---
            score_loss = torch.dot(adv, token_lp)                 # scalar

            # --- Pathwise term: Œ£ H_k (no detach, to get ‚àÇH) ---
            pathwise_loss = Hk.sum()

            if normalize_by_length and L > 0:
                loss = (score_loss + pathwise_loss) / float(L)
            else:
                loss = score_loss + pathwise_loss

            total_loss = total_loss + loss
            idx_seq += 1

    return total_loss


Notes on signs:

We use score_loss = dot(adv, token_logprobs). Backprop of log œÄ yields 
+
+ the score.

pathwise_loss = sum(H_k) backprops 
‚àë
ùëò
‚àá
ùêª
ùëò
‚àë
k
	‚Äã

‚àáH
k
	‚Äã

.

Optional normalization by length keeps parity with your existing loss scaling.

2.5 Accumulator for 
ùëã
X: branch on x_estimator_mode

EDIT your current accumulate_sum_X(...) to call the new RB builder when configured. Minimal pattern:

def accumulate_sum_X(self, batch: Dict[str, Any]) -> None:
    mode = (self.config.get('estimator', {}) or {}).get('x_estimator_mode', 'naive')
    normalize_by_length = bool(self.config.get('estimator', {}).get('rb_normalize_by_length', True))

    # 1) Convert batch -> BatchedSequences for SP TF with grad
    bs = self._to_batched_sequences_from_probe(batch)

    # 2) Run TF with grad + RB
    logprob_results = self._sequence_processor.teacher_force_logprobs(
        sequences=bs, with_grad=True,
        tf_batch_size=self._sp_gen_config.tf_batch_size,
        compute_rb=(mode == 'rb_residual')
    )

    # 3) Build X loss (branch)
    if mode == 'rb_residual':
        loss_X = self._build_X_loss_rb_residual(
            logprob_results=logprob_results,
            prompt_lens=bs.prompt_lens,
            normalize_by_length=normalize_by_length
        )
    else:
        # Fallback to your existing naive builder (unchanged)
        loss_X = self._build_LX_from_S_naive(logprob_results, prompt_lens=bs.prompt_lens)

    # 4) Backward accumulates ‚àáH(Œ∏) into param.grad buffers
    self._zero_grad_for_sum_X()     # your existing helper
    loss_X.backward()
    self._accumulate_param_grads_into_sum_X()  # existing: read .grad into your sum buffer


(If your current accumulate_sum_X splits into microbatches, keep that structure and call the builder per microbatch. The code above reflects the single-call version; adapt to your microbatch loop accordingly.)

2.6 Placeholder: regression baseline hook

Add a stub you can later fill without changing call sites:

def _baseline_predict_regression(self, features: torch.Tensor) -> torch.Tensor:
    """
    Placeholder for a learned baseline b_j(s_j) ~ H_j + V(s_j).
    For now, returns zeros with correct shape; implement later.
    """
    return torch.zeros_like(features[..., 0])


And extend the baseline logic in _build_X_loss_rb_residual to:

bl_mode = (self.config.get('estimator', {}).get('baseline', {}) or {}).get('mode', 'residual_mu')
if bl_mode == 'residual_mu':
    mu = mu_vec[:L]
    b_j = Hk.detach() + mu
elif bl_mode == 'none':
    b_j = Hk.detach()  # Œº=0
else:
    # "constant" or "regression" can be added later; for now treat as residual_mu
    mu = mu_vec[:L]
    b_j = Hk.detach() + mu

3) offline_entropy_probe.py ‚Äî Minimal or no change

Ensure you pass through the config additions (already used inside ProbeComponents).

Optionally log which x_estimator_mode and baseline you are using at the start of run_mixed_probe(...).

4) Invariants & quick checks (temporary)

Add a short diagnostic in accumulate_sum_X after backward() when mode=="rb_residual":

# Basic guard: confirm ‚àÇH path contributes some grad signal
total_norm = 0.0
for p in self.model.parameters():
    if p.grad is not None:
        total_norm += float(p.grad.data.norm().item())
self.logger.info(f"[RB-X] grad-norm(after backward)={total_norm:.4e}")


You should see non-zero norms. For an A/B check, set rb_requires_grad=false ‚Üí the pathwise piece vanishes and norms should drop (but remain >0 due to the score term).

5) Rollback & risk

Set estimator.x_estimator_mode: "naive" to restore the old 
ùëã
X estimator.

Set generation.rb_requires_grad: false to disable the pathwise ‚àÇH component while keeping the RB score term; estimator remains unbiased.

6) Minimal test plan (15‚Äì20 minutes)

Smoke run with tiny E/U batches: verify that RB-X grad-norm is non-zero and stable across a couple of microbatches.

Sign sanity: temporarily compute both ‚Äúna√Øve X‚Äù and ‚ÄúRB-residual X‚Äù on the same eval batch and print the dot with a small random vector; the means should be close, with visibly lower variance (std over repeated draws) for RB-residual.

Baseline effect: log the scalar variance of the per-token advantages 
ùê∫
ùëó
‚àí
ùëè
ùëó
G
j
	‚Äã

‚àíb
j
	‚Äã

 before/after enabling Œº; it should drop materially.

No change in Y: confirm your 
ùëå
Y accumulator and 
ùõø
ùêª
1
Œ¥H
1
	‚Äã

 head remain bit-for-bit unless numerical noise.