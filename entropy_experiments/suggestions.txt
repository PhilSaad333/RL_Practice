A) â€œTrueâ€ side (post-update entropy) â€” bias & stability

A1. Log ESS and clipping diagnostics for every Î·.

Why: SNIS and ratio estimators are biased; bias inflates when effective sample size is low and with weight clipping.

How: For each Î· used in delta_entropy_true.py, log

ESS
=
(
âˆ‘
ğ‘–
ğ‘¤
ğ‘–
)
2
/
âˆ‘
ğ‘–
ğ‘¤
ğ‘–
2
ESS=(âˆ‘
i
	â€‹

w
i
	â€‹

)
2
/âˆ‘
i
	â€‹

w
i
2
	â€‹

 and the clipping rate (fraction of sequences where 
log
â¡
ğ‘¤
logw hit the clip).

Done when: You can plot â€œgap = Î”H_true âˆ’ (Î· Ä)â€ vs ESS and clip rate; gaps should shrink as ESSâ†‘ and clippingâ†“.

A2. Two-sided (Â±Î·) â€œtruthâ€ check at the smallest Î·.

Why: A symmetric finite difference cancels most one-sided SNIS bias.

How: With the same base sample at Î¸, compute importance weights to Î¸Â±Î·v and evaluate

ğ‘”
~
fd
(
ğœ‚
)
=
[
Î”
ğ»
true
(
+
ğœ‚
)
âˆ’
Î”
ğ»
true
(
âˆ’
ğœ‚
)
]
/
(
2
ğœ‚
)
g
~
	â€‹

fd
	â€‹

(Î·)=[Î”H
true
	â€‹

(+Î·)âˆ’Î”H
true
	â€‹

(âˆ’Î·)]/(2Î·).

Done when: 
ğ‘”
~
fd
g
~
	â€‹

fd
	â€‹

 agrees with your Ä within combined SEs.

A3. Clipping sweep at the smallest Î·.

Why: Quantify clipping-induced bias.

How: Recompute the smallest-Î· point with high clip thresholds (e.g., 20, 50, 100).

Done when: The point stabilizes as the clip is relaxed.

A4. Direct sampling spot-check.

Why: Establish a bias â€œanchor.â€

How: For one tiny Î·, actually sample at Î¸+Î·v (no IS) and compute RB entropy directly.

Done when: The direct-sample point sits between SNIS and linear prediction (or coincides with the latter if curvature is tiny).

A5. Confidence intervals for Î”H_true(Î·).

Why: Ratio (SNIS) needs a consistent SE.

How: Use a weighted bootstrap over sequences (resample pairs 
(
ğ‘¤
ğ‘–
,
ğ‘“
ğ‘–
)
(w
i
	â€‹

,f
i
	â€‹

)) to obtain 95% CIs for Î”H_true(Î·). Record ESS-aware percentile intervals.

Done when: Each Î· has a CI you can put on plots.

B) Linear/Quadratic approximation â€” variance reduction & diagnostics

B1. Turn your CV study into an unbiased, variance-reduced estimator (cross-fitted).

Why: Current CV analysis is diagnostic; to use it you need unbiasedness.

How: Add --crossfit_folds=K (e.g., 5). Split E into K folds. For fold k: fit ridge on the other Kâˆ’1 folds to get 
ğ›½
^
(
âˆ’
ğ‘˜
)
Î²
^
	â€‹

(âˆ’k)
; adjust the held-out foldâ€™s contributions:

ğ‘”
^
(
ğ‘˜
)
=
ğ‘¦
â€¾
(
ğ‘˜
)
âˆ’
(
ğ›½
^
(
âˆ’
ğ‘˜
)
)
âŠ¤
(
ğ‘§
â€¾
(
ğ‘˜
)
âˆ’
ğ‘š
ğ‘§
(
âˆ’
ğ‘˜
)
)
g
^
	â€‹

(k)
=
y
	â€‹

(k)
âˆ’(
Î²
^
	â€‹

(âˆ’k)
)
âŠ¤
(
z
(k)
âˆ’m
z
(âˆ’k)
	â€‹

).
Aggregate 
ğ‘”
^
=
1
ğ¾
âˆ‘
ğ‘˜
ğ‘”
^
(
ğ‘˜
)
g
^
	â€‹

=
K
1
	â€‹

âˆ‘
k
	â€‹

g
^
	â€‹

(k)
. Use the fold variability for SE.

Good when: Batch SE(Ä) clearly drops versus no-CV, and Ä remains numerically stable across runs.

B2. Use a primal-only CV set for production.

Why: JVP-derived features (â€œwjlogp_sumâ€, â€œjH_sumâ€) are tautological.

How: Default features: {length_log, sum_w, sum_w2 (or sum_w_abs), rb_entropy_sum, var_logp}. Standardize columns before ridge. Keep a small ridge (1e-6 to 1e-4) when using many features.

Good when: Single-feature and joint reductions (reported already) are consistent across runs; no numerical explosions in Î².

B3. Robust aggregation for heavy tails (optional but powerful).

Why: y_i has heavy tails; the sample mean is high-variance.

How: Implement median-of-means (MoM) on sequences: partition E into M groups; take the mean per group; report the median across groups as the estimator of g, with the Lugosiâ€“Mendelson SE proxy. Do the same inside each cross-fit fold.

Good when: MoM SE is materially lower than vanilla mean for the same tokens on your heaviest-tailed runs.

B4. Quadratic term quality.

Why: A noisy 
ğ‘
^
q
^
	â€‹

 makes 
ğœ‚
^
â‹†
=
2
ğ‘”
^
/
ğ‘
^
Î·
^
	â€‹

â‹†
	â€‹

=2
g
^
	â€‹

/
q
^
	â€‹

 unstable.

How: Mirror what you did for g: produce per-sequence contributions 
ğ‘§
ğ‘–
z
i
	â€‹

 for q (directional HVP path you already use), and log SE(
ğ‘
^
q
^
	â€‹

).

Good when: SE(
ğ‘
^
q
^
	â€‹

) is small enough that 
ğœ‚
^
â‹†
Î·
^
	â€‹

â‹†
	â€‹

 CIs are informative (not dominated by denominator noise).

B5. Log the right SNRs.

Why: Your visual correlation â€œlarge 
ğœ‚
^
â‹†
Î·
^
	â€‹

â‹†
	â€‹

 â‡’ good fitâ€ is SNR-driven. Make this explicit.

How: Record 
S
N
R
ğ‘”
=
âˆ£
ğ‘”
^
âˆ£
/
S
E
(
ğ‘”
^
)
SNR
g
	â€‹

=âˆ£
g
^
	â€‹

âˆ£/SE(
g
^
	â€‹

), 
S
N
R
ğ‘
=
âˆ£
ğ‘
^
âˆ£
/
S
E
(
ğ‘
^
)
SNR
q
	â€‹

=âˆ£
q
^
	â€‹

âˆ£/SE(
q
^
	â€‹

).

Good when: 
S
N
R
ğ‘”
SNR
g
	â€‹

 explains most run-to-run variation in approximation error.

C) A principled acceptance criterion (what you can put in a paper)

Define a clear a priori yardstick that does not â€œcheatâ€ by conditioning on 
ğœ‚
^
â‹†
Î·
^
	â€‹

â‹†
	â€‹

:

C1. Normalized RMSE over your Î·-grid.

Metric:

NRMSE
=
1
âˆ£
ğ¸
âˆ£
âˆ‘
ğœ‚
âˆˆ
ğ¸
(
Î”
ğ»
^
(
ğœ‚
)
âˆ’
Î”
ğ»
true
(
ğœ‚
)
)
2
/
(
max
â¡
ğœ‚
âˆ£
Î”
ğ»
true
(
ğœ‚
)
âˆ£
)
NRMSE=
âˆ£Eâˆ£
1
	â€‹

âˆ‘
Î·âˆˆE
	â€‹

(
Î”H
(Î·)âˆ’Î”H
true
	â€‹

(Î·))
2
	â€‹

/(max
Î·
	â€‹

âˆ£Î”H
true
	â€‹

(Î·)âˆ£).

Goal: median NRMSE across runs below a threshold (e.g., 10â€“20%), with interquartile ranges shown.

C2. Error bars that account for both sides.

Plot: For each Î·, show Î”H_true with bootstrap CI (A5). Overlay Î”H_lin and Î”H_lin+quad with propagated bands using SE(Ä) and SE(
ğ‘
^
q
^
	â€‹

) (delta method).

Pass criterion: Fraction of Î· points where bands overlap â‰¥ 80% across runs.

C3. Trust-region overlay (population rationale, not a filter).

Compute: 
ğœ‚
^
â‹†
Î·
^
	â€‹

â‹†
	â€‹

 and its 95% CI via the delta method; draw 
ğœ‚
/
ğœ‚
^
â‹†
,
l
o
w
Î·/
Î·
^
	â€‹

â‹†,low
	â€‹

.

Narrative: â€œWithin 0.1Ã—
ğœ‚
^
â‹†
,
l
o
w
Î·
^
	â€‹

â‹†,low
	â€‹

, linearization error is â‰¤5% by Taylor; residual discrepancy is explained by Monte-Carlo noise (bands overlap).â€

D) Small, targeted code additions

delta_entropy_true.py

Log ESS and clip rate per Î·.

Add a function to evaluate Â±Î· using the same base sample (A2).

Add weighted bootstrap CI for Î”H_true(Î·).

control_variates.py

Implement K-fold cross-fitted CV and return an adjusted Ä with SE.

Optionally add MoM aggregation for y_i (toggle via flag).

Expose 
S
N
R
ğ‘”
SNR
g
	â€‹

 and 
S
N
R
ğ‘
SNR
q
	â€‹

 in the summary (B5).

Keep the â€œall featuresâ€ explorer, but add a --feature_policy=primal_only switch for production.

Aggregator (analyze_cv_runs.py)

For each run, compute NRMSE (C1), overlap fraction (C2), and record 
S
N
R
ğ‘”
SNR
g
	â€‹

, ESS at smallest Î·, and 
ğœ‚
^
â‹†
Î·
^
	â€‹

â‹†
	â€‹

 with CI.

Produce scatter plots: NRMSE vs 
S
N
R
ğ‘”
SNR
g
	â€‹

; â€œgap at Î·_minâ€ vs ESS; 
âˆ£
ğ‘”
^
âˆ£
âˆ£
g
^
	â€‹

âˆ£ vs SE(Ä).

E) What to expect if things are working

With cross-fitted primal CVs, SE(Ä) typically drops by 2â€“4Ã— relative to baseline EMA only.

The two-sided finite difference at small Î· agrees with Ä within SE, while the one-sided SNIS point lies below both when clipping/ESS are marginal (diagnosing SNIS bias).

Across runs, NRMSE falls and correlates strongly (negatively) with 
S
N
R
ğ‘”
SNR
g
	â€‹

.

The lin+quad curve sits within the Î”H_true CI across most of the Î·-grid; deviations concentrate where ESS is low or 
ğ‘
^
q
^
	â€‹

 is noisy.

F) Suggested end-state figure

One panel per run (or an aggregated panel with ribbons):

Î”H_true(Î·) with bootstrap CI.

Î”H_lin(Î·) and Î”H_lin+quad(Î·) with propagated CIs.

A faint vertical band marking 
ğœ‚
â‰¤
0.1
Ã—
ğœ‚
^
â‹†
,
l
o
w
Î·â‰¤0.1Ã—
Î·
^
	â€‹

â‹†,low
	â€‹

.

Caption reports NRMSE, 
S
N
R
ğ‘”
SNR
g
	â€‹

, ESS(Î·_min), and 
ğœ‚
^
â‹†
Î·
^
	â€‹

â‹†
	â€‹

Â±SE.

This lets you sayâ€”without conditioning on favorable runsâ€”that â€œour linear (and lin+quad) approximations match the measured change within statistically predicted error bars over the operational Î·-range; residual discrepancies are explained by finite-sample SNIS bias at low ESS and by estimator variance, both quantified in-plot.â€

If you want, I can draft the cross-fitted CV path and the Î”H_true bootstrap in your current files; theyâ€™re both self-contained additions.