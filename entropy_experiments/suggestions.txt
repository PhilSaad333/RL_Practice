A) “True” side (post-update entropy) — bias & stability

A1. Log ESS and clipping diagnostics for every η.

Why: SNIS and ratio estimators are biased; bias inflates when effective sample size is low and with weight clipping.

How: For each η used in delta_entropy_true.py, log

ESS
=
(
∑
𝑖
𝑤
𝑖
)
2
/
∑
𝑖
𝑤
𝑖
2
ESS=(∑
i
	​

w
i
	​

)
2
/∑
i
	​

w
i
2
	​

 and the clipping rate (fraction of sequences where 
log
⁡
𝑤
logw hit the clip).

Done when: You can plot “gap = ΔH_true − (η ĝ)” vs ESS and clip rate; gaps should shrink as ESS↑ and clipping↓.

A2. Two-sided (±η) “truth” check at the smallest η.

Why: A symmetric finite difference cancels most one-sided SNIS bias.

How: With the same base sample at θ, compute importance weights to θ±ηv and evaluate

𝑔
~
fd
(
𝜂
)
=
[
Δ
𝐻
true
(
+
𝜂
)
−
Δ
𝐻
true
(
−
𝜂
)
]
/
(
2
𝜂
)
g
~
	​

fd
	​

(η)=[ΔH
true
	​

(+η)−ΔH
true
	​

(−η)]/(2η).

Done when: 
𝑔
~
fd
g
~
	​

fd
	​

 agrees with your ĝ within combined SEs.

A3. Clipping sweep at the smallest η.

Why: Quantify clipping-induced bias.

How: Recompute the smallest-η point with high clip thresholds (e.g., 20, 50, 100).

Done when: The point stabilizes as the clip is relaxed.

A4. Direct sampling spot-check.

Why: Establish a bias “anchor.”

How: For one tiny η, actually sample at θ+ηv (no IS) and compute RB entropy directly.

Done when: The direct-sample point sits between SNIS and linear prediction (or coincides with the latter if curvature is tiny).

A5. Confidence intervals for ΔH_true(η).

Why: Ratio (SNIS) needs a consistent SE.

How: Use a weighted bootstrap over sequences (resample pairs 
(
𝑤
𝑖
,
𝑓
𝑖
)
(w
i
	​

,f
i
	​

)) to obtain 95% CIs for ΔH_true(η). Record ESS-aware percentile intervals.

Done when: Each η has a CI you can put on plots.

B) Linear/Quadratic approximation — variance reduction & diagnostics

B1. Turn your CV study into an unbiased, variance-reduced estimator (cross-fitted).

Why: Current CV analysis is diagnostic; to use it you need unbiasedness.

How: Add --crossfit_folds=K (e.g., 5). Split E into K folds. For fold k: fit ridge on the other K−1 folds to get 
𝛽
^
(
−
𝑘
)
β
^
	​

(−k)
; adjust the held-out fold’s contributions:

𝑔
^
(
𝑘
)
=
𝑦
‾
(
𝑘
)
−
(
𝛽
^
(
−
𝑘
)
)
⊤
(
𝑧
‾
(
𝑘
)
−
𝑚
𝑧
(
−
𝑘
)
)
g
^
	​

(k)
=
y
	​

(k)
−(
β
^
	​

(−k)
)
⊤
(
z
(k)
−m
z
(−k)
	​

).
Aggregate 
𝑔
^
=
1
𝐾
∑
𝑘
𝑔
^
(
𝑘
)
g
^
	​

=
K
1
	​

∑
k
	​

g
^
	​

(k)
. Use the fold variability for SE.

Good when: Batch SE(ĝ) clearly drops versus no-CV, and ĝ remains numerically stable across runs.

B2. Use a primal-only CV set for production.

Why: JVP-derived features (“wjlogp_sum”, “jH_sum”) are tautological.

How: Default features: {length_log, sum_w, sum_w2 (or sum_w_abs), rb_entropy_sum, var_logp}. Standardize columns before ridge. Keep a small ridge (1e-6 to 1e-4) when using many features.

Good when: Single-feature and joint reductions (reported already) are consistent across runs; no numerical explosions in β.

B3. Robust aggregation for heavy tails (optional but powerful).

Why: y_i has heavy tails; the sample mean is high-variance.

How: Implement median-of-means (MoM) on sequences: partition E into M groups; take the mean per group; report the median across groups as the estimator of g, with the Lugosi–Mendelson SE proxy. Do the same inside each cross-fit fold.

Good when: MoM SE is materially lower than vanilla mean for the same tokens on your heaviest-tailed runs.

B4. Quadratic term quality.

Why: A noisy 
𝑞
^
q
^
	​

 makes 
𝜂
^
⋆
=
2
𝑔
^
/
𝑞
^
η
^
	​

⋆
	​

=2
g
^
	​

/
q
^
	​

 unstable.

How: Mirror what you did for g: produce per-sequence contributions 
𝑧
𝑖
z
i
	​

 for q (directional HVP path you already use), and log SE(
𝑞
^
q
^
	​

).

Good when: SE(
𝑞
^
q
^
	​

) is small enough that 
𝜂
^
⋆
η
^
	​

⋆
	​

 CIs are informative (not dominated by denominator noise).

B5. Log the right SNRs.

Why: Your visual correlation “large 
𝜂
^
⋆
η
^
	​

⋆
	​

 ⇒ good fit” is SNR-driven. Make this explicit.

How: Record 
S
N
R
𝑔
=
∣
𝑔
^
∣
/
S
E
(
𝑔
^
)
SNR
g
	​

=∣
g
^
	​

∣/SE(
g
^
	​

), 
S
N
R
𝑞
=
∣
𝑞
^
∣
/
S
E
(
𝑞
^
)
SNR
q
	​

=∣
q
^
	​

∣/SE(
q
^
	​

).

Good when: 
S
N
R
𝑔
SNR
g
	​

 explains most run-to-run variation in approximation error.

C) A principled acceptance criterion (what you can put in a paper)

Define a clear a priori yardstick that does not “cheat” by conditioning on 
𝜂
^
⋆
η
^
	​

⋆
	​

:

C1. Normalized RMSE over your η-grid.

Metric:

NRMSE
=
1
∣
𝐸
∣
∑
𝜂
∈
𝐸
(
Δ
𝐻
^
(
𝜂
)
−
Δ
𝐻
true
(
𝜂
)
)
2
/
(
max
⁡
𝜂
∣
Δ
𝐻
true
(
𝜂
)
∣
)
NRMSE=
∣E∣
1
	​

∑
η∈E
	​

(
ΔH
(η)−ΔH
true
	​

(η))
2
	​

/(max
η
	​

∣ΔH
true
	​

(η)∣).

Goal: median NRMSE across runs below a threshold (e.g., 10–20%), with interquartile ranges shown.

C2. Error bars that account for both sides.

Plot: For each η, show ΔH_true with bootstrap CI (A5). Overlay ΔH_lin and ΔH_lin+quad with propagated bands using SE(ĝ) and SE(
𝑞
^
q
^
	​

) (delta method).

Pass criterion: Fraction of η points where bands overlap ≥ 80% across runs.

C3. Trust-region overlay (population rationale, not a filter).

Compute: 
𝜂
^
⋆
η
^
	​

⋆
	​

 and its 95% CI via the delta method; draw 
𝜂
/
𝜂
^
⋆
,
l
o
w
η/
η
^
	​

⋆,low
	​

.

Narrative: “Within 0.1×
𝜂
^
⋆
,
l
o
w
η
^
	​

⋆,low
	​

, linearization error is ≤5% by Taylor; residual discrepancy is explained by Monte-Carlo noise (bands overlap).”

D) Small, targeted code additions

delta_entropy_true.py

Log ESS and clip rate per η.

Add a function to evaluate ±η using the same base sample (A2).

Add weighted bootstrap CI for ΔH_true(η).

control_variates.py

Implement K-fold cross-fitted CV and return an adjusted ĝ with SE.

Optionally add MoM aggregation for y_i (toggle via flag).

Expose 
S
N
R
𝑔
SNR
g
	​

 and 
S
N
R
𝑞
SNR
q
	​

 in the summary (B5).

Keep the “all features” explorer, but add a --feature_policy=primal_only switch for production.

Aggregator (analyze_cv_runs.py)

For each run, compute NRMSE (C1), overlap fraction (C2), and record 
S
N
R
𝑔
SNR
g
	​

, ESS at smallest η, and 
𝜂
^
⋆
η
^
	​

⋆
	​

 with CI.

Produce scatter plots: NRMSE vs 
S
N
R
𝑔
SNR
g
	​

; “gap at η_min” vs ESS; 
∣
𝑔
^
∣
∣
g
^
	​

∣ vs SE(ĝ).

E) What to expect if things are working

With cross-fitted primal CVs, SE(ĝ) typically drops by 2–4× relative to baseline EMA only.

The two-sided finite difference at small η agrees with ĝ within SE, while the one-sided SNIS point lies below both when clipping/ESS are marginal (diagnosing SNIS bias).

Across runs, NRMSE falls and correlates strongly (negatively) with 
S
N
R
𝑔
SNR
g
	​

.

The lin+quad curve sits within the ΔH_true CI across most of the η-grid; deviations concentrate where ESS is low or 
𝑞
^
q
^
	​

 is noisy.

F) Suggested end-state figure

One panel per run (or an aggregated panel with ribbons):

ΔH_true(η) with bootstrap CI.

ΔH_lin(η) and ΔH_lin+quad(η) with propagated CIs.

A faint vertical band marking 
𝜂
≤
0.1
×
𝜂
^
⋆
,
l
o
w
η≤0.1×
η
^
	​

⋆,low
	​

.

Caption reports NRMSE, 
S
N
R
𝑔
SNR
g
	​

, ESS(η_min), and 
𝜂
^
⋆
η
^
	​

⋆
	​

±SE.

This lets you say—without conditioning on favorable runs—that “our linear (and lin+quad) approximations match the measured change within statistically predicted error bars over the operational η-range; residual discrepancies are explained by finite-sample SNIS bias at low ESS and by estimator variance, both quantified in-plot.”

If you want, I can draft the cross-fitted CV path and the ΔH_true bootstrap in your current files; they’re both self-contained additions.