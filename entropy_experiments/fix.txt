Here’s a tight review + a small, safe patch set you can paste in. I’ll keep it focused on (i) LoRA setup sanity and (ii) making the \bar{s} (bars_dot) computation numerically robust and batch-size invariant.

1) LoRA setup sanity check

You are explicitly activating the adapter on the same PEFT instance used for forward passes (and verifying with an on/off delta). That is exactly what we want; e.g. in ProbeComponents._teacher_force_logprobs you resolve the PEFT wrapper and call set_adapter("default"), then log the active adapter and compare logits with/without adapters. 
 

Similarly, in the aligned/variance path you do the same checks. 
 

You added a canonical parameter registry (self._peft, self._trainable_params) intended to be the single source of truth for trainable parameters. Good. 

⚠️ What still looks risky is duplicate utilities that sometimes iterate self.model.parameters() and sometimes self._trainable_params. For example, there are two versions of the buffer/dot helpers in the file set you shared:

One version iterates all model params: for param in self.model.parameters(): (this can drift from the PEFT-trainable set and re-introduce identity mismatches), including in dot_param_buffers. 

Another (preferred) version iterates the canonical trainable set: for p in self._trainable_params: (this is what we want everywhere). 

If both versions exist in the file, it’s easy to call the wrong one by accident. Below I unify these.

2) Why bars_dot flips between exactly 0 and “too big” with batch size

Two main culprits:

(A) Parameter set drift/identity mismatches
If any of zeros_like_params/add_into_param_buffer/dot_param_buffers iterate a different set (or a different object identity) than the set that received gradients, the dictionary keys won’t line up. That can silently produce a zero dot even when gradients exist. The duplicate helpers above make this failure mode plausible. (We fix this by forcing all helpers to use self._trainable_params only.)

(B) Precision + scaling order
Right now you (i) average losses in bf16/fp16, (ii) multiply grads by the microbatch prompt count in low precision, and (iii) only then move to CPU fp32 buffers. That “divide then multiply” in reduced precision can become batch-size dependent (tiny coefficients + bf16 rounding). Moreover, your dot currently sums in fp32 and converts each per-tensor sum to a Python float using .item(), which loses opportunities to accumulate in higher precision.

3) Minimal, safe patches (copy–paste)

Paste each replacement verbatim. These do not change the math—only how we accumulate and with which parameter set/dtype.

3.1 probe_components.py — canonicalize helpers + improve precision

Replace the buffer helpers with this single, canonical implementation (make sure you have only one copy of each):

# === Buffer utilities (canonical) ===

def zeros_like_params(self,
                      dtype: torch.dtype = torch.float32,
                      device: str = "cpu") -> Dict[int, torch.Tensor]:
    """
    Create zero buffers matching the canonical trainable parameters.
    """
    param_buffers = {}
    for p in self._trainable_params:
        param_buffers[id(p)] = torch.zeros_like(p, dtype=dtype, device=device)
    return param_buffers

def add_into_param_buffer(self,
                          buf: Dict[int, torch.Tensor],
                          scale: float = 1.0) -> None:
    """
    Add current param.grad into buffer with optional scaling.
    Scaling is applied in the buffer's dtype/device (typically CPU fp32) to
    avoid bf16/fp16 roundoff when converting average->sum.
    """
    for p in self._trainable_params:
        if p.grad is None:
            continue
        pid = id(p)
        if pid not in buf:
            continue
        g = p.grad.detach()
        want = buf[pid]
        if want.device != g.device or want.dtype != g.dtype:
            g = g.to(want.device, dtype=want.dtype)
        want.add_(g, alpha=float(scale))

def scale_param_gradients(self, scale_factor: float):
    """
    (Optional) Keep for other call sites, but the probe accumulation
    should no longer depend on this; we scale inside add_into_param_buffer
    in fp32/CPU.
    """
    for p in self._trainable_params:
        if p.grad is not None:
            p.grad.data.mul_(float(scale_factor))

def dot_param_buffers(self,
                      buf_a: Dict[int, torch.Tensor],
                      buf_b: Dict[int, torch.Tensor]) -> float:
    """
    Compute ⟨buf_a, buf_b⟩ with high-precision accumulation.
    Buffers can remain fp32; we upcast on-the-fly for the product/sum.
    """
    total = torch.zeros((), dtype=torch.float64)
    for p in self._trainable_params:
        pid = id(p)
        ta = buf_a.get(pid); tb = buf_b.get(pid)
        if ta is None or tb is None:
            continue
        # High-precision multiply-and-sum to reduce batch-size sensitivity
        total += (ta.double() * tb.double()).sum()
    return float(total.item())


This removes any path that iterates self.model.parameters() for these utilities and forces double-precision dot accumulation. (You already have the canonical registry. 
)

Then change the accumulation code to scale in fp32 at the point of accumulation (not in bf16 grads):

In accumulate_sum_X (X path), replace:

# after L_X_mb.backward()
self.scale_param_gradients(mb_prompt_count)
self.add_into_param_buffer(sum_X_buf)


with:

# after L_X_mb.backward()
# Scale in CPU/fp32 during accumulation to avoid bf16 multiply
self.add_into_param_buffer(sum_X_buf, scale=float(mb_prompt_count))


(You already have the surrounding structure; only these two lines change.) 

In accumulate_sum_Y (Y path), replace:

# after L_Y_mb.backward()
self.scale_param_gradients(mb_prompt_count)
# ... preconditioner ...
self.add_into_param_buffer(sum_Y_buf)


with:

# after L_Y_mb.backward()
# ... preconditioner in-place ...
# Accumulate with fp32 scaling (average -> sum)
self.add_into_param_buffer(sum_Y_buf, scale=float(mb_prompt_count))


(Again, just the accumulation call changes.) 
 

Small, recommended tweak for the preconditioner loop
Before you divide by sqrt(exp_avg_sq), make sure the math runs in fp32:

# inside the Y preconditioner loop, right before using g and state tensors
g32 = g.to(torch.float32)
gtilde = adam_preconditioner.apply_preconditioner(g32, p)
if gtilde is None:
    gtilde = g32
# copy back (will downcast if p.grad is bf16)
g.copy_(gtilde)


(This avoids subnormal behavior if exp_avg_sq norms are ~1e-10 to 1e-8, as we observed.)

Finally, make forward passes deterministic w.r.t. dropout
You don’t need model.train() to enable gradients; autograd works in eval() just fine. The batch-size sensitivity you’re seeing can be amplified by stochastic layers:

Replace the “ensure training mode” section in your TF methods:

was_training = self.model.training
self.model.train()
try:
    ...
finally:
    self.model.train(was_training)


with:

was_training = self._peft.training  # use the same PEFT instance used for forward
self._peft.eval()                   # turn off dropout/noise
try:
    ...
finally:
    self._peft.train(was_training)


(Do this in both the main TF path and the aligned-batch TF path.) 
 

Rationale: gradients are available in eval mode as long as torch.is_grad_enabled() is True. Disabling dropout removes one common source of batch-size sensitivity.

3.2 offline_entropy_probe.py — compute bars_dot in high precision

No structural change needed—your means are FP32 CPU, which is fine—but you will now get the high-precision product automatically via the patched dot_param_buffers. This is the call site: 

If you want to be extra cautious, you may upcast the means before the dot:

mu_X = {k: v.to(torch.float32) / max(B_E_global, 1) for k, v in sum_X_buf.items()}
mu_Y = {k: v.to(torch.float32) / max(B_U_global, 1) for k, v in sum_Y_buf.items()}
bars_dot = self.probe_components.dot_param_buffers(mu_X, mu_Y)  # now sums in fp64

4) What this addresses

Zero bars_dot when batch size changes → eliminated by (i) unified param set for buffers, (ii) scaling in fp32 during accumulation, (iii) double-precision reduction for the dot, and (iv) disabling stochastic layers during TF.

Occasional “too large” bars_dot → most often double-counting/scale-order issues (avg-then-mul in reduced precision) or dropout. We now (a) avoid bf16 scaling, (b) sum in fp64, (c) turn off dropout.

5) Quick instrumentation to confirm

Before returning bars_dot, log some invariants:

nz_X = sum(int(t.abs().sum().item() > 0) for t in mu_X.values())
nz_Y = sum(int(t.abs().sum().item() > 0) for t in mu_Y.values())
self.logger.info(f"[CHECK] nonzero μ_X entries: {nz_X}, nonzero μ_Y entries: {nz_Y}")
self.logger.info(f"[CHECK] ||μ_X||₂={sum(float((t.double().pow(2)).sum().item()) for t in mu_X.values())**0.5:.6e}, "
                 f"||μ_Y||₂={sum(float((t.double().pow(2)).sum().item()) for t in mu_Y.values())**0.5:.6e}")


If either count is 0, the issue is not numerical—something upstream failed to accumulate.