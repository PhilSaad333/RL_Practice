TITLE: Make α-trick grads flow reliably (forbid inference_mode; assert graph; remove TF temperature)
FILE: probe_components.py only

SUMMARY
- Add a tiny guard to *fail fast* if the probe runs under torch.inference_mode() or torch.no_grad().
- Assert that TF logits have a grad_fn and that at least one model parameter receives a non-None grad from L.
- Remove temperature scaling from teacher-forced forwards used by the probe (sampling-only concern).
- Provide precise places to paste code.

A) Add a helper to assert a valid grad context
----------------------------------------------
Paste near the top (after imports):

    def _assert_grad_context(self, where: str):
        # Inference mode cannot be turned back on locally; never enter it for probe passes.
        if torch.is_inference_mode_enabled():
            raise RuntimeError(
                f"{where}: torch.inference_mode() is enabled. "
                "This disables autograd globally; rerun probe outside inference_mode."
            )
        if not torch.is_grad_enabled():
            raise RuntimeError(
                f"{where}: torch.no_grad() is active. "
                "Enable grad (remove no_grad) for probe computations."
            )

(Requires torch>=1.11 for torch.is_inference_mode_enabled; present in recent PyTorch.)


B) Guard all probe forwards that must carry gradients
-----------------------------------------------------
At the **very start** of each method below, insert:

    self._assert_grad_context("<METHOD NAME>")

Methods to patch:
- `_teacher_force_logprobs`
- `_compute_aligned_batch_logprobs`
- `accumulate_sum_X`
- `accumulate_sum_Y`
- `compute_conditional_variance_over_E_alpha`

Example (first line inside `_compute_aligned_batch_logprobs`):

    def _compute_aligned_batch_logprobs(self, aligned_batch: Dict[str, Any]) -> Dict[str, Any]:
        self._assert_grad_context("_compute_aligned_batch_logprobs")
        ...

Do the same for the other four methods listed above.


C) Strengthen TF path: assert logits/S link back to params
----------------------------------------------------------
Inside `_compute_aligned_batch_logprobs`, immediately after the model forward:

    with torch.amp.autocast("cuda", dtype=amp_dtype, enabled=use_amp):
        outputs = self.model(flat_sequences, attention_mask=flat_attention_masks)
        logits = outputs.logits

    # REMOVE any temperature scaling here (sampling-only). Keep TF at T=1.0.
    # temp = self.config['generation'].get('temperature', 1.0)
    # if temp != 1.0:
    #     logits = logits / temp

    # New hard assertions (keep during bring-up)
    if not logits.requires_grad or logits.grad_fn is None:
        raise RuntimeError(
            "TF logits are not connected to autograd graph "
            "(likely due to surrounding inference_mode/no_grad)."
        )

Later, just before returning, you already compute:
    S_batch = (gen_token_log_probs * gen_masks).sum(dim=2)

Add:

    if not S_batch.requires_grad or S_batch.grad_fn is None:
        raise RuntimeError("S_batch lost its grad_fn; check TF pipeline and masking.")


D) α-trick site: prove parameters receive grads from L
------------------------------------------------------
In `compute_conditional_variance_over_E_alpha` after you build `L` and before contracting with μY:

    params = [p for p in self.model.parameters() if p.requires_grad]
    g_list = torch.autograd.grad(
        L, params, create_graph=True, allow_unused=True, retain_graph=False
    )
    non_none = sum(int(gi is not None) for gi in g_list)
    if non_none == 0:
        # Extra diagnostics to pinpoint the break
        # Check a direct grad w.r.t. logits to confirm graph to forward exists:
        # (Optional: comment out if you don’t keep logits around here.)
        raise RuntimeError(
            "α-trick: No parameter received a gradient from L. "
            "Most likely the model forward happened under inference_mode/no_grad "
            "in a higher-level caller (cannot be overridden locally)."
        )

**Important:** This message is more precise than the previous generic error. It tells you the *actual cause* (outer inference/no-grad) when it happens.


E) Remove TF temperature scaling everywhere
-------------------------------------------
Search in this file for divisions like `logits = logits / temp` inside any teacher-forcing path (`_teacher_force_logprobs`, `_compute_aligned_batch_logprobs`, or other probe forwards). **Delete** them. Temperature is part of your *sampling* policy, not the teacher-forcing likelihood; keeping `T=1` avoids damping both signal and gradients.


F) Where the real bug usually lives (outside this file)
-------------------------------------------------------
Your probe methods can *enforce* a fail-fast, but they **cannot** override a parent `torch.inference_mode()` scope—the graph is already dead by then (documented behavior). You must ensure the runner doesn’t wrap probe phases in inference. Concretely:

- In your driver (`offline_entropy_probe.py` or calling script), **do not** do:
  
      with torch.inference_mode():
          results = probe.run_mixed_probe()  # WRONG

  or:
  
      @torch.inference_mode()
      def run_mixed_probe(...): ...         # WRONG

- Also avoid a broad:
  
      with torch.no_grad():
          ... probe passes ...

- Restrict `inference_mode` strictly to **response generation** (sampling) only.
  Your file already uses `with torch.inference_mode():` in *sampling* code—keep that—but **not** around teacher-forcing passes.

Add an early guard at the **top** of the public entry point you call (e.g., `run_mixed_probe`):

    if torch.is_inference_mode_enabled():
        raise RuntimeError("Probe entrypoint called under torch.inference_mode(); remove that context.")

This forces the calling code to run the probe with autograd enabled.

G) Optional: make autograd.grad stricter during bring-up
--------------------------------------------------------
Temporarily change:

    g_list = torch.autograd.grad(..., allow_unused=True, ...)

to

    g_list = torch.autograd.grad(..., allow_unused=False, ...)

so PyTorch raises immediately and points at the offending inputs if anything is still unused. Revert to `allow_unused=True` once fixed.

H) Sanity checklist (run in order)
----------------------------------
1) Confirm **no** outer `inference_mode` / `no_grad` wraps your probe call.
2) The new guards should *not* trigger; TF logits should report `requires_grad=True` and a non-None `grad_fn`.
3) The α-trick debug should report a **non-zero** “non-None parameter grads” count.
4) Your `s_n` batches (from the α-trick) should now be computed without per-prompt backprops.

If any guard trips, the message tells you exactly what to remove in the outer code.

