Why this is happening (quick context)

QLoRA backpropagates through a frozen, 4-bit base into LoRA adapters; only the adapter weights carry gradients (typically lora_A, lora_B, sometimes selected biases). Gradients w.r.t. the base embeddings are supposed to be None. 
arXiv
ROCm Blogs
PyTorch Docs

PEFT/LoRA exposes adapter parameters on the wrapped PeftModel; you must forward through that same wrapper and differentiate w.r.t. those adapter tensors. 
Hugging Face
+1
GitHub

If you call torch.autograd.grad on a tensor that doesn’t require grad (e.g., frozen embeddings), PyTorch raises or returns None depending on allow_unused; this is the error you see. 
PyTorch Forums
PyTorch Docs
Stack Overflow

Minimal, targeted edits (single file: probe_components.py)

I refer to your current file; line numbers may shift. Replace the indicated blocks.

1) Replace the embedding diagnostic in _compute_aligned_batch_logprobs

Find the block that probes d(sum S[0]) / d(embedding) (the one that currently trips). Replace it with a LoRA-only probe:

# --- Robust TF probe for QLoRA/LoRA ---
# Prove S backprops into trainable LoRA params (not the frozen embeddings).
probe = S_batch[0].sum()  # scalar
lora_params = [p for n, p in self.model.named_parameters()
               if p.requires_grad and ("lora_A" in n or "lora_B" in n)]
if len(lora_params) == 0:
    raise RuntimeError("TF probe: no trainable LoRA params found (adapters missing/merged/disabled).")

g_lora = torch.autograd.grad(probe, lora_params, retain_graph=True, allow_unused=True)
if not any(g is not None for g in g_lora):
    raise RuntimeError("TF probe: S does not backprop to LoRA params. "
                       "Ensure the TF forward uses the PEFT-wrapped model and adapters are active.")
# --- end probe ---


(Delete the old emb = self.model.get_input_embeddings().weight diagnostic.)
This makes the probe conform to QLoRA’s gradient topology. 
arXiv
Hugging Face

2) In the α-trick path, test LoRA params—not embeddings

In compute_conditional_variance_over_E_alpha, remove the “Direct L→embedding test” and add:

# --- Diagnostic: does L backprop into trainable LoRA params? ---
lora_params = [p for n, p in self.model.named_parameters()
               if p.requires_grad and ("lora_A" in n or "lora_B" in n)]
if len(lora_params) == 0:
    raise RuntimeError("α-trick: no trainable LoRA params found.")

g_test = torch.autograd.grad(L, lora_params, retain_graph=True, allow_unused=True)
if not any(g is not None for g in g_test):
    raise RuntimeError("α-trick: L does not backprop into LoRA params. "
                       "If μY was built on the PEFT wrapper, make sure TF also uses the same wrapper.")
# --- end diagnostic ---


This prevents an early false failure when embeddings are frozen (QLoRA default). 
arXiv

3) Ensure the α-trick differentiates w.r.t. the same trainable params

Where you build params for the VJP step, leave the filter as “trainable only” (this picks up LoRA tensors under PEFT/DDP):

params = [p for p in self.model.parameters() if p.requires_grad]


If you want to be explicit (and to reduce work), you can restrict to LoRA only:

params = [p for n,p in self.model.named_parameters()
          if p.requires_grad and ("lora_A" in n or "lora_B" in n)]


That’s fully consistent with QLoRA, since the base is frozen. 
arXiv

Sanity checklist for the calling code (no file changes needed)

Forward through the same PEFT wrapper used to build μY and the optimizer. Do not forward on a base model or a different wrapper instance. (When using DDP, that usually means self.model(... ) where self.model is DistributedDataParallel(PeftModel(...)), not self.model.module.base_model(...).) 
Hugging Face
+1

Do not run teacher-forced forwards under torch.inference_mode() / no_grad(); that would permanently sever the graph. (It’s fine for sampling, not for the probe.) 
PyTorch Docs
Stack Overflow

Adapters active, not merged: don’t merge_and_unload() before the probe; and confirm at runtime that LoRA tensors have requires_grad=True. 
Hugging Face

Why bars_dot worked but the α-trick failed

Your bars_dot path uses .backward() and inspects param.grad on trainable (LoRA) tensors—so it succeeds.

The failing diagnostic was differentiating w.r.t. embeddings (frozen) and threw exactly the PyTorch error you’d expect when asking for grads from non-grad tensors. Once you change diagnostics and α-trick to target LoRA params, the inconsistency disappears.