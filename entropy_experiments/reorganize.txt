--- a/delta_entropy_approx.py
+++ b/delta_entropy_approx.py
@@
-from typing import Any, Dict, Generator, Iterable, List, Optional, Tuple, OrderedDict as TOrderedDict
+from typing import Any, Dict, Generator, Iterable, List, Optional, Tuple, OrderedDict as TOrderedDict
@@
 import torch
+from torch.func import jvp, functional_call  # (used in the JVP method)
+
+# Functional-call param helpers (LoRA-safe; leave import tolerant if you prefer)
+try:
+    from .param_overrides import build_functional_params_named, merge_params_and_buffers
+except Exception:
+    from param_overrides import build_functional_params_named, merge_params_and_buffers  # type: ignore
@@
 class DeltaEntropyApprox:
@@
         self.var_jackknife = bool(var_cfg.get("jackknife", True))
 
+    # ---------------------------------------------------------------------------------------------
+    # Internal helpers (logic lifted out of compute_delta_h_approx)
+    # ---------------------------------------------------------------------------------------------
+    def _gather_named_grads(self, name_to_param: "TOrderedDict[str, torch.nn.Parameter]") -> Dict[str, torch.Tensor]:
+        grads_named: Dict[str, torch.Tensor] = {}
+        for n, p in name_to_param.items():
+            if p.grad is None:
+                grads_named[n] = torch.zeros_like(p.detach()).to("cpu", torch.float32)
+            else:
+                grads_named[n] = p.grad.detach().to("cpu", torch.float32).clone()
+        return grads_named
+
+    def _variance_record_mb(
+        self,
+        name_to_param: "TOrderedDict[str, torch.nn.Parameter]",
+        v_named_cpu: Dict[str, torch.Tensor],
+        contribs: List[float],
+        last_dot_val: float,
+    ) -> float:
+        grads_named_mb = self._gather_named_grads(name_to_param)
+        dot_now = float(dot_named(grads_named_mb, v_named_cpu).item())
+        contribs.append(dot_now - last_dot_val)
+        return dot_now
+
+    def _surrogate_from_results(
+        self,
+        res: "LogprobResults",
+        *,
+        baseline_kind: str,
+        use_rb: bool,
+    ) -> Tuple[torch.Tensor, int, float, float]:
+        """
+        Build the scalar surrogate for one microbatch, with EMA update if selected.
+        Returns: (sur_scalar, T_mb, mean_b, mean_G)
+        """
+        device = next(self.model.parameters()).device
+        sur = torch.zeros((), device=device, dtype=torch.float32)
+        T_mb = 0
+        b_vals: List[float] = []
+        G_vals: List[float] = []
+
+        for b in range(len(res.logprobs)):
+            lp: torch.Tensor = res.logprobs[b][0]  # [T]
+            T = int(lp.numel())
+            if T == 0:
+                continue
+            T_mb += T
+
+            if use_rb:
+                if res.rb_entropies_torch is None or len(res.rb_entropies_torch[b]) == 0:
+                    raise RuntimeError("[delta-h approx] RB estimator active but rb_entropies_torch is missing.")
+                H_rb = res.rb_entropies_torch[b][0]  # [T]
+                if H_rb is None or H_rb.numel() != T:
+                    raise RuntimeError("[delta-h approx] RB tensor shape mismatch.")
+                # G_k = reverse cumsum of H_rb (detached)
+                G = torch.flip(torch.cumsum(torch.flip(H_rb.detach(), dims=[0]), dim=0), dims=[0])  # [T]
+
+                # Baseline b_k
+                if baseline_kind == "hk":
+                    b_k = H_rb.detach()
+                    bins = resid = None  # for EMA branch only
+                elif baseline_kind == "hk_ema":
+                    pos = torch.arange(T, device=H_rb.device, dtype=torch.float32) / max(T, 1)
+                    bins = torch.clamp((pos * self._pos_bins).long(), 0, self._pos_bins - 1)
+                    resid = (G - H_rb.detach())
+                    mu_hat = self._ema_resid[bins.cpu()]
+                    b_k = H_rb.detach() + mu_hat.to(H_rb.device)
+                elif baseline_kind in {"none"}:
+                    b_k = torch.zeros_like(G)
+                    bins = resid = None
+                else:
+                    # regression / ridge already wired higher up; fallback to Hk
+                    b_k = H_rb.detach()
+                    bins = resid = None
+
+                b_vals.append(b_k.mean().item())
+                G_vals.append(G.mean().item())
+                sur = sur + ((G - b_k) * lp).sum() + H_rb.sum()
+
+                # EMA update AFTER use
+                if baseline_kind == "hk_ema" and bins is not None and resid is not None:
+                    with torch.no_grad():
+                        for j in range(T):
+                            bb = int(bins[j].item())
+                            self._ema_cnt[bb] += 1
+                            beta = self._ema_beta
+                            self._ema_resid[bb] = (
+                                beta * self._ema_resid[bb]
+                                + (1.0 - beta) * resid[j].to(self._ema_resid.dtype).cpu()
+                            )
+            else:
+                # Simple estimator: - sum (logπ - b).detach() * logπ
+                lp_list: List[torch.Tensor] = []
+                lengths: List[int] = []
+                for bb in range(len(res.logprobs)):
+                    lp_b = res.logprobs[bb][0]
+                    lengths.append(int(lp_b.numel()))
+                    if int(lp_b.numel()) > 0:
+                        lp_list.append(lp_b)
+                baselines = self._timewise_baseline_from_logps(lp_list, kind=self.simple_baseline_kind)
+                # Accumulate
+                list_idx = 0
+                for bb in range(len(res.logprobs)):
+                    lp_b: torch.Tensor = res.logprobs[bb][0]
+                    T_b = int(lp_b.numel())
+                    if T_b == 0:
+                        continue
+                    b_b = baselines[list_idx]
+                    list_idx += 1
+                    sur = sur - ((lp_b - b_b).detach() * lp_b).sum()
+                    b_vals.append(float(b_b.mean().item()))
+                    G_vals.append(float(lp_b.detach().mean().item()))
+
+        mean_b = float(sum(b_vals) / max(len(b_vals), 1)) if b_vals else 0.0
+        mean_G = float(sum(G_vals) / max(len(G_vals), 1)) if G_vals else 0.0
+        return sur, T_mb, mean_b, mean_G
@@
     def compute_delta_h_approx(
@@
-        for mb_E in self._iter_microbatches(E_batch, self.mb):
+        for mb_E in self._iter_microbatches(E_batch, self.mb):
@@
-            res, _diag = self.sp.teacher_force_logprobs_with_diagnostics(
+            res, _diag = self.sp.teacher_force_logprobs_with_diagnostics(
                 sequences=mb_E,
                 tf_batch_size=tf_bs,
                 compute_rb=True,
                 with_grad=True,
                 return_baseline_features=want_feats,
             )
 
-            # Build surrogate for this microbatch
-            sur = torch.zeros((), device=device, dtype=torch.float32)
-            T_mb = 0
-            b_vals = []
-            G_vals = []
-
-            for b in range(len(res.logprobs)):
-                ...
-                # (large inner loop omitted here)
-                ...
+            # Build surrogate for this microbatch (lifted into helper)
+            sur, T_mb, mean_b, mean_G = self._surrogate_from_results(
+                res, baseline_kind=self.baseline_kind, use_rb=self.use_rb
+            )
 
             # Normalize this microbatch contribution
             scale = self._scale_for_average(B_total, T_total, B_mb, T_mb)
             total_tokens_used += T_mb
-            baseline_means.append(float(sum(b_vals) / max(len(b_vals), 1)) if b_vals else 0.0)
-            G_means.append(float(sum(G_vals) / max(len(G_vals), 1)) if G_vals else 0.0)
+            baseline_means.append(mean_b)
+            G_means.append(mean_G)
 
             (sur * float(scale)).backward()
 
             # --- Variance: record this microbatch's contribution to g·v (optional)
             if self.var_enabled:
-                # Gather grads for intersecting params (CPU/fp32)
-                grads_named_mb = {}
-                for n, p in name_to_param.items():
-                    if p.grad is None:
-                        grads_named_mb[n] = torch.zeros_like(p.detach()).to("cpu", torch.float32)
-                    else:
-                        grads_named_mb[n] = p.grad.detach().to("cpu", torch.float32)
-                # Current cumulative dot
-                dot_now = float(dot_named(grads_named_mb, v_named_cpu).item())
-                # Incremental contribution from this microbatch
-                contribs.append(dot_now - last_dot_val)
-                last_dot_val = dot_now
+                last_dot_val = self._variance_record_mb(
+                    name_to_param, v_named_cpu, contribs, last_dot_val
+                )
@@
-        grads_named = {}
-        for n, p in name_to_param.items():
-            if p.grad is None:
-                grads_named[n] = torch.zeros_like(p.detach()).to("cpu", torch.float32)
-            else:
-                grads_named[n] = p.grad.detach().to("cpu", torch.float32).clone()
+        grads_named = self._gather_named_grads(name_to_param)
