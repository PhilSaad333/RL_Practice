Below is a ready-to-hand-off implementation plan (TXT) to switch to Option B: perform one RL step inside Stage-2, compute and return the named-parameter update buffer from that step, and then compute 
ğ›¿
ğ»
1
Î´H
1
	â€‹

 from that very 
Î”
ğœƒ
Î”Î¸ (no second step). It also standardizes on named parameters and adds a small adapter so existing code that uses id-keyed buffers keeps working.

IMPLEMENT OPTION B: Single-Step Î”H_true + Reuse Î”Î¸ for Î´Hâ‚

Files touched:

delta_entropy_is.py (Stage-2: compute ground truth Î”H and produce Î”Î¸ buffer keyed by names)

probe_components.py (accept name-keyed buffers; add key-coercion util)

offline_entropy_probe.py (reorder run_mixed_probe to call Stage-2 once, then compute Î´Hâ‚ using returned Î”Î¸)

1) delta_entropy_is.py: return the parameter update buffer (named) from Stage-2
1.1 Add a helper to compute a named-parameter update buffer

Insert near the other private helpers:

# --- NEW: build named-parameter update buffer and its L2 norm ---
def _build_param_update_buf_named(
    self,
    model: torch.nn.Module,
    cpu_snaps: Dict[str, torch.Tensor],
    *,
    to_device: str = "cpu",
    dtype: torch.dtype = torch.float32,
) -> Tuple[Dict[str, torch.Tensor], float]:
    """
    Construct Î”Î¸ buffer keyed by parameter *name*: Î”Î¸[name] = (Î¸_after - Î¸_before).
    Returns (buffer, l2_norm).
    """
    update_buf: Dict[str, torch.Tensor] = {}
    l2_accum = 0.0
    with torch.no_grad():
        for name, p in model.named_parameters():
            after = p.detach().to("cpu", torch.float32)
            before = cpu_snaps[name].to("cpu", torch.float32)
            upd = (after - before).to(to_device, dtype=dtype)
            update_buf[name] = upd
            l2_accum += float((upd.double() * upd.double()).sum().item())
    return update_buf, float(l2_accum ** 0.5)

1.2 Make Stage-2 optionally override LR, compute Î”Î¸, and include it in results

In entropy_change_two_batch_rl(...), at the top where you read cfg_importance, add:

# --- NEW cfg knobs ---
lr_override = cfg_importance.get('lr_override', None)
return_param_update_buf = bool(cfg_importance.get('return_param_update_buf', True))


Immediately after snapshotting (cpu_snaps, opt_state_snapshot = self._snapshot_model_optimizer(...)), and before the RL step:

Keep your existing computation of S_orig, RB_orig on E (unchanged).

Apply LR override just for the single step:

# --- NEW: temporary LR override for the *single* U step ---
lr_backup = [pg.get('lr', None) for pg in optimizer.param_groups]
try:
    if lr_override is not None:
        for pg in optimizer.param_groups:
            pg['lr'] = float(lr_override)
    # C) Take the RL step on U
    self._rl_update_streaming(U_batch, optimizer, rl_grad_accum, importance_mb_size)
finally:
    # restore LR in param groups (optimizer state is restored later anyway)
    for pg, lr0 in zip(optimizer.param_groups, lr_backup):
        if lr0 is not None:
            pg['lr'] = lr0


Immediately after the step, before measuring S_upd, build the named update buffer:

param_update_buf_named, param_update_l2 = (None, 0.0)
if return_param_update_buf:
    param_update_buf_named, param_update_l2 = self._build_param_update_buf_named(
        model, cpu_snaps, to_device="cpu", dtype=torch.float32
    )


Proceed to compute S_upd, RB_upd, compute SNIS, and finally restore the snapshot (unchanged).

Add these fields to the returned dict:

results = {
    # ... existing fields ...
    'param_update_buf_named': param_update_buf_named,   # may be None if disabled
    'param_update_l2': float(param_update_l2),
    'learning_rate_used': float(lr_override if lr_override is not None else optimizer.param_groups[0]['lr']),
}


Notes
â€¢ Keeping the buffer on CPU/FP32 is intentional to make the dot product numerically stable and memory-friendly.
â€¢ The returned 
Î”
ğœƒ
Î”Î¸ already includes any LR scaling and preconditioning from Adam/weight-decay.

2) probe_components.py: accept name-keyed buffers

We standardize internal registries and add a small adapter so both id-keyed and name-keyed dictionaries are accepted.

2.1 Ensure we have a nameâ†”param mapping in the registry (you already build _trainable_named)

Confirm you have (or add if missing) in the constructor:

self._trainable_named = [(n, p) for (n, p) in self._peft.named_parameters() if p.requires_grad]
self._trainable_params = [p for _, p in self._trainable_named]
self._trainable_ids    = {id(p) for p in self._trainable_params}
# --- NEW ---
self._name_to_param = {n: p for (n, p) in self._trainable_named}
self._id_to_name    = {id(p): n for (n, p) in self._trainable_named}

2.2 Add a small coercion utility

Place near your buffer utilities:

# --- NEW: accept both name-keyed and id-keyed buffers ---
def coerce_buffer_to_id_keys(
    self,
    buf: Dict[Union[str, int], torch.Tensor]
) -> Dict[int, torch.Tensor]:
    """
    Convert a possibly name-keyed buffer to an id-keyed buffer that matches self._trainable_params.
    Unknown entries are ignored.
    """
    if not buf:
        return {}
    # Fast path: already id-keyed
    any_key = next(iter(buf.keys()))
    if isinstance(any_key, int):
        return buf  # assume already keyed by id(p)
    # Convert name -> id
    out: Dict[int, torch.Tensor] = {}
    for name, t in buf.items():
        p = self._name_to_param.get(name, None)
        if p is None:
            continue
        out[id(p)] = t
    return out

2.3 Use the coercion when computing Î´Hâ‚ with a provided Î”Î¸ buffer

In compute_delta_h1_from_batches(...), in the branch that handles param_update_buf is not None, coerce before the dot:

if param_update_buf is not None:
    # 1) Compute Î£X/Î¼X on E as you already do
    # ...
    mu_X = {pid: buf / max(B_E_local, 1) for pid, buf in sum_X_buf.items()}

    # --- NEW: accept name-keyed or id-keyed incoming Î”Î¸ ---
    param_update_buf = self.coerce_buffer_to_id_keys(param_update_buf)

    # 2) Dot product: NOTE lr=1.0 because Î”Î¸ already contains the step size
    bars_dot = self.dot_param_buffers(mu_X, param_update_buf)
    lr = 1.0
    delta_h1 = bars_dot
    # ...


No other changes to the legacy path are required.

3) offline_entropy_probe.py: call Stage-2 once, reuse Î”Î¸ for Î´Hâ‚

We will remove the separate Stage-1 call that took an additional optimizer step to build 
Î”
ğœƒ
Î”Î¸. The flow becomes:

Sample E and U

Stage-2: call entropy_change_two_batch(...) with cfg_importance including return_param_update_buf=True (and lr_override if you are sweeping). This takes one RL step on U, measures 
ğ»
orig
H
orig
	â€‹

 and 
ğ»
upd
H
upd
	â€‹

 on E with SNIS, builds Î”Î¸ (named), and then restores the snapshot.

Compute 
ğ›¿
ğ»
1
Î´H
1
	â€‹

 by calling probe_components.compute_delta_h1_from_batches(..., param_update_buf=Î”Î¸_named); this recomputes the X-gradients on E at the restored parameters Î¸ (correct for first-order), and uses the supplied Î”Î¸ to form the dot product.

3.1 Remove the earlier Î”Î¸ computation

Delete (or comment out) the block that called:

delta_theta_buf, delta_theta_norm, B_U_used = self._compute_param_update_buffer(U_batch, mb_size_prompts)
# ... and the immediate compute_delta_h1_from_batches that used this buffer ...

3.2 Replace with: Stage-2 first, then Î´Hâ‚

Right after sampling E and U (and any logging), insert:

# --- Phase 1: Ground-truth Î”H via SNIS, *and* get Î”Î¸ from the same step ---
cfg_importance = dict(self.config.get('true_delta_h', {}))
cfg_importance.setdefault('return_param_update_buf', True)

# If sweeping LR, ensure the override is provided here (single source of truth)
if 'lr_override' in self.config.get('true_delta_h', {}):
    cfg_importance['lr_override'] = self.config['true_delta_h']['lr_override']

gt_start = time.time()
ground_truth_results = self.delta_entropy_is.entropy_change_two_batch(
    self.model, E_batch, U_batch, self.optimizer, cfg_importance
)
gt_time = time.time() - gt_start

# Î”Î¸ buffer comes back keyed by parameter *name*
delta_theta_named = ground_truth_results.get('param_update_buf_named', None)
delta_theta_l2 = ground_truth_results.get('param_update_l2', 0.0)
lr_used = ground_truth_results.get('learning_rate_used', None)

self.logger.info(
    f"[GT] deltaH_true = {ground_truth_results['deltaH_true']:.10f} "
    f"(||Î”Î¸||â‚‚={delta_theta_l2:.4e}, lr_used={lr_used})"
)

# --- Phase 2: Î´Hâ‚ using X on E and the *same* Î”Î¸ ---
if delta_theta_named is None:
    raise RuntimeError("Ground-truth stage did not return param_update_buf_named; set return_param_update_buf=True")

compute = self.probe_components.compute_delta_h1_from_batches(
    E_batch=E_batch,
    U_batch=U_batch,                         # not used in buffer path
    mb_size_prompts=mb_size_prompts,
    weighting_mode=self.config['estimator'].get('weighting_mode', 'dr_grpo'),
    adam_preconditioner=self.adam_preconditioner,
    optimizer=self.optimizer,                # learning_rate field will be 1.0 in buffer path
    param_update_buf=delta_theta_named,      # <-- named-keyed; class will coerce
)
delta_h1   = compute['deltaH1']
bars_dot   = compute['bars_dot']
B_E_global = compute['B_E']
B_U_global = compute['B_U']  # (may reflect local accounting; fine for logs)

self.logger.info(f"[Î´H1] bars_dot={bars_dot:.10f}, deltaH1={delta_h1:.10f} (uses same Î”Î¸)")


You can then proceed to collate and log both estimates, plot comparisons, etc.

4) Behavior and correctness notes

Exactly one optimizer step. The only parameter update happens inside Stage-2, and that step is both:
(i) the step used to measure 
ğ»
upd
H
upd
	â€‹

 for Î”H_true, and
(ii) the source of 
Î”
ğœƒ
Î”Î¸ for the Î´Hâ‚ dot-product.

First-order point of expansion is correct. entropy_change_two_batch_rl restores the snapshot before returning; thus, compute_delta_h1_from_batches computes 
âˆ‡
ğœƒ
ğ»
âˆ‡
Î¸
	â€‹

H at the pre-update parameters Î¸, as required by the first-order expansion 
ğ»
(
ğœƒ
+
Î”
ğœƒ
)
âˆ’
ğ»
(
ğœƒ
)
â‰ˆ
âŸ¨
âˆ‡
ğ»
(
ğœƒ
)
,
Î”
ğœƒ
âŸ©
H(Î¸+Î”Î¸)âˆ’H(Î¸)â‰ˆâŸ¨âˆ‡H(Î¸),Î”Î¸âŸ©.

Learning-rate proportionality.

Î”Î¸ includes the LR scaling.

In the â€œbuffer pathâ€, compute_delta_h1_from_batches sets lr=1.0 and reports deltaH1 = bars_dot.

If you sweep lr_override in Stage-2, both 
âˆ¥
Î”
ğœƒ
âˆ¥
2
âˆ¥Î”Î¸âˆ¥
2
	â€‹

 and 
ğ›¿
ğ»
1
Î´H
1
	â€‹

 should scale approximately linearly with the override (up to clipping/second-order effects). Log the ratio across two runs as a sanity check.

Keying by names. Returning a name-keyed Î”Î¸ is safer and future-proof. The probe_components coercion converts it to id-keyed buffers internally to reuse your existing dot product path without rewriting.

Distributed (DDP).

The RL step uses DDP all-reduce on grads, so the post-step parameters are identical across ranks; the name-keyed Î”Î¸ built per rank will be identical.

compute_delta_h1_from_batches will perform its usual local micro-batching; your existing reductions (if any) remain unchanged.

5) Optional: guardrails and diagnostics

Add invariant logs: in Stage-2, log param_update_l2 and (optionally) the top-K parameter names by 
âˆ¥
Î”
ğœƒ
âˆ¥
âˆ¥Î”Î¸âˆ¥.

Scale check: print param_update_l2 for two LR settings and the ratio.

ESS checks: keep your SNIS ESS diagnostics; abort the run or warn if ESS_fraction drops below a threshold (e.g., 0.1), since large LR breaks the linear regime.

6) Minimal API expectations after this change

DeltaEntropyIS.entropy_change_two_batch(...) now returns:

existing keys (e.g., H_orig, H_upd, deltaH_true, diagnostics, optional per-token variants),

plus:

param_update_buf_named: Dict[str, Tensor] (CPU/FP32),

param_update_l2: float,

learning_rate_used: float.

ProbeComponents.compute_delta_h1_from_batches(..., param_update_buf=...) accepts either name-keyed or id-keyed buffers (internally coerced).

OfflineEntropyProbe.run_mixed_probe(...) no longer calls a second _rl_update_streaming to build Î”Î¸; it consumes Stage-2â€™s Î”Î¸ and computes Î´Hâ‚ from it.

7) Post-patch quick test (manual)

Run once with baseline LR 
ğœ‚
Î·. Record:
param_update_l2(Î·), deltaH1(Î·), deltaH_true(Î·).

Run again with lr_override = Î·/10. Expect approximately:
param_update_l2(Î·)/param_update_l2(Î·/10) â‰ˆ 10, and likewise for deltaH1 (if clipping and second-order terms are mild).

Confirm only one optimizer step occurs (you can add a counter in the optimizer wrapper or assert that pre/post hashes of parameters match after the function returns).