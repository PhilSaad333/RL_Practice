Summary
Slim ProbeComponents to pure compute and remove its sampling responsibilities. Introduce a single compute entrypoint that consumes E and U batches (prepared by SequenceProcessor) and returns delta H1 plus supporting stats. Simplify offline_entropy_probe to call this compute entrypoint and drop the remaining legacy sampling fallbacks. Keep the design DDP‑ready by keeping compute side effects isolated and relying on the orchestrator for any cross‑rank planning.

Rationale
- Eliminate duplicated sampling logic and stopword handling scattered in ProbeComponents.
- Make ProbeComponents single‑responsibility: map prepacked batches -> gradients -> X̄, Ȳ -> delta H1.
- Reduce complexity in offline_entropy_probe by collapsing Phase 1–3 compute flow into one call.

Config
- No new keys. Optional: deprecate probe_rework.use_sequence_processor_sampling (always true after this stage).

Minimal Diffs (copy‑pastable)

1) entropy_experiments/probe_components.py — add pure compute entrypoint; remove sampling methods
---
Add method inside class ProbeComponents (place near accumulate_sum_X/Y):

"""
    def compute_delta_h1_from_batches(
        self,
        *,
        E_batch: Dict[str, Any],
        U_batch: Dict[str, Any],
        mb_size_prompts: int,
        weighting_mode: str,
        adam_preconditioner,
        optimizer: Optional[torch.optim.Optimizer] = None,
    ) -> Dict[str, Any]:
        """Compute delta H1 given packed E and U batches.

        Returns:
            {
              'deltaH1': float,
              'bars_dot': float,
              'B_E': int,
              'B_U': int,
              'learning_rate': float,
              'timing': {'phase1_time': float, 'phase2_time': float, 'phase3_time': float}
            }
        """
        self._assert_grad_context("compute_delta_h1_from_batches")

        t1 = time.time()
        sum_X_buf, B_E_local = self.accumulate_sum_X(E_batch, mb_size_prompts, weighting_mode)
        phase1_time = time.time() - t1

        t2 = time.time()
        sum_Y_buf, B_U_local = self.accumulate_sum_Y(U_batch, mb_size_prompts, adam_preconditioner)
        phase2_time = time.time() - t2

        # Averages
        mu_X = {pid: buf / max(B_E_local, 1) for pid, buf in sum_X_buf.items()}
        mu_Y = {pid: buf / max(B_U_local, 1) for pid, buf in sum_Y_buf.items()}

        # Dot product and deltaH1
        t3 = time.time()
        bars_dot = self.dot_param_buffers(mu_X, mu_Y)
        lr = self._get_learning_rate(optimizer)
        delta_h1 = lr * bars_dot
        phase3_time = time.time() - t3

        return {
            'deltaH1': float(delta_h1),
            'bars_dot': float(bars_dot),
            'B_E': int(B_E_local),
            'B_U': int(B_U_local),
            'learning_rate': float(lr),
            'timing': {
                'phase1_time': float(phase1_time),
                'phase2_time': float(phase2_time),
                'phase3_time': float(phase3_time),
            },
        }
"""

Remove the three sampling methods and local stop helpers:
- Delete def sample_batch(self, B: int, G: int, indices: Optional[List[int]] = None) -> Dict[str, Any]
- Delete def sample_E_batch_with_replacement(self, E_total_sequences: int, G: int) -> Dict[str, Any]
- Delete def sample_U_batch_distinct(self, B_U: int, G: int) -> Dict[str, Any]
- Delete def _get_stop_processor(self)
- Delete def _trim_at_stop_tag(self, token_ids: torch.Tensor)

Optionally remove now‑unused imports: DataLoader, os, defaultdict, math (verify usage first; keep if referenced elsewhere).

2) entropy_experiments/offline_entropy_probe.py — collapse compute path and remove legacy sampling fallbacks
---
(a) Remove legacy sampling references in Phase 0 of run_mixed_probe; always use SequenceProcessor path from Stage 2. Replace the conditional/fallback block with:

"""
            # Phase 0: Sampling E and U batches
            self.logger.info("Phase 0: Sampling E and U batches")
            phase0_start = time.time()
            G_U = self.config['batch_config']['G']
            E_batch, U_batch = self._sample_EU_via_sequence_processor(B_E=B_E, B_U=B_U, G_U=G_U)
            phase0_time = time.time() - phase0_start
            self.logger.info(f"Phase 0 complete: {phase0_time:.2f}s")
"""

(b) Replace the lengthy Phase 1–3 accumulation block with a single call to the new ProbeComponents entrypoint. Find the section starting with:
    "Phase 1: Accumulating" and ending before variance/importance sampling, and replace with:

"""
            self.logger.info("Phase 1–3: Computing delta H1 from E/U batches")
            compute = self.probe_components.compute_delta_h1_from_batches(
                E_batch=E_batch,
                U_batch=U_batch,
                mb_size_prompts=mb_size_prompts,
                weighting_mode=weighting_mode,
                adam_preconditioner=self.adam_preconditioner,
                optimizer=self.optimizer,
            )
            delta_h1 = compute['deltaH1']
            bars_dot = compute['bars_dot']
            learning_rate = compute['learning_rate']
            phase1_time = compute['timing']['phase1_time']
            phase2_time = compute['timing']['phase2_time']
            phase3_time = compute['timing']['phase3_time']
            B_E_global = B_E
            B_U_global = B_U
            self.logger.info(f"[RESULTS] bars_dot={bars_dot:.10f}, lr={learning_rate:.2e}, deltaH1={delta_h1:.10f}")
"""

(c) Deprecate _sample_batch() and its use in run_offline_analysis. Replace the body of _sample_batch with a thin SP wrapper (distinct prompts, G responses):

"""
    def _sample_batch(self) -> Dict[str, Any]:
        # Legacy shim for older codepaths: use SP to sample B distinct prompts with G responses
        B = int(self.config['batch_config']['B_E_values'][0]) if isinstance(self.config['batch_config']['B_E_values'], list) else int(self.config['batch_config']['B_E_values'])
        G = int(self.config['batch_config']['G'])
        self._ensure_sequence_processor()
        ds_name = self.config['batch_config']['dataset_name']
        split = self.config['batch_config']['split']
        seqs, lp, _ = self._sequence_processor.generate_with_logprobs(
            prompts=None, G=G, dataset_name=ds_name, split=split, num_prompts=B, compute_rb=False, with_grad=False,
        )
        rewards = lp.rewards
        return self._pack_U_from_sequences(seqs, rewards)
"""

And ensure run_offline_analysis continues to work by consuming this U‑style batch, or update it to call run_mixed_probe under the hood (preferred if behavior alignment is desired).

Implementation Steps
1) Add compute_delta_h1_from_batches() method to ProbeComponents.
2) Remove sampling methods and local stop helpers from ProbeComponents (and prune unused imports where safe).
3) Simplify run_mixed_probe:
   - Always use SP sampling in Phase 0.
   - Replace Phase 1–3 compute with the single compute entrypoint.
4) Update _sample_batch() shim in offline_entropy_probe to use SP so that run_offline_analysis remains functional (or refactor it to delegate to run_mixed_probe).

Validation
- Single‑GPU, fixed checkpoint + optimizer state, 3 seeds:
  - Compare deltaH1 and bars_dot before vs after Stage 3 (should match within tolerance).
  - Confirm logs show Phase 1–3 times from the compute entrypoint.
- Code hygiene:
  - Grep repo to ensure no remaining references to ProbeComponents.sample_* methods.
  - Run mypy/ruff (if configured) to catch stale imports or type issues.

Acceptance Criteria
- ProbeComponents exposes compute_delta_h1_from_batches and contains no dataset/generation/sampling code.
- offline_entropy_probe uses SP for all sampling and a single ProbeComponents call for compute.
- Functional parity: deltaH1 matches Stage 2 for the same seeds/config.

Rollback
- Revert the removal of sampling methods and restore the Phase 1–3 compute block and legacy fallback in offline_entropy_probe.
