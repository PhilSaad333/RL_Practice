
OFFLINE ENTROPY PROBE — IMPLEMENTATION STRATEGY (v1)

Purpose
-------
Given a saved training checkpoint (model parameters θ and optimizer state), this tool measures:
  (1) The first‑order prediction of the entropy change, δH₁, using a statistically principled and efficient estimator.
  (2) Two variance/SE estimates for that estimator: (i) a plug‑in estimate based on row‑means/influences; and (ii) a delete‑1 jackknife.
  (3) The realized change in entropy ΔH across a *real* optimizer step taken on the same batch (using self‑normalized importance sampling), so δH₁ can be compared to ΔH.

This document is self‑contained for implementation. It assumes familiarity with the attached derivation notes (“RL_studies.pdf”) for background and notation.


I. Notation and target
----------------------
• Prompts are the independent units. For a batch of B prompts, each prompt pₙ has G sampled responses tₙ,g ~ πθ(·|pₙ). Let Sₙ,g := log πθ(tₙ,g | pₙ), gₙ,g := ∂θ Sₙ,g.

• Per‑prompt summaries (using leave‑one‑out baselines to remove the 1/G bias):
    Xₙ := (1/G) ∑_{g=1}^G (Sₙ,g − S̄ₙ,−g) · gₙ,g               [entropy-side summary]
    Yₙ := (1/(Lmax(pₙ)·G)) ∑_{g=1}^G Aₙ,g · gₙ,g               [objective-side summary]
  where S̄ₙ,−g is the average of Sₙ,· excluding index g; Lmax(p) is the per‑prompt normalization if you use DR-GRPO‑style length scaling.

• Optimizer geometry (Adam). Let P be the preconditioner implied by Adam’s second moments v (per-parameter): we use elementwise P^{1/2} ~ (v^{1/2}+ε)^{-1}, i.e. apply P^{1/2} to Xₙ and Yₙ to match the actual update geometry:
    X̃ₙ := P^{1/2} ⊙ Xₙ ,   Ỹₙ := P^{1/2} ⊙ Yₙ.

• Cross‑prompt order‑2 U‑statistic (diagonals excluded). The point estimator is
    U_B^cross = [B/(B−1)] · (X̃̄ · Ȳ̃) − [1/(B(B−1))] ∑ₙ (X̃ₙ · Ỹₙ),
  where X̃̄ = (1/B)∑ₙ X̃ₙ and Ȳ̃ = (1/B)∑ₙ Ỹₙ.
  The first‑order prediction is δH₁ ≈ −η · U_B^cross.

This U‑statistic is unbiased for the “two independent prompts” target and has variance ≈ 4ζ₁/B + 2ζ₂/(B(B−1)). See RL_studies.pdf for the derivation and detailed motivation.


II. High‑level offline workflow
-------------------------------
Input: checkpoint {θ, Adam state}, tokenizer/pad ids, RL config, rollout collector/dataloader.

1) Load checkpoint (model + optimizer). Ensure Adam state (exp_avg_sq) is available for P^{1/2}. Set model to train mode (to keep autograd live). Disable gradient scaling/AMP only if necessary for numerical reasons; ideally, probe uses same AMP settings as training.

2) Sample data at θ (once):
   • Draw B prompts uniformly from dataset D. For each prompt, sample G responses tₙ,g ~ πθ(·|pₙ) using the same sampling settings as training (temperature, top‑p, etc.).
   • Cache token‑level logprobs Sₙ,g and any auxiliary info needed to recompute logprobs under θ⁺ (see Section V for ΔH via SNIS).

3) Build per‑prompt X̃ₙ and Ỹₙ without storing parameter‑sized vectors (single‑buffer, microbatch‑friendly):
   • Use scalar probe losses on the *same batch* so autograd always returns gradients:
        L_X := (1/(B·G)) ∑_{n,g} (Sₙ,g − S̄ₙ,−g) · Sₙ,g
        L_Y := (1/(B·G)) ∑_{n,g} (Aₙ,g / Lmax(pₙ)) · Sₙ,g
     Notes:
       – L_X and L_Y are scalars; do NOT call .item() until all grads are computed.
       – Compute S̄ₙ,−g per prompt on the fly (exclude index g).

   • Microbatching (RAM‑safe, 4 prompts per microbatch is fine):
       Treat each microbatch as a “block” b. You will build block‑level summaries X̃_b and Ỹ_b and then compute a block‑level U‑statistic (exact over blocks) to approximate the per‑prompt one when exact per‑prompt is too costly.
       Two realizations are supported:

       (A) Exact per‑prompt (use sparingly): for each prompt n
           – Forward just that prompt’s G responses; compute its scalar L_X^(n) and L_Y^(n).
           – Backward L_X^(n) to populate ∑ (S−S̄_loo)∇S into param.grad. Multiply elementwise by P^{1/2}. Immediately compute the inner product with the gradient of L_Y^(n) returned by autograd.grad; store (X̃ₙ · Ỹₙ). Also accumulate X̃ₙ into a running sum for X̃̄ and similarly Ỹₙ for Ȳ̃ *without* storing the full vectors: see “Accumulating bars without storing vectors” below.

       (B) Block U‑statistic via recomputation (default):
           Let there be M blocks of prompts (across all devices and accumulation steps). For each ordered pair b≠c:
             1. Forward block b, build L_X^(b), then backward once → param.grad now holds X̃_b (after applying P^{1/2}).
             2. Forward block c (reuse cached S to avoid recompute where possible), build L_Y^(c), then call autograd.grad(L_Y^(c), params) → returns Ỹ_c per parameter *without* touching param.grad.
             3. Accumulate scalar contribution d_{b→c} := ∑_p [ (param.grad)_p ⊙ (Ỹ_c)_p ].
             4. Zero param.grad and proceed.
           After visiting sufficient pairs, compute the block‑level
               U_M^cross = [M/(M−1)] · (X̃̄_blocks · Ȳ̃_blocks) − [1/(M(M−1))] ∑_b (X̃_b · Ỹ_b),
           where X̃̄_blocks and Ȳ̃_blocks are block means (accumulated the same way). This avoids storing any parameter‑length vectors and keeps peak RAM at one microbatch.

   • Accumulating bars without storing vectors:
       Maintain two running scalars and two running “bar multipliers”:
         – For each backward pass computing X̃ (into param.grad), also compute a dot with a persistent “probe vector” Z_p := 1 for all params to obtain s_X += ∑_p (param.grad)_p. Similarly for Ỹ using autograd.grad to get s_Y.
         – Keep track of the *counts* to form X̃̄ and Ȳ̃ from these sums. Since we only need X̃̄·Ȳ̃ and ∑ X̃·Ỹ, you can compute both on the fly as scalar reductions over parameters and never keep full vectors.
       (If you prefer clarity over micro‑optimizations, it’s fine to store per‑block vectors X̃_b, Ỹ_b at float16/bfloat16 and free them immediately after forming the required block‑level scalars.)

   • Diagonal correction (exclude same prompt/block pairs):
       Compute ∑_n (X̃ₙ·Ỹₙ) or ∑_b (X̃_b·Ỹ_b) explicitly with two small passes (X then Y on the same unit) and subtract as shown in U_B^cross / U_M^cross.

4) Compute U^cross, then δH₁ = −η·U^cross.

5) Variance/SE estimates from the SAME batch (two methods)
   Let r_u be the “row‑mean” for unit u (prompt or block):
       r_u := (1/(U−1)) ∑_{v≠u} (1/2)[ X̃_u·Ȳ̃_{−u} + Ỹ_u·X̃̄_{−u} ].
   Define φ_u := r_u − U^cross. Then:
   (i) Plug‑in ζ₁:
       ζ̂₁ = (1/(U−1)) ∑_u φ_u² ,  SE_plugin(U^cross) ≈ sqrt( 4ζ̂₁ / U ).
   (ii) Jackknife (delete‑1) using only r_u:
       U_{(−u)} = [U·U^cross − 2r_u]/(U−2),
       Var_jack = [(U−1)/U] ∑_u ( U_{(−u)} − mean(U_{(−·)}) )²,
       SE_jack = sqrt(Var_jack).
       You may also report ζ̂₁,jack ≈ (U/4)·Var_jack.
   Use prompts (U=B) for exact statistics, or blocks (U=M) in block mode. The formulas are algebraically identical.


III. Distributed (multi‑GPU) design — O(1) collects
---------------------------------------------------
Goal: exact global scalar results with only a few all‑reduces; no parameter‑vector communication.

Per device, compute locally:
  • Sums for bars: ∑ X̃_u, ∑ Ỹ_u  (implemented as scalar reductions per param as noted above).
  • Diagonal sum: ∑ (X̃_u · Ỹ_u).
  • For each local unit u, keep r_u’s ingredients using the *global* leave‑one‑out bars.

Collectives (all‑reduce across ranks):
  1) Global counts (#prompts per unit, #units). 
  2) Global bars: X̃̄_global, Ȳ̃_global (as scalar sums per param; again reduce as scalars via running dot with a ones‑vector). 
  3) Global diagonal sum: ∑ (X̃_u · Ỹ_u) over all ranks.
Each rank then forms its local r_u using global bars (leave‑one‑out is exact since removing one local unit is handled via simple algebra). Finally, all‑reduce the scalars:
  • U^cross, ∑ r_u, ∑ r_u² (or push U_{(−u)} contributions) to assemble SEs.
This yields *exact* global U^cross and both SEs with constant communication (independent of B, M).


IV. Autograd rules of engagement (avoid None grads)
---------------------------------------------------
To ensure autograd produces gradients consistently:
  • Always reduce to *scalar* probe losses L_X and L_Y. Do NOT call autograd.grad on a tensor output without supplying grad_outputs; reduce instead.
  • Do not run probe forwards under torch.inference_mode(); this disables autograd tracking more strongly than no_grad.
  • If you extract two grads from the same graph, the first backward should use retain_graph=True. Prefer separate forwards for clarity and lower RAM.
  • It is OK that some parameters are unused by the probe (e.g., gated heads). In autograd.grad, pass allow_unused=True and skip None entries. In DDP, set find_unused_parameters=True (or configure static graph once stable).
  • Zero optimizer gradients before and after probe passes. Do not call optimizer.step() during probe’s gradient extractions.

(These points should be enforced in code by checks and helpful error messages.)


V. Realized entropy change ΔH via SNIS on the same batch
---------------------------------------------------------
We want ΔH := H(θ⁺) − H(θ) using the same prompts/responses, to compare with δH₁.

Given cached samples {pₙ, tₙ,g} drawn from πθ(·|p), after taking the real optimizer step (θ→θ⁺) compute:
  • New log‑probs S⁺ₙ,g = log π_{θ⁺}(tₙ,g | pₙ).
  • Importance ratios wₙ,g = exp( S⁺ₙ,g − Sₙ,g ).
  • Self‑normalized weights ŵₙ,g = wₙ,g / ∑_{i} w_i.
  • Estimate H(θ⁺) by:  Ĥ(θ⁺) = −∑_i ŵ_i · S⁺_i,  and Ĥ(θ) = −(1/N)∑_i S_i.
  • Report ΔĤ_SNI S := Ĥ(θ⁺) − Ĥ(θ). Compute ESS = (∑ w_i)² / ∑ w_i². If ESS/(B·G) is small (e.g., < 0.3–0.5), the reweighting is unreliable — resample fresh responses at θ⁺ and recompute Ĥ(θ⁺) directly. (Optionally add Pareto‑smoothed IS (PSIS) as a robustification.)

Log: ΔĤ_SNI S, ESS, (optionally) PSIS k‑diag, and compare ΔĤ_SNI S to δH₁.


VI. Scheduling and configurations
---------------------------------
• Exact per‑prompt mode (validation/offline study): compute X̃ₙ, Ỹₙ per prompt; use U_B^cross, plug‑in ζ̂₁, jackknife SE. Use this to calibrate batch sizes and to fit the “slope” model (B·Var(U_B) vs 1/(B−1)) to recover ζ₁ and residual ζ₂.
• Block mode (default, RAM‑light): choose M ∈ {4,8}. Costs 2M backward passes per measurement and negligible storage. Use identical formulas at block level.
• Microbatching: if only 4 prompts fit per microbatch, simply increase #microbatches (blocks); the estimator remains unbiased. You never store param‑length vectors; you recompute “X then Y” per pair as needed.
• Cross‑fitting within prompt (optional): split G responses into two halves when forming X vs Y to reduce the small O(1/(B·G)) variance term.

Knobs to expose in config:
  B (prompts), G (responses per prompt), block_size / M, pairs_per_block (for incomplete U-statistics), AMP dtype, use_psis (bool), ess_threshold, do_exact (bool).


VII. Pseudocode sketches (prompt‑level exact and block‑level)
-------------------------------------------------------------
A) Exact per‑prompt (conceptual)
--------------------------------
for n in 1..B:
    S_n_g, A_n_g = forward_and_cache(prompt=n, responses=G)
# Build U, bars, diag without storing vectors:
sum_Xbar = 0; sum_Ybar = 0; sum_diag = 0
for n in 1..B:
    L_X^(n) = mean_g((S_n_g - loo_mean(S_n_·)) * S_n_g)
    zero_grad(params)
    backward(L_X^(n), retain_graph=True)      # fills param.grad with un-preconditioned sum (S-S̄)∇S
    apply_preconditioner_inplace(param.grad)  # multiply by P^{1/2}
    XdotOnes = param.grad.sum(); sum_Xbar += XdotOnes
    y_grads = autograd.grad(L_Y^(n), params, retain_graph=False, allow_unused=True)
    dot = 0
    for p, gy in zip(params, y_grads):
        if p.grad is None or gy is None: continue
        apply_preconditioner_inplace(gy)
        dot += (p.grad * gy).sum()
    sum_diag += dot
Xbar = sum_Xbar / B; Ybar = (analogous using L_Y passes or tracked alongside)
U_cross = (B/(B-1)) * (Xbar * Ybar) - (1/(B*(B-1))) * sum_diag
# Row means and plug-in zeta1/jackknife from bars and per-unit r_n (same algebra with leave-one-out bars).

B) Block U‑statistic via recomputation (RAM‑light)
--------------------------------------------------
Partition the global batch into M blocks {b}.
Compute block bars X̃̄_blocks, Ȳ̃_blocks exactly as above (two passes per block, no vector storage).
For selected ordered pairs (b ≠ c):    # exact uses all pairs; incomplete uses a subset
    zero_grad(params)
    backward(L_X^(b))                 # → param.grad == X̃_b
    y_grads = autograd.grad(L_Y^(c), params, allow_unused=True)
    contrib += Σ_p (param.grad_p * precond(y_grads_p))
U_M^cross = (M/(M−1))*(X̃̄_blocks·Ȳ̃_blocks) − (1/(M(M−1))) * Σ_b (X̃_b·Ỹ_b)
Row means r_b from global leave‑one‑out bars → ζ̂₁ and jackknife SE.


VIII. Outputs to log/store per run
----------------------------------
• δH₁, U^cross, (X̃̄·Ȳ̃), Σ X̃·Ỹ, B, G, M.
• SE_plugin, ζ̂₁, SE_jack, ζ̂₁,jack.
• ΔĤ_SNIS, ESS (and PSIS diagnostics if enabled), ΔĤ_resample (if fallback).
• Timing (forward/backward counts), AMP dtype, world size.
• (Optional) r_u histogram / a few largest |φ_u| units for interpretability.


IX. Brief note on the ONLINE probe (for later)
----------------------------------------------
The online probe reuses the *training* batch each optimizer step. Use the block U‑statistic (M≈#accumulation microbatches across devices) every N steps, plus an occasional exact per‑prompt measurement (every K steps) to validate. Compare δH₁ to ΔH via SNIS on the same batch. Keep communications O(1) with the same global‑bar trick. This is a drop‑in into the current runner at the “synchronized optimizer step” boundary.


X. Common pitfalls checklist
----------------------------
• Non-scalar autograd target → autograd.grad returns None. Always build scalar probe losses L_X, L_Y.
• Graph freed between two grad extractions → missing retain_graph=True on the first backward; or better, do separate forwards.
• Disabled tracking → do not run under torch.inference_mode(); prefer normal train mode or no_grad only for pieces that truly don’t touch the graph.
• Unused parameters → allow_unused=True for autograd.grad; in DDP, set find_unused_parameters=True (or adopt static graph once stable).
• Diagonal exclusion → don’t forget the −(1/(B(B−1))) Σ X̃·Ỹ term.
• Distributed → compute leave‑one‑out bars using globally reduced bars; never all‑gather parameter‑length vectors.
• Microbatching → treat microbatches as blocks; recompute “X then Y” per pair; no vector storage.
• AMP/dtypes → ensure probe forwards use exactly the same autocast as training; apply preconditioner in the same dtype before reductions.


Appendix: Minimal interfaces
----------------------------
probe.run_offline(checkpoint, B, G, mode="blocks", M=8, pairs="all", amp=True, psis=False, ess_threshold=0.5):
    returns {
        "U_cross": float,
        "deltaH1": float,
        "se_plugin": float, "zeta1_plugin": float,
        "se_jack": float,   "zeta1_jack": float,
        "deltaH_snis": float, "ESS": float, "psis_k": optional,
        "timing": {...}, "config": {...}
    }

This is everything you need to implement the offline entropy probe end-to-end.
