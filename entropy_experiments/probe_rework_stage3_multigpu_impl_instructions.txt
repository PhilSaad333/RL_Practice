Stage 3 (Multi‑GPU Only): Precise Implementation Instructions

Objective
- Make the mixed E/U probe (Stage 1/2) run correctly and deterministically across multiple GPUs with DDP, while keeping memory bounded via microbatching and avoiding unintended DDP gradient synchronization during probe passes.

Assumptions
- Stage 1 and Stage 2 entrypoints and helpers exist (accumulate_sum_X/Y, compute_VX/VY, teacher_force_logprobs, buffer utils, importance two‑batch path).
- DDP is initialized externally (torchrun/accelerate). Model is wrapped as DDP(self.model, device_ids=[rank]).

Files to modify
- entropy_experiments/distributed_helpers.py
- entropy_experiments/probe_components.py
- entropy_experiments/offline_entropy_probe.py
- (optional) entropy_experiments/importance_sampling.py for two‑batch IS reductions

Step 1 — Deterministic rank‑aware sampling (E/U partition)
1.1 Add helpers in distributed_helpers.py
- def get_dist_info() -> tuple[bool, int, int]:
  - Returns (is_dist, rank, world_size) using torch.distributed.
- def count_global(n_local: int) -> int:
  - Wrap n_local as a CPU tensor(int64), dist.all_reduce(SUM), return int.
- def broadcast_int_list(root_rank: int, indices: list[int]) -> list[int]:
  - On root: convert to torch.int64 tensor shape [N] on CPU; broadcast shape N (int64 scalar), then broadcast data tensor. On non‑root: receive N, allocate tensor, receive data; return list.

1.2 In OfflineEntropyProbe (or ProbeComponents) add deterministic index selection
- Inputs from config: B_E_global, B_U_global, master_seed.
- On rank 0 only:
  - Use master_seed to seed Python random and sample exactly B_E_global and B_U_global distinct dataset indices for E and U from range(len(dataset)).
  - Store as Python lists E_indices_global, U_indices_global.
- Broadcast both lists to all ranks via broadcast_int_list(root=0, ...).
- Create local shards:
  - E_indices_local = E_indices_global[rank::world_size]
  - U_indices_local = U_indices_global[rank::world_size]
  - B_E_local = len(E_indices_local); B_U_local = len(U_indices_local)
- Log on each rank: sizes; on rank 0, log global sizes.
- Modify ProbeComponents.sample_batch to accept an explicit indices list to build a batch from (instead of random sampling). Do NOT random‑sample independently per rank.

Step 2 — Disable DDP gradient sync for probe passes
2.1 In ProbeComponents.accumulate_sum_X and accumulate_sum_Y
- Before backward() calls, wrap in a conditional no_sync context:
  - from contextlib import nullcontext
  - ctx = self.model.no_sync() if isinstance(self.model, torch.nn.parallel.DistributedDataParallel) else nullcontext()
  - with ctx:
      L_X_mb.backward()   # or L_Y_mb.backward()
- Rationale: prevent DDP reducers from firing during probe accums; we are handling reductions manually.

2.2 In compute_VX and compute_VY (per‑unit backward)
- Apply the same no_sync wrapping around each backward.

Step 3 — All‑reduce param buffers and counts (SUM)
3.1 In distributed_helpers.py add:
- def all_reduce_param_buffer_(buf_by_param: dict[int, torch.Tensor]) -> None:
  - For each tensor in dict: ensure it is on the same device/dtype across ranks (recommend CPU fp32). Move to CUDA only if needed, but prefer CPU.
  - Convert to a flat list and call torch.distributed.all_reduce(t, op=SUM) on each tensor. If CPU tensors, use backend that supports CPU (e.g., gloo).
- def all_reduce_scalar_sum(x: torch.Tensor) -> float:
  - x is 0D tensor on CPU or CUDA; call dist.all_reduce(x, op=SUM); return float(x.item()).

3.2 In OfflineEntropyProbe.run_mixed_probe
- After local accumulations:
  - B_E_global = count_global(B_E_local)
  - B_U_global = count_global(B_U_local)
  - all_reduce_param_buffer_(sum_X_local)
  - all_reduce_param_buffer_(sum_Y_local)
  - Compute means:
    - I_X = {pid: t / max(B_E_global, 1) for pid, t in sum_X_local.items()}
    - I_Y = {pid: t / max(B_U_global, 1) for pid, t in sum_Y_local.items()}
- bars_dot: compute locally via dot_param_buffers(I_X, I_Y). No extra comm needed (identical across ranks). Optionally sanity‑check with a scalar all‑reduce and divide by world_size; they should match.
- Store B_E_global, B_U_global in results; only rank 0 writes to disk.

Step 4 — Variance reductions across ranks
4.1 In OfflineEntropyProbe.run_mixed_probe after computing V_X_local, V_Y_local (Stage 2):
- Wrap as 0D tensors and reduce:
  - V_X = all_reduce_scalar_sum(torch.tensor(V_X_local, dtype=torch.float64, device='cpu'))
  - V_Y = all_reduce_scalar_sum(torch.tensor(V_Y_local, dtype=torch.float64, device='cpu'))
- Use reduced V_X, V_Y to compute SE and frac_var on rank 0; other ranks can compute too, but only rank 0 should print/save.

Step 5 — Two‑batch IS on E (multi‑GPU reductions)
5.1 Keep training step on U with normal DDP sync (DO NOT use no_sync in this phase).
- During the U optimizer step, let DDP synchronize grads as usual.

5.2 For E entropy and IS sums, compute local partial sums and reduce
- Each rank computes on its E_indices_local:
  - sum_w_local, sum_wS_local (and sum_wL_local if per‑token) using microbatch evaluation with no_grad.
- Reduce:
  - sum_w = all_reduce_scalar_sum(torch.tensor(sum_w_local, ...))
  - sum_wS = all_reduce_scalar_sum(...)
  - [sum_wL = all_reduce_scalar_sum(...)]
- Rank 0 computes H_upd and ΔH_true from reduced sums; broadcast scalars if others need them.

Step 6 — Rank‑0 logging and reproducibility
- Only rank 0 should save JSON files and print final summaries.
- For reproducibility:
  - Gather E/U prompt ids across ranks to rank 0 via dist.all_gather if desired and log a short hash (e.g., SHA256 of concatenated ids).

Step 7 — Barriers (optional) and timing
- If config.distributed.barriers is True, insert dist.barrier() between phases (after reductions) to simplify profiling.

Validation checklist
- Multi‑GPU run produces identical I_X/I_Y buffers on all ranks (spot‑check a few param norms).
- bars_dot consistent across ranks without extra reduction.
- B_E_global/B_U_global equal expected global sizes from config.
- Probe phases (X/Y/variance) do not trigger DDP grad sync (no_sync in effect); U training step does sync as expected.
- Final outputs written once by rank 0.

Notes
- Prefer CPU fp32 for param buffers and reductions to minimize VRAM. Ensure the process group backend supports CPU reduction (gloo does).
- If using NCCL only, move buffers to CUDA before all_reduce and back to CPU afterward; do so in place and free promptly.
- Keep microbatching unchanged; multi‑GPU does not alter memory footprint per rank.

