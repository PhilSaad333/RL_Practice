Offline Entropy Probe OOM Patch Proposals (Do not apply without review)

Goal
- Eliminate CUDA OOMs by reducing peak activation/memory while preserving correctness of deltaH1 and SNIS measurements.
- Keep diffs small, localized, and behind config flags with safe defaults.

Proposal 1: G-microbatch for gradient path (teacher-forced S)
Files: entropy_experiments/probe_components.py
Sections: _teacher_force_logprobs, _compute_sequence_logprobs

Rationale
- Currently all G sequences are processed together for S with gradients, building `[G,L,V]` graphs. Chunking G reduces peak.

Config additions
- memory_config.teacher_force_microbatch_size: int (default 2)

Minimal diff (conceptual)
--- a/entropy_experiments/probe_components.py
+++ b/entropy_experiments/probe_components.py
@@
     def _teacher_force_logprobs(self, prompt_batch: Dict[str, Any]) -> Dict[str, Any]:
         ...
         S_batch = []  # [batch_size, G]
         for b in range(batch_size):
             seqs_b = sequences[b]              # [G, max_len]
             G_total = seqs_b.shape[0]
             g_mb = self.config['memory_config'].get('teacher_force_microbatch_size', 2)
             S_b_parts = []
             for g_start in range(0, G_total, g_mb):
                 g_end = min(g_start + g_mb, G_total)
                 S_b_part = self._compute_sequence_logprobs(seqs_b[g_start:g_end], prompt_lens[b])  # [g_end-g_start]
                 S_b_parts.append(S_b_part)
             S_b = torch.cat(S_b_parts, dim=0)
             S_batch.append(S_b)
         S = torch.stack(S_batch, dim=0)

@@ def _compute_sequence_logprobs(...):
 -        with torch.amp.autocast("cuda", dtype=self.amp_dtype, enabled=self.use_amp):
 -            logits = self.model(sequences).logits  # [G, total_len, vocab_size]
 +        with torch.amp.autocast("cuda", dtype=self.amp_dtype, enabled=self.use_amp):
 +            # Pass attention_mask if available to align with training usage
 +            logits = self.model(sequences).logits  # [g_mb, total_len, vocab_size]
 -        log_probs = F.log_softmax(logits, dim=-1)
 -        token_log_probs = log_probs[:, :-1].gather(2, target_ids).squeeze(-1)
 +        # Use cross-entropy to avoid materializing full log_softmax graph in memory
 +        ce = F.cross_entropy(
 +            logits[:, :-1, :].contiguous().view(-1, logits.size(-1)),
 +            sequences[:, 1:].contiguous().view(-1),
 +            reduction='none'
 +        ).view(logits.size(0), -1)  # [g_mb, total_len-1]
 +        token_log_probs = -ce  # log p(token)

Notes
- This reduces intermediate tensor size and peak graph memory. Accuracy preserved.

Proposal 2: Keep X_sum/Y_sum on CPU float32
Files: entropy_experiments/probe_components.py
Sections: _compute_delta_h1_exact, _compute_delta_h1_blocks

Rationale
- Two param-sized GPU buffers double memory; CPU accumulators free VRAM.

Minimal diff (conceptual)
--- a/entropy_experiments/probe_components.py
+++ b/entropy_experiments/probe_components.py
@@
 -        X_sum = {}
 -        for param in params:
 -            X_sum[id(param)] = torch.zeros_like(param, memory_format=torch.preserve_format)
 +        X_sum = {}
 +        for param in params:
 +            X_sum[id(param)] = torch.zeros(param.numel(), dtype=torch.float32, device='cpu').view_as(param, memory_format=torch.preserve_format)
@@ microbatch loop
 -            for param in params:
 -                if param.grad is not None:
 -                    X_sum[id(param)].add_(param.grad)
 +            for param in params:
 +                if param.grad is not None:
 +                    X_sum[id(param)].add_(param.grad.detach().to('cpu', dtype=torch.float32))

@@ Y_sum similar change and when computing bars_dot / diag terms
 -                bars_dot += (X_sum[param_id] * Y_sum[param_id]).sum().item()
 +                bars_dot += (X_sum[param_id].float() * Y_sum[param_id].float()).sum().item()

Notes
- Dot-products happen on CPU; negligible overhead versus OOM risk.

Proposal 3: Enforce LoRA-only grads or fail fast
Files: entropy_experiments/offline_entropy_probe.py
Sections: load_checkpoint()/after model init

Rationale
- Prevent accidental full-model gradient computation.

Minimal diff (conceptual)
--- a/entropy_experiments/offline_entropy_probe.py
+++ b/entropy_experiments/offline_entropy_probe.py
@@ after model creation
     trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
     if trainable > 50_000_000:  # heuristic threshold
         self.logger.error(f"Too many trainable params ({trainable:,}). Ensure LoRA-only or freeze base.")
         raise RuntimeError("Refusing to run probe with full-model grads; would likely OOM.")

Option: If using full checkpoint intentionally, call a helper to freeze non-LoRA layers.

Proposal 4: Importance step safety (no GPU clone, correct restore)
Files: entropy_experiments/importance_sampling.py
Sections: _compute_updated_logprobs

Rationale
- Current restore uses `param.grad * lr` (not Adam update) and keeps grads around.

Minimal diff (conceptual)
--- a/entropy_experiments/importance_sampling.py
+++ b/entropy_experiments/importance_sampling.py
@@
     # Snapshot param data to CPU before step
     cpu_snapshots = [p.detach().to('cpu').clone() for p in self.model.parameters()]
     optimizer.zero_grad(set_to_none=True)
     training_loss.backward()
     optimizer.step()
     updated_logprobs = ...
     # Restore from CPU snapshot
     with torch.no_grad():
         for p, snap in zip(self.model.parameters(), cpu_snapshots):
             p.data.copy_(snap.to(p.device))
     optimizer.zero_grad(set_to_none=True)

Notes
- Avoids a second full GPU copy; correctness improved for AdamW.

Proposal 5: Store sampled sequences on CPU
Files: entropy_experiments/probe_components.py
Sections: sample_batch

Rationale
- Keep persistent tensors (sequences/masks) on CPU; move to GPU per microbatch.

Minimal diff (conceptual)
--- a/entropy_experiments/probe_components.py
+++ b/entropy_experiments/probe_components.py
@@ after generation
     all_sequences.append(sequences.cpu())
     all_attention_masks.append(attention_mask.cpu())
 ...
@@ before returning batch_data
     sequences = torch.stack(padded_sequences, dim=0)            # [B,G,L]
     attention_masks = torch.stack(padded_masks, dim=0)
     sequences = sequences.cpu()
     attention_masks = attention_masks.cpu()

Downstream: when using these in forwards, move micro slices with `.to(self.device, non_blocking=True)`.

Proposal 6: Optional debug instrumentation (behind config flag)
Files: probe_components.py, importance_sampling.py
Sections: microbatch loops

Rationale
- Track and log peak VRAM to identify regressions.

Minimal diff (conceptual)
--- add helper
     def _log_mem(self, tag: str):
         if torch.cuda.is_available():
             alloc = torch.cuda.memory_allocated() / 1e9
             peak = torch.cuda.max_memory_allocated() / 1e9
             self.logger.info(f"[mem] {tag}: alloc={alloc:.2f} GB, peak={peak:.2f} GB")
--- call at loop boundaries when `advanced.profile_memory` is true.

Rollout plan
1) Land Proposal 1 and 2 first (largest impact, minimal surface).
2) Add Proposal 3 guard to prevent misuse (LoRA vs full model).
3) Apply Proposal 4 to fix correctness of importance step (and minor memory relief).
4) Apply Proposal 5 to shave persistent VRAM.
5) Keep Proposal 6 behind a debug flag for profiling.

Validation
- Start with `B=8, G=4, max_new_tokens=64, teacher_force_microbatch_size=1, importance_microbatch_size=1`.
- Confirm: stable memory, no OOM, metrics close to current small-run baseline.
- Scale B/G gradually; observe peak VRAM plateaus per step.

