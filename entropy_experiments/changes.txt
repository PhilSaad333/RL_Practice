--- a/delta_entropy_approx.py
+++ b/delta_entropy_approx.py
@@
     def _scale_for_average(self, B_total: int, T_total: int, B_mb: int, T_mb: int) -> float:
         """
         Compute the scale factor so that Σ_mb scale_mb * L_sur_mb
         produces the desired expectation over the E-batch.
 
         - 'per_sequence': average over sequences ⇒ scale_mb = B_mb / B_total
         - 'per_token'   : average over tokens    ⇒ scale_mb = T_mb / T_total
         - 'none'        : leave as sum
         """
         if self.normalize == "per_sequence":
             return float(B_mb) / max(B_total, 1)
         elif self.normalize == "per_token":
             return float(T_mb) / max(T_total, 1)
         else:
             return 1.0
+
+    def _scale_for_derivative(self, B_total: int, T_total: int) -> float:
+        """
+        Scale to average **directional derivatives** over the E-batch.
+        For per-token mean:  (1 / T_total) * Σ_mb D_v L_mb
+        For per-sequence mean: (1 / B_total) * Σ_mb D_v L_mb
+        """
+        if self.normalize == "per_token":
+            return 1.0 / max(T_total, 1)
+        elif self.normalize == "per_sequence":
+            return 1.0 / max(B_total, 1)
+        else:
+            return 1.0
@@
     def compute_dir_linear_and_quadratic_jvp(
@@
-            scale = self._scale_for_average(B_total, T_total, B_mb, T_mb)
-            scale_sum += float(scale)
+            # Derivative averaging: constant scale per microbatch
+            scale = self._scale_for_derivative(B_total, T_total)
+            scale_sum += float(scale)
             g_contribs_mb.append(float(gdotv_mb.item()) * float(scale))
             h_contribs_mb.append(float(vHvv_mb.item()) * float(scale))
             gdotv_total += g_contribs_mb[-1]
             vHvv_total += h_contribs_mb[-1]
@@
         if self.logger:
             self.logger.info(
                 f"[dir JVP] g·v={gdotv_total:.6e}  vHv={vHvv_total:.6e}  eta*={eta_star:.3e}  "
                 f"B={out['num_sequences']} T={out['num_tokens']} baseline={self.baseline_kind}"
             )
             self.logger.info(
-                f"[dir JVP][audit] scale_sum={scale_sum:.6f} (target≈1.0 for {self.normalize}), "
-                f"tokens_used={total_tokens_used} vs pre_count={T_total}"
+                f"[dir JVP][audit] deriv_scale={self._scale_for_derivative(B_total, T_total):.6e}, "
+                f"sum_deriv_scales={scale_sum:.6e} "
+                f"(per_token ⇒ ≈ #microbatches/T_total), tokens_used={total_tokens_used}, pre_count={T_total}"
             )
@@
     def compute_delta_h_approx_jvp(
@@
-            scale = self._scale_for_average(B_total, T_total, B_mb, T_mb)
-            total_tokens_used += T_mb
-            scale_sum += float(scale)
-            contribs_mb.append(mb_contrib * float(scale))
+            # Derivative averaging: constant scale per microbatch
+            scale = self._scale_for_derivative(B_total, T_total)
+            total_tokens_used += T_mb
+            scale_sum += float(scale)
+            contribs_mb.append(mb_contrib * float(scale))
@@
         if self.logger:
             self.logger.info(
                 f"[delta-h approx JVP] ⟨∇H, v⟩={out['delta_h_per_lr']:.6e} | "
                 f"B={out['num_sequences']} T={out['num_tokens']} | baseline={self.baseline_kind}"
             )
@@
             self.logger.info(
-                f"[dir JVP][audit] scale_sum={scale_sum:.6f} (target≈1.0 for {self.normalize}), "
-                f"total_tokens_used={total_tokens_used} vs pre_count={T_total}"
+                f"[dir JVP][audit] deriv_scale={self._scale_for_derivative(B_total, T_total):.6e}, "
+                f"sum_deriv_scales={scale_sum:.6e} "
+                f"(per_token ⇒ ≈ #microbatches/T_total), total_tokens_used={total_tokens_used}, pre_count={T_total}"
             )
