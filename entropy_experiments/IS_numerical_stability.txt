Title: Stabilize “ground truth” ΔH via SNIS; use sampling-measure weights (top-p); add eval-mode & diagnostics

Author: (your name)
Date: 2025-09-03

Context
-------
You observed: with small E/U batches ΔH_true (SNIS) and δH₁ agree in sign; with large E/U, ΔH_true becomes implausibly large and can flip sign (e.g., H_orig≈39 → H_upd≈78). The linear predictor δH₁ (computed from ⟨∇H, δθ⟩) remains stable and negative (expected).

Root causes to address:
(1) SNIS numerical stability: weights are formed and accumulated in fp32 with only a local shift; this is fragile at scale. Even on 1 GPU, fp32 can cause degeneracy.
(2) Weighting measure mismatch: E was generated with top_p=0.995 (temperature=1), but weights currently use raw model probabilities p(·), not the *sampling* distribution q(·) (top-p truncated & renormalized). Correct IS requires w ∝ q_{θ+}(Y)/q_{θ}(Y).
(3) Minor: ensure model.eval() during TF/RB passes to avoid dropout noise; add diagnostics (ESS, weight stats) and optional per-token normalization.

Answers to your two questions
-----------------------------
Q1: “I was on 1 GPU—could the per-rank max bug be the issue?”  
A1: On 1 GPU, “global max” ≡ “local max”, so that *specific* bug did not cause your symptom. However, please still implement the global-max pattern for future multi-GPU runs. The dominant issues for your case are fp32 weight algebra (overflow/underflow) and the p vs q weighting.

Q2: “I used temp=1.0 but top_p=0.995—does p vs q matter?”  
A2: Yes. With top-p < 1, the sampling distribution is q (probability mass renormalized on the nucleus). SNIS must use w ∝ q_{θ+}(Y)/q_{θ}(Y). Using p instead of q introduces bias that can grow with batch size and step size, and can flip signs.

Deliverables
------------
1) sequence_processor.py: add per-token/per-sequence log-q (sampling measure) alongside existing log-p; thread through LogprobResults.
2) delta_entropy_is.py:
   a) Use global log-weight shift; compute weight algebra in float64.
   b) Add a config switch importance.measure ∈ {'p','q'}; default 'q' if top_p<1 else 'p'.
   c) Wrap TF/RB evaluation in model.eval()/restore.
   d) Add diagnostics (ESS, ESS_fraction, logw_max_global, logw_mean).
   e) Optional toggles: importance.use_is (re-sample under θ+ when false), importance.normalize_rb_by_length (per-token normalization of payload).
3) offline_entropy_probe.py (optional): provide a code path to re-sample E under θ+ when use_is=false (small dev subset is fine).

Implementation details and patch sketches
-----------------------------------------

A) sequence_processor.py — compute log-q and expose it
-----------------------------------------------------
Goal: alongside token_logprobs (log-p) we need token_logqs (log-q) that account for temperature and top-p truncation/renormalization used for sampling.

Add to LogprobResults (and its construction):
- token_logqs: List[List[np.ndarray]]  # per sequence, per token log-q
- sequence_logqs: List[List[float]]    # sum over generated tokens per sequence

Algorithm for per-token log-q (vectorized per time step):
Given logits L ∈ R^{B×V}, temperature τ, top_p ∈ (0,1], and realized token ids y:
1) a = L / τ
2) p = softmax(a)  # still full support
3) If top_p==1.0: logq = log softmax(a)[y]. Done.
   Else (top_p<1):
   - Sort p descending along vocab: (p_sorted, idx_sorted) = sort(p, desc)
   - c = cumsum(p_sorted, dim=−1)
   - keep_sorted = (c <= top_p) | (arange==0)  # ensure at least one kept
   - head_mask (original vocab order) = scatter(keep_sorted, idx_sorted)
   - masked_logits = where(head_mask, a, −inf)
   - logZ = logsumexp(masked_logits, dim=−1)
   - logq = a[range(B), y] − logZ           # and if head_mask[y]==False: set logq=−inf

Accumulate per-sequence sums: sequence_logq = sum_t logq_t over generated positions.

Thread these as new fields in LogprobResults that teacher_force_logprobs[_with_diagnostics] returns, analogous to existing sequence_logprobs.

Notes:
- If a realized token falls outside the top-p head under θ+ (possible after update), logq_upd=−inf → w=0 for that sequence. This is correct but can reduce ESS; we will report ESS.
- With top_p=1, sequence_logq==sequence_logprob (sanity check).

B) delta_entropy_is.py — robust SNIS and measure='q'
----------------------------------------------------
1) Wrap TF/RB calls in eval mode:
   - In the method that evaluates S and RB on E (both before and after the RL step), do:
     ```
     was_training = model.training
     model.eval()
     try:
         # TF pass ...
     finally:
         model.train(was_training)
     ```
2) Choose weights from 'p' or 'q':
   - Extract both S_upd/S_orig (sequence_logprobs) and Sq_upd/Sq_orig (sequence_logqs).
   - Config:
     ```
     importance:
       measure: "q"   # or "p"; default "q" if top_p<1 else "p"
     ```
   - Build:
     ```
     if measure == 'q':
         logw = Sq_upd - Sq_orig
     else:
         logw = S_upd - S_orig
     ```
3) Global-max shift in float64 + diagnostics:
   Replace the weight block in _compute_snis_two_batch(...) with:

   ```python
   def _compute_snis_two_batch(self, payload_upd: torch.Tensor, logw: torch.Tensor, ...):
       is_dist, _, _ = distributed_helpers.get_dist_info()

       # Global max
       logw_local_max = logw.max()
       if is_dist:
           t = logw_local_max.detach().clone()
           dist.all_reduce(t, op=dist.ReduceOp.MAX)
           logw_max = t
       else:
           logw_max = logw_local_max

       # Float64 weights
       logw64 = (logw - logw_max).to(torch.float64)
       w = torch.exp(logw64)

       # Local sums (fp64)
       w_sum_local   = w.sum()
       num_local     = (w * payload_upd.to(torch.float64)).sum()
       w2_sum_local  = (w * w).sum()

       # All-reduce sums
       if is_dist:
           for t in (w_sum_local, num_local, w2_sum_local):
               dist.all_reduce(t, op=dist.ReduceOp.SUM)
           w_sum_global  = float(w_sum_local.item())
           num_global    = float(num_local.item())
           w2_sum_global = float(w2_sum_local.item())
       else:
           w_sum_global  = float(w_sum_local.item())
           num_global    = float(num_local.item())
           w2_sum_global = float(w2_sum_local.item())

       # Estimate and diagnostics
       H_upd = (num_global / w_sum_global) if w_sum_global != 0.0 else 0.0
       ESS   = (w_sum_global ** 2) / max(w2_sum_global, 1e-300)
       N     = logw.numel()
       results = {
           "H_upd": H_upd,
           "diagnostics": {
               "ESS": ESS,
               "ESS_fraction": ESS / max(float(N), 1.0),
               "logw_max_global": float(logw_max.item()),
               "logw_mean_local": float(logw.mean().item()),
               "w_sum_global": w_sum_global,
           }
       }
       return results



Optional: “no-IS” sanity mode

Add importance.use_is (default True). If False:

Resample E sequences under θ+ (same generation config), compute H_upd by plain average (no weights), and report ΔH_true = H_upd − H_orig.

This gives a clean check when ESS_fraction is very small.

Logging

Log: H_orig, H_upd, ΔH_true; ESS and ESS_fraction; logw_max_global.

If ESS_fraction < a threshold (e.g., 0.05), emit a warning recommending use_is=False for that run or reduce step size.