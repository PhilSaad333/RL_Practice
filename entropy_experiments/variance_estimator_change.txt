TITLE: Estimating the Variance **Over E Only** (Conditional on a Fixed U) — Design & Implementation Notes

OBJECTIVE
=========
We want the standard error for the mixed-batch predictor
    δH₁ = η · ( X̄_E · Ȳ_U )
when **U is fixed once and for all**, and randomness comes only from sampling E.
In this regime, the relevant variance is the **conditional variance over E**:
    Var_E(δH₁ | U fixed).
This document explains the math, provides an efficient estimator that requires **only scalar
projections**, and outlines precise implementation steps (microbatched, DDP-safe, low memory).

KEY RESULT (what you need to implement)
=======================================
Let s_n := μ_Y^T X_n, where μ_Y := Ȳ_U is the (fixed) mean Y-vector computed from batch U,
and X_n are per-prompt entropy-gradient summaries from E (raw, not preconditioned).
Define
    s̄ = (1/B_E) Σ_{n∈E} s_n,
    sample_var_s = ( Σ s_n^2 − B_E · s̄^2 ) / (B_E − 1).
Then the conditional variance and standard error are
    Var_E(δH₁ | U)  =  η² · sample_var_s / B_E
    SE_E(δH₁ | U)   =  η · sqrt( sample_var_s / B_E ).

This is exactly the “V_X only” term you suspected, but expressed in a way that avoids
building/storing any parameter-sized covariance objects.

DERIVATION (brief)
==================
We have δH₁ = η · ( X̄_E · Ȳ_U ) with Ȳ_U fixed. Let Σ_X be the population covariance of X.
Then
    Var_E(δH₁ | U) = η² · Var( μ_Y^T X̄_E ) = η² · μ_Y^T [ Σ_X / B_E ] μ_Y
                   = η² · (1/B_E) · Var( μ_Y^T X_n ).
A plug-in estimator uses the sample variance of s_n = μ_Y^T X_n:
    Var_E ≈ η² · [ sample_var_s / B_E ].
This is equivalent to the “V_X” delta-method piece you already compute, but it can be
implemented **without** materializing μ_X or X-centering in parameter space.

IMPLEMENTATION PLAN (DDP, microbatched, low memory)
===================================================

Inputs/assumptions
------------------
• You already have the two-batch probe structure:
    – U (update batch) used to build Y and take the model step in the truth measurement
    – E (evaluation batch) used to build X and evaluate entropies
• You can iterate E and U as per-prompt units, in microbatches of 2–4 prompts.
• You have the Adam preconditioner P available for Y (not used here; X is raw).
• You run under DDP; use `no_sync()` for backward-only probe passes.

Step 0. Compute μ_Y once (from U), broadcast to all ranks
---------------------------------------------------------
This is identical to your existing Phase-2 pass:
    – Sweep U in microbatches
    – For each microbatch: build L_Y, backward(), apply P in-place to .grad → Y_mb
    – Accumulate ΣY_local += .grad, zero_grad()
    – All-reduce ΣY_local → ΣY_global; B_U_global = all-reduced count
    – μ_Y = ΣY_global / B_U_global
Keep μ_Y in fp32 param-shaped buffers; broadcast so every rank holds the same μ_Y.

Step 1. Single pass over E to collect scalar s-statistics
---------------------------------------------------------
We do **not** need μ_X, nor any param-sized storage; only scalar projections s_n.

Per rank r:
    sum_s_local   = 0.0
    sum_s2_local  = 0.0
    B_E_local     = 0

For each E unit (prompt group), microbatched:
    • zero_grad()
    • Build L_X (with LOO baseline and **minus** sign):  L_X = - mean( detach(S_w - S_w^LOO) * S )
    • with ddp.no_sync(): L_X.backward()  # raw X_n now in .grad across params
    • Compute s_n = μ_Y · X_n as a single scalar:
          s_n = Σ_params ( grad_X_n[p] * μ_Y[p] )   # sum of elementwise products
      Implementation detail: loop over model.parameters(); for each p with p.grad not None,
      add (p.grad * muY_buf[id(p)]).sum().item() to s_n.
    • sum_s_local  += s_n
    • sum_s2_local += s_n * s_n
    • B_E_local    += 1
    • zero_grad()

All-reduce scalars:
    B_E  = Σ_r B_E_local
    S1   = Σ_r sum_s_local
    S2   = Σ_r sum_s2_local

Compute global statistics:
    s̄               = S1 / B_E
    sample_var_s     = ( S2 − B_E · s̄² ) / max(B_E − 1, 1)
    Var_E(δH₁ | U)   = η² · sample_var_s / B_E
    SE_E(δH₁ | U)    = η · sqrt( sample_var_s / B_E )

Notes:
• No parameter-sized buffers needed in Step 1, just μ_Y.
• If B_E < 2, set sample_var_s = 0 and report SE = 0 with a warning.

Optional: Jackknife over E (cheap, scalar-only)
-----------------------------------------------
Define s̄ as above, and Z = η · s̄. Jackknife deletes one E unit at a time:
    Z_{(-n)} = η · ( (B_E · s̄ − s_n) / (B_E − 1) ).
Then:
    mean_Z_minus = (1/B_E) Σ_n Z_{(-n)}
    Var_jack     = ((B_E − 1)/B_E) Σ_n (Z_{(-n)} − mean_Z_minus)²
Implementation needs only the {s_n}, not parameter-sized data.
(In practice, the sample-variance formula above is sufficient; jackknife is a cross-check.)

CODE HOOKS (where to add this)
==============================
In your probe driver (after μ_Y is available):
  + def compute_SE_over_E_only(self, E_batch, muY, lr, mb_size_prompts):
        sum_s_local, sum_s2_local, B_local = 0.0, 0.0, 0
        for unit in iter_units(E_batch, mb_size_prompts):
            self.model.zero_grad(set_to_none=True)
            L_X = build_LX(unit)                 # with minus sign and LOO detach
            with ddp.no_sync(): L_X.backward()
            s_n = 0.0
            for p in self.model.parameters():
                if p.grad is not None:
                    s_n += (p.grad * muY[id(p)]).sum().item()
            sum_s_local  += s_n
            sum_s2_local += s_n * s_n
            B_local      += 1
            self.model.zero_grad(set_to_none=True)
        # all-reduce S1, S2, B
        S1 = all_reduce_scalar_sum(torch.tensor(sum_s_local,  device=device, dtype=torch.float32)).item()
        S2 = all_reduce_scalar_sum(torch.tensor(sum_s2_local, device=device, dtype=torch.float32)).item()
        B  = all_reduce_scalar_sum(torch.tensor(B_local,     device=device, dtype=torch.float32)).item()
        if B < 2:
            return 0.0, 0.0
        s_bar          = S1 / B
        sample_var_s   = (S2 - B * s_bar * s_bar) / (B - 1)
        var_cond_E     = (lr * lr) * sample_var_s / B
        se_cond_E      = (lr) * math.sqrt(sample_var_s / B)
        return var_cond_E, se_cond_E

Make sure:
• build_LX(unit) uses **attention_mask** in teacher-forcing forwards, and includes the **minus sign**:
      coeff = (S_w - S_w_LOO).detach()
      L_X   = -(coeff * S).mean()
• You do **not** apply the preconditioner to X anywhere; μ_Y is preconditioned already when built from U.
• Use `ddp.no_sync()` around the backward to avoid reducer averaging of .grad across ranks.

REPORTING
=========
When U is fixed, report:
  – δH₁ = η · ( X̄_E · Ȳ_U )
  – SE_E(δH₁ | U)  (as above)
and, if helpful, also the unconditional SE by adding your existing V_Y term (this quantifies uncertainty
from both E and U when U is *not* fixed).

PRACTICAL TIPS
==============
• To reduce SE_E without increasing the number of prompts, increase responses per prompt in E (larger G_E);
  X_n averages over responses, so within-prompt variance drops ~ 1/G_E.
• Prefer per-token averaging weights (`w_k = m_k / L_eff`) for X to reduce heteroskedasticity across lengths.
• Keep μ_Y fixed across multiple E shards; averaging δH₁ over many E shards with the same U is equivalent to
  one large E, and the conditional SE will shrink ~ 1/√(total E size).

VALIDATION
==========
1) Fix U; run multiple independent E shards. Empirical variance of the δH₁ measurements across shards
   should match `var_cond_E` above within error bars.
2) Scale B_E by factor c while keeping U fixed. The measured SE_E should decrease ~ 1/√c.
3) On a toy model, verify that switching off advantages (A=0) makes μ_Y≈0 → δH₁≈0 and SE_E small.


This completes the changes required to estimate the variance **over E only** (with U fixed).
