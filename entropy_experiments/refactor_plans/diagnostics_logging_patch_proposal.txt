Summary:
Create a shared diagnostics pipeline that writes standardized artifacts (JSON, CSV, plots) for ?H_true and ?H_approx runs, leveraging the richer outputs from revised estimators and consolidating histogram/feature logging.

Rationale:
- Diagnostic logging is currently scattered (runner prints, control_variates writes CSVs). Centralizing ensures consistent artifacts across Colab + local notebooks.
- With expanded per-sequence data, we need utilities to serialize histograms, CV fits, ESS sweeps, and error metrics without duplicating plotting code.
- Aligns with the workflow of copying run outputs into `entropy_experiments/results` for analysis.

Minimal Diffs:
diff --git a/entropy_experiments/utils/control_variates.py b/entropy_experiments/utils/control_variates.py
index EEE5555..FFF6666 100644
--- a/entropy_experiments/utils/control_variates.py
+++ b/entropy_experiments/utils/control_variates.py
@@
-def run_control_variate_analysis(
-    *,
-    delta_approx,
-    E_batch: Dict[str, Any],
-    v_named: Dict[str, torch.Tensor],
-    normalization: str = "per_token",
-    out_dir: str = "entropy_experiments/cv_runs",
-    features: Union[List[str], str] = ("length", "mean_logp", "var_logp"),
-    ridge: float = 1e-8,
-    crossfit_folds: int = 0,
-) -> Dict[str, Any]:
+def run_control_variate_analysis(
+    *,
+    delta_approx,
+    approx_payload: Optional[Dict[str, Any]] = None,
+    E_batch: Dict[str, Any],
+    v_named: Dict[str, torch.Tensor],
+    normalization: str = "per_token",
+    out_dir: str = "entropy_experiments/cv_runs",
+    features: Union[List[str], str] = DeltaEntropyApprox.DEFAULT_CV_FEATURES,
+    ridge: float = 1e-8,
+    crossfit_folds: int = 0,
+) -> Dict[str, Any]:
@@
-    records, meta = compute_jvp_per_sequence(
-        delta_approx=delta_approx,
-        E_batch=E_batch,
-        v_named=v_named,
-        normalization=normalization,
-    )
+    if approx_payload and "per_sequence" in approx_payload:
+        records, meta = build_records_from_payload(
+            payload=approx_payload,
+            normalization=normalization,
+        )
+    else:
+        records, meta = compute_jvp_per_sequence(
+            delta_approx=delta_approx,
+            E_batch=E_batch,
+            v_named=v_named,
+            normalization=normalization,
+        )
+
+    _log_histograms(records, out_dir)

+diff --git a/entropy_experiments/utils/diagnostics_logger.py b/entropy_experiments/utils/diagnostics_logger.py
+new file mode 100644
+--- /dev/null
++++ b/entropy_experiments/utils/diagnostics_logger.py
+"""Utilities to persist entropy-experiment diagnostics (JSON, CSV, plots)."""
+
+@dataclass
+class DiagnosticsBundle:
+    run_id: str
+    config: Dict[str, Any]
+    approx: Optional[Dict[str, Any]] = None
+    true: Optional[Dict[str, Any]] = None
+    control_variates: Optional[Dict[str, Any]] = None
+    update_stats: Optional[Dict[str, Any]] = None
+
+
+def write_bundle(bundle: DiagnosticsBundle, *, output_dir: Path) -> Dict[str, str]:
+    """Serialize diagnostics to disk (JSON, CSV, PNG) and return paths."""
+    raise NotImplementedError
+
+
+def log_histogram(name: str, values: Sequence[float], *, output_dir: Path, bins: int = 40) -> str:
+    raise NotImplementedError

Implementation Steps:
1. Allow `run_control_variate_analysis` to reuse per-sequence payload from the new `DeltaEntropyApprox.compute_delta_h_approx` (when available), falling back to recomputation for legacy callers.
2. Add helpers (`build_records_from_payload`, `_log_histograms`) that convert stored per-seq diagnostics into `CVBatchRecord`s and persist histograms (y_i, weights, feature residuals).
3. Introduce `diagnostics_logger.py` as a central sink for writing JSON summaries, CSV tables, and plots (ESS vs ?H, histogram of y_i, log-weight scatter, etc.). Provide an easy-to-call `write_bundle` used by the runner at the end of each execution.
4. Update the runner to assemble a `DiagnosticsBundle` containing approx, true, and CV sections; ensure Google Drive path handling (timestamped directories) is preserved.
5. Extend CLI scripts to surface output directory/folder names in logs for the Colab workflow, and to copy artifacts into `entropy_experiments/results` as done manually.
6. Document the artifact schema (filenames, JSON keys) in `CLAUDE.md` for cross-agent consistency.

Config/Flags:
- Add `config['output']['diagnostics_dir']` (default `entropy_experiments/results`) and `config['output']['hist_bins']` to control histogram granularity.
- Provide CLI flag `--diagnostics-level {none,summary,full}` to guard heavy plotting/writing.

Validation:
- Smoke test: `python run_entropy_experiments_batch.py --mode full --diagnostics-level full` writes bundle under `entropy_experiments/results/<timestamp>` with JSON, CSV, PNG files; verify sizes/logs.
- Unit test `tests/utils/test_diagnostics_logger.py` to ensure `write_bundle` handles missing sections and returns correct paths.
- In Colab, run both scripts and confirm artifacts land in expected Google Drive folder (matching manual workflow).

Rollback:
- Delete `diagnostics_logger.py` and restore `control_variates.py` to recompute per-seq stats internally.
- Remove new config keys/env flags if logging consolidation is backed out.