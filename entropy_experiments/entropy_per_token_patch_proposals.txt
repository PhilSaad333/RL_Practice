Per-Token Entropy Reporting and EOS Handling (Proposal for Claude Code)

Goal
- Add a length-normalized (per-token) entropy option alongside the current sequence-level entropy, with an option to exclude EOS token contributions. Keep default behavior backward compatible.

Config additions
--- Add to config yaml ---
entropy:
  unit: "sequence"   # or "per_token"
  include_eos: true  # count EOS as a normal token if present
  length_normalize_deltah1: false  # apply 1/L normalization to S in delta H₁
--- end ---

Changes (conceptual, file-scoped and function-specific)
1) Return generated lengths from logprob helpers
File: entropy_experiments/importance_sampling.py
Function: _compute_logprobs_for_sequences

Current: returns [B, G] tensor of sequence log-prob sums (S).
Change: also return [B, G] integer lengths L (number of non-pad tokens after prompt; minus 1 if exclude_eos=false and you want to drop EOS when include_eos=false).

Sketch:
--- PATCH SKETCH START ---
def _compute_logprobs_for_sequences(...):
    ...
    all_logprobs, all_lengths = [], []
    ...
        # compute gen_mask = micro_masks[:, prompt_len:].float()
        # if not include_eos: detect eos positions and set those mask entries to 0
        lengths = gen_mask.sum(dim=1).to(torch.int32)  # [micro]
        seq_log_probs = (gen_token_log_probs * gen_mask).sum(dim=1)  # [micro]
        prompt_logprobs.append(seq_log_probs)
        prompt_lengths.append(lengths)
    ...
    return torch.stack(all_logprobs, dim=0), torch.stack(all_lengths, dim=0)
--- PATCH SKETCH END ---

2) Compute per-token entropy when requested
File: entropy_experiments/importance_sampling.py
Function: compute_entropy_change

Replace calls to the helper to capture both (S, L) for original and updated. Then compute:
  - If entropy.unit == "sequence": keep existing −mean (original) and SNIS weighted mean (updated) on S.
  - If entropy.unit == "per_token": use −Σ S / Σ L for original, and −Σ w·S / Σ w·L for updated (with w = exp(log p₁ − log p₀)).

Sketch:
--- PATCH SKETCH START ---
logprobs_original, lengths = self._compute_logprobs_for_sequences(...)
logprobs_updated, _ = self._compute_logprobs_for_sequences(...)
...
if unit == "sequence":
    original_entropy = -logprobs_original.mean().item()
    weights = torch.exp((logprobs_updated - logprobs_original) - (logprobs_updated - logprobs_original).max())
    updated_entropy = -(weights.flatten() * logprobs_updated.flatten()).sum() / weights.flatten().sum()
else:  # per_token
    total_len = lengths.sum().clamp_min(1)
    original_entropy = -(logprobs_original.sum() / total_len).item()
    logw = (logprobs_updated - logprobs_original)
    logw_shift = logw - logw.max()
    w = torch.exp(logw_shift)
    num = (w * logprobs_updated).sum()
    den = (w * lengths).sum().clamp_min(1)
    updated_entropy = -(num / den).item()
--- PATCH SKETCH END ---

3) Optional: EOS exclusion in lengths/masks
File: entropy_experiments/probe_components.py and importance_sampling.py where masks are formed
Approach:
  - If include_eos=false, detect the first eos token id after prompt per sequence and set that position in gen_mask to 0; optionally also ignore it in S.
  - Token id is tokenizer.eos_token_id; you have access in ProbeComponents; in ImportanceSampler fetch or pass it similarly.

4) Optional: length-normalize S for delta H₁
File: entropy_experiments/probe_components.py
Functions: _teacher_force_logprobs, _build_probe_loss_X_from_S, _build_probe_loss_Y_from_S
Approach:
  - Compute per-sequence lengths L_b,g alongside S.
  - If entropy.length_normalize_deltah1=true, use S’ = S / L in the probe losses (ensure numerical safety with clamp(L,1)).
  - Keep defaults unchanged (false).

Validation & acceptance
- With unit=per_token and include_eos=true, report:
  - Original H_tok and updated H_tok in nats/token and bits/token (divide by ln 2).
  - ΔH_tok consistent with expectations (e.g., small negative after a tiny training step).
- Compare sequence-level vs per-token outputs on same run to verify the length bias effect disappears in per-token view.
- No OOM regression from added bookkeeping; lengths are small int tensors and reuse existing masks.

Risks & notes
- SNIS weights can be heavy-tailed; per-token normalization reduces variance sensitivity but doesn’t eliminate it. Consider PSIS if ESS is low.
- Excluding EOS changes the definition relative to standard LM perplexity; make it a non-default, clearly documented.

