# ðŸ§ª Lambda Test with top_p=1.0
# Testing full distribution (no truncation)

# === MODEL AND CHECKPOINT ===
checkpoint:
  checkpoint_path: "/content/drive/MyDrive/RL_Practice_Files/new_rl_checkpoint/step_40/model"
  optimizer_path: "/content/drive/MyDrive/RL_Practice_Files/new_rl_checkpoint/step_40/optimizer.pt"
  backbone: "qwen2_5_15"
  dtype: "bfloat16"
  device_map: "cuda"

# === BATCH CONFIGURATION ===
batch_config:
  dataset_name: "gsm8k_r1_template"
  E_split: "test"
  U_split: "train"
  B_E: 512          # Smaller batch for debugging
  B_U: 64
  G: 8

# === COMPUTATION OPTIONS ===
computation_options:
  compute_delta_h1: true
  master_seed: none
  mb_size_prompts: 4
  weighting_mode: "dr_grpo"

estimator:
  use_simple_entropy_for_x: false

# === GROUND TRUTH ENTROPY CHANGE ===  
true_delta_h:
  enabled: true
  training_loss: "rl"
  microbatch_size: 4
  clip_c: 10.0
  report_per_token: false
  measure: "p"  # Use p measure since top_p=1.0

# === PROBE REUSE / LOGGING ===
probe_reuse:
  # Do not write the raw Î”Î¸ buffer to JSON results (kept in-memory only)
  log_param_update_buf: false


# === GENERATION SETTINGS ===  
generation:
  temperature: 1.0
  top_p: 1.0      # Full distribution - no truncation
  max_new_tokens: 150
  gen_batch_size: 64
  tf_batch_size: 8

# === MEMORY CONFIGURATION ===
memory_config:
  amp: true
  dtype: "bfloat16" 
  microbatch_size: 4

# === DETAILED LOGGING ===
detailed_logging:
  enabled: true
  level: "standard"
  log_sequences: false
  log_tokens: false
  log_raw_tensors: false
  output_directory: "entropy_experiments/logs"
  compress: true
  max_files: 50

# === OUTPUT CONFIGURATION ===
output:
  log_level: "INFO"
  results_path: ""

# NEW === NUMERICAL PRECISION SETTINGS ===
# master precision settings for different compute paths
precision:
  # global GPU math toggles (once, at process init)
  allow_tf32: true           # A100: tf32 on matmul/conv for fp32 ops
  matmul_precision: high     # torch.set_float32_matmul_precision('high'|'medium'|'highest')

  # forward-only (generation, teacher-forcing without grads)
  tf_nograd:
    autocast: false
    dtype: float32           # for accurate entropy, use true fp32 math
    cast_logits_fp32: true   # upcast logits to fp32 before entropy/logprob math

  # forward with grads (used by Î”H1, RB-with-grad, update-vector loss)
  tf_withgrad:
    autocast: false          # turn off autocast here by default
    dtype: float32           # compute in true fp32 for stability
    grad_to_fp32: true       # force-accum grads to fp32 if mixed modes leak in

  # functional_call with param overrides Î¸' = Î¸ + Î· v
  func_override:
    autocast: false
    dtype: float32           # do this path in fp32 to keep tiny Î· effects visible
    cast_params: true       # if true, explicitly cast effective params to this dtype

  # update-vector builder (rl_loss microbatched on U)
  update_vector:
    use_amp: true            # keep as you have today if stable
    amp_dtype: bfloat16      # use bf16 autocast in the microbatched forward
    grads_fp32: true         # ensure grads stored/used in fp32
