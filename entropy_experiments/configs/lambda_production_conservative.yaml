# ðŸš€ Lambda Production Conservative Multi-E-batch Config
# Memory-conservative settings with smaller microbatch sizes
# Production scale batch sizes but optimized for memory efficiency

# === MODEL AND CHECKPOINT ===
checkpoint:
  checkpoint_path: "/home/ubuntu/localfs/training_runs/run_2025-09-03_03-59-57/training_state/step_40/model"
  optimizer_path: "/home/ubuntu/localfs/training_runs/run_2025-09-03_03-59-57/training_state/step_40/optimizer.pt"
  backbone: "qwen2_5_15"
  dtype: "bfloat16"
  device_map: "cuda"

# === BATCH CONFIGURATION ===
batch_config:
  dataset_name: "gsm8k_r1_template"
  E_split: "test"   # Evaluation batch uses test set
  U_split: "train"  # Update batch uses train set
  B_E: 512         # Production E batch size
  B_U: 64          # Production U batch size
  G: 8             # Number of responses per prompt (U batch)

# === COMPUTATION OPTIONS ===
computation_options:
  compute_delta_h1: true
  master_seed: none
  mb_size_prompts: 1      # REDUCED: was 2, now 1 for memory conservation
  weighting_mode: "dr_grpo"

# === GROUND TRUTH ENTROPY CHANGE ===  
true_delta_h:
  enabled: true                   # Required for DeltaEntropyIS component
  training_loss: "rl"
  microbatch_size: 1              # REDUCED: was 2, now 1 for memory conservation
  clip_c: 10.0
  report_per_token: true

# === GENERATION SETTINGS ===  
generation:
  temperature: 0.7
  top_p: 0.995  
  max_new_tokens: 150
  gen_batch_size: 64              # REDUCED: was 128, now 64 for memory conservation
  tf_batch_size: 4                # REDUCED: was 8, now 4 for memory conservation

# === MEMORY CONFIGURATION ===
memory_config:
  amp: true
  dtype: "bfloat16" 
  microbatch_size: 1              # REDUCED: was 2, now 1 for memory conservation

# === DETAILED LOGGING ===
detailed_logging:
  enabled: true
  level: "standard"
  log_sequences: true
  log_tokens: false               # Keep disabled for memory
  log_raw_tensors: false         # Keep disabled for memory
  output_directory: "entropy_experiments/logs"
  compress: true
  max_files: 50

# === OUTPUT CONFIGURATION ===
output:
  log_level: "INFO"
  results_path: ""

# === MEMORY OPTIMIZATIONS APPLIED ===
# All microbatch sizes reduced from 2 â†’ 1:
# - mb_size_prompts: 2 â†’ 1 (computation microbatch)
# - true_delta_h.microbatch_size: 2 â†’ 1 (IS microbatch)  
# - memory_config.microbatch_size: 2 â†’ 1 (general microbatch)
# 
# Generation batch sizes reduced for memory:
# - gen_batch_size: 128 â†’ 64
# - tf_batch_size: 8 â†’ 4
#
# This should reduce peak memory usage by ~50% while maintaining
# the same batch sizes (B_E=512, B_U=64) for statistical quality.