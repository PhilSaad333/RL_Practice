# üöÄ Lambda H100 Optimized Entropy Probe Config
# Streamlined configuration with hardcoded sensible defaults
# and renamed sections for clarity

# === MODEL AND CHECKPOINT ===
checkpoint:
  checkpoint_path: "/home/ubuntu/localfs/training_runs/run_2025-09-03_01-53-23/training_state/step_10/model"  # Use the new RL-trained checkpoint 
  optimizer_path: "/home/ubuntu/localfs/training_runs/run_2025-09-03_01-53-23/training_state/step_10/optimizer.pt"   # Explicit optimizer path with name-aware reconstruction
  backbone: "qwen2_5_15"
  dtype: "bfloat16"
  device_map: "cuda"

# === BATCH CONFIGURATION ===
batch_config:
  dataset_name: "gsm8k_r1_template"
  # Separate splits: E batch from test set, U batch from train set
  E_split: "test"   # Evaluation batch uses test set for unbiased entropy estimation  
  U_split: "train"  # Update batch uses train set for policy gradient computation
  B_E: 64        # E batch size - small for testing
  B_U: 8         # U batch size - small for testing  
  G: 8           # Number of responses per prompt (U batch)

# === COMPUTATION OPTIONS ===
# Renamed from 'probe_rework' for clarity
computation_options:
  compute_delta_h1: true           # Enable Œ¥H‚ÇÅ prediction (Phase 1-3)
  master_seed: none                  # Deterministic sampling
  mb_size_prompts: 2              # Microbatch size for gradient computation
  weighting_mode: "dr_grpo"       # RL-style weighting

# === GROUND TRUTH ENTROPY CHANGE ===  
# Renamed from 'importance' to be more descriptive
true_delta_h:
  enabled: false                   # Disable ground truth ŒîH computation (Phase 5) for now
  training_loss: "rl"             # Use RL-aligned loss for updates
  microbatch_size: 4              # IS microbatch size
  clip_c: 10.0                    # Importance weight clipping
  report_per_token: true          # Include per-token entropy analysis

# === GENERATION SETTINGS ===  
# Streamlined - removed rb_requires_grad (now hardcoded to true)
generation:
  temperature: 0.7
  top_p: 0.995  
  max_new_tokens: 150
  gen_batch_size: 256
  tf_batch_size: 8

# === MEMORY CONFIGURATION ===
# Simplified - removed amp (hardcoded true), kept essentials
memory_config:
  amp: true
  dtype: "bfloat16" 
  microbatch_size: 2

# === DETAILED LOGGING ===
detailed_logging:
  enabled: true
  level: "standard"               # Good balance: core metrics + diagnostics (~5-10KB)
  log_sequences: true             # Include sequence data for debugging
  log_tokens: false               # Disable token-level logging for performance
  log_raw_tensors: false          # Disable raw tensor dumps for performance
  output_directory: "entropy_experiments/logs"
  compress: true                  # Gzip compression for log files
  max_files: 50                   # Auto-cleanup old logs

# === OUTPUT CONFIGURATION ===
output:
  log_level: "INFO"
  results_path: ""                # Auto-generate path

# === HARDCODED OPTIMIZATIONS APPLIED ===
# The following parameters have been hardcoded in the implementation for simplicity:
# - rb_requires_grad: true (always required for proper gradient computation)
# - x_estimator_mode: 'rb_residual' (preferred over 'naive' mode)
# - entropy_mode: 'rb' (standard RB-based entropy computation)
# - use_qlora: false (standard LoRA setup)  
# - amp: true (automatic mixed precision enabled)
# - is_mode: 'snis' (self-normalized importance sampling)
# - snapshot_device: 'cpu' (memory management)
# - do_sample: true (required for generation)

# === REMOVED REDUNDANCIES ===
# - compute_importance_sampling: redundant with true_delta_h.enabled
# - Multiple device_map specifications: consolidated
# - Duplicate dtype specifications: unified