# Test configuration for complete δH₁ computation
# Tests: E/U batch separation, Phase 3b regression baseline, proper optimizer loading

model_config:
  backbone: qwen2_5_15
  lora_simple: true
  dtype: bf16

batch_config:
  dataset_name: gsm8k_r1_template
  split: test
  B_E_values: [64]       # Reduce E batch to debug (E: 64×1=64 sequences)
  B_U_values: [16]       # Reduce U batch to debug (U: 16×8=128 sequences)  
  B_U: 16                # Required by validation
  G: 8                  # Keep same G for U batch
  # Total: 64 + 128 = 192 sequences (much more manageable)
  
learning_rate: 1.0e-5   # Proper learning rate for δH₁ = lr × X·Y

generation:
  temperature: 0.7
  top_p: 1.0
  max_new_tokens: 150   # Keep same
  gen_batch_size: 16    # Reduce from 64->16 for safety
  tf_batch_size: 8      # Keep same
  do_sample: true       # Required for sampling
  pad_token_id: 151643  # Qwen2.5 pad token ID
  rb_requires_grad: true  # Required for Phase 3b regression baseline

memory_config:
  amp: true
  dtype: bfloat16
  microbatch_size: 1    # Reduce to 1 for maximum safety

# Phase 3b: RB-residual estimator with regression baseline
estimator:
  x_estimator_mode: rb_residual
  rb_normalize_by_length: true
  baseline:
    mode: basic    # Use new regression baseline or basic
    ridge_lambda: 1e-3
    features:
      - "H"
      - "top1"
      - "margin"
      - "head_mass"
      - "two_point_entropy"
      - "logit_var"
      - "pos_frac"

probe_rework:
  compute_delta_h1: true
  mb_size_prompts: 2    # Small microbatch
  weighting_mode: dr_grpo
  prompt_sampling_seed: null  # null = random prompts each run, int = fixed for reproducibility

importance:
  enabled: false        # Skip for test

output:
  log_level: INFO
  save_path: /home/ubuntu/localfs/entropy_test

checkpoint:
  checkpoint_path: /home/ubuntu/localfs/rl_training_runs/training_state/step_60/model
  optimizer_path: /home/ubuntu/localfs/rl_training_runs/training_state/step_60/optimizer.pt
  use_qlora: false
  dtype: bf16
  device_map: cuda