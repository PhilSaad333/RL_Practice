# Test configuration for complete δH₁ computation
# Tests: E/U batch separation, Phase 3b regression baseline, proper optimizer loading

model_config:
  backbone: qwen2_5_15
  lora_simple: true
  dtype: bf16

batch_config:
  dataset_name: gsm8k_r1_template
  split: test
  B_E_values: [16]      # Larger E batch for better signal
  B_U_values: [8]       # Larger U batch for more data
  B_U: 8                # Required by validation
  G: 8                  # More generations to get some correct responses
  rollout_batch_size: 2 # Keep small for memory
  
learning_rate: 1.0e-5   # Proper learning rate for δH₁ = lr × X·Y

generation:
  temperature: 0.7
  top_p: 0.995
  max_new_tokens: 200   # Sufficient tokens for complete math solutions
  gen_batch_size: 4
  tf_batch_size: 8
  do_sample: true       # Required for sampling
  pad_token_id: 151643  # Qwen2.5 pad token ID
  rb_requires_grad: true  # Required for Phase 3b regression baseline

memory_config:
  amp: true
  dtype: bfloat16
  microbatch_size: 1    # Small for testing

# Phase 3b: RB-residual estimator with regression baseline
estimator:
  x_estimator_mode: rb_residual
  rb_normalize_by_length: true
  baseline:
    mode: regression    # Use new regression baseline
    ridge_lambda: 1e-3
    features:
      - "H"
      - "top1"
      - "margin"
      - "head_mass"
      - "two_point_entropy"
      - "logit_var"
      - "pos_frac"

probe_rework:
  compute_delta_h1: true
  compute_conditional_variance: false  # Skip to save time in test
  mb_size_prompts: 2    # Small microbatch
  weighting_mode: dr_grpo
  prompt_sampling_seed: null  # null = random prompts each run, int = fixed for reproducibility

importance:
  enabled: false        # Skip for test

output:
  log_level: INFO
  save_path: /home/ubuntu/localfs/entropy_test

checkpoint:
  checkpoint_path: /home/ubuntu/localfs/checkpoints/qwen2_5_15_finetuned/qwen2_5_15_gsm8k_lora/checkpoint-156