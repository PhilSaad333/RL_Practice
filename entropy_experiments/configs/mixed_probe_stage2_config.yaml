# Stage 2 Mixed E/U Batch Probe Configuration
# Complete configuration including variance estimation and two-batch ground-truth

# Core sampling parameters - separate E and U batches
batch_config:
  B: 16                             # Fallback batch size for compatibility
  B_E: 16                           # Evaluation batch size
  B_U: 16                           # Update batch size  
  G: 8                              # Keep at target value
  rollout_batch_size: 32            # Efficient batch generation
  dataset_name: "gsm8k_r1_template" # Dataset to sample prompts from
  split: "train"                    # Dataset split to use

# Probe configuration (for compatibility with existing code)
probe_config:
  mode: "exact"                     # Legacy mode (not used in mixed probe, but needed for initialization)

# Stage 1+2 Mixed Probe Configuration
probe_rework:
  mb_size_prompts: 2                # Microbatch size (prompts per microbatch)
  buffers_dtype: "float32"          # Buffer precision (float32/float16) - keep fp32 for stability
  weighting_mode: "dr_grpo"         # Weighting mode (dr_grpo/per_token_avg)
  variance_enabled: true            # Enable Stage 2 variance computation (V_X, V_Y)
  ddp_allreduce: true               # Enable DDP all-reduce (no-op on single GPU)

# Stage 2 Importance Sampling Configuration
importance:
  enabled: true                     # Enable Stage 2 two-batch ground-truth computation
  is_mode: "snis"                   # Importance sampling mode: "snis" or "clip"
  clip_c: 10.0                      # Clipping constant for clipped IS (only used if is_mode="clip")
  training_loss: "nll"              # Training loss type: "nll" or "rl" (RL not implemented yet)
  snapshot_device: "cpu"            # Device for model snapshots: "cpu" or "gpu"
  importance_microbatch_size: 1     # Microbatch size for importance sampling computations
  report_per_token: false           # Compute per-token entropy metrics (optional)

# Memory and performance - Conservative for testing
memory_config:
  microbatch_size: 1                # For legacy importance sampling (if needed)
  importance_microbatch_size: 1     # Conservative microbatch size
  teacher_force_microbatch_size: 2  # G-microbatching for gradients
  amp: true                         # Use automatic mixed precision
  dtype: "bfloat16"                 # Match training configuration

# Variance estimation (for compatibility with legacy code)
stats_config:
  compute_plugin_se: true           # Plug-in standard error estimate (legacy)
  compute_jackknife_se: true        # Jackknife standard error estimate (legacy)

# Legacy importance sampling (for compatibility)
importance_sampling:
  use_snis: true                    # Self-normalized importance sampling
  use_psis: false                   # Skip PSIS for now
  ess_threshold: 0.5                # Effective sample size threshold
  resample_on_low_ess: false        # Don't resample for testing

# Checkpoint paths - using extracted RL training checkpoint
checkpoint:
  checkpoint_path: "/home/ubuntu/localfs/rl_training_runs/training_state/step_latest/model"
  optimizer_path: "/home/ubuntu/localfs/rl_training_runs/training_state/step_latest/optimizer.pt"
  model_config_path: "Qwen/Qwen2.5-1.5B"

# Generation settings - moderate for testing
generation:
  max_new_tokens: 100               # Reduced for faster testing
  temperature: 0.7                  # Match training temperature
  top_p: 1.0                       # Full distribution
  do_sample: true                  # Enable sampling
  pad_token_id: null               # Auto-detect from tokenizer

# Output and logging
output:
  save_results: true               # Save detailed results
  results_path: "mixed_probe_stage2_results.json"
  log_level: "INFO"                # Standard logging
  save_samples: false              # Don't save samples (saves space)

# Distributed settings - single GPU for Stage 2 testing
distributed:
  find_unused_parameters: true     # Required for DDP compatibility
  reduce_dtype: "float32"          # Standard precision for reductions

# Advanced options - minimal for Stage 2 testing
advanced:
  force_recompute: true            # Don't use cached computations
  validation_mode: false           # Skip extra validation checks for speed
  profile_memory: true             # Monitor memory usage
  profile_timing: true             # Profile timing components

# Stage 2 Acceptance Criteria Notes:
# - V_X, V_Y should be positive and scale ~1/B_E, 1/B_U
# - SE_deltaH1 should be finite and reasonable
# - deltaH_true should have similar sign/magnitude to deltaH1 for small steps
# - Memory usage should stay within A100 40GB limits
# - All phases should complete without CUDA OOM