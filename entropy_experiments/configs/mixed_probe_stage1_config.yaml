# Stage 1 Mixed E/U Batch Probe Configuration
# Test configuration for the new mixed batch approach

# Core sampling parameters - separate E and U batches
batch_config:
  B: 16                             # Fallback batch size for compatibility
  B_E: 16                           # Evaluation batch size
  B_U: 16                           # Update batch size  
  G: 8                              # Keep at target value
  rollout_batch_size: 32            # Efficient batch generation
  dataset_name: "gsm8k_r1_template" # Dataset to sample prompts from
  split: "train"                    # Dataset split to use

# Probe configuration (for compatibility with existing code)
probe_config:
  mode: "exact"                     # Legacy mode (not used in Stage 1, but needed for initialization)

# Stage 1 Mixed Probe Configuration
probe_rework:
  mb_size_prompts: 2                # Microbatch size (prompts per microbatch)
  buffers_dtype: "float32"          # Buffer precision (float32/float16)
  weighting_mode: "dr_grpo"         # Weighting mode (dr_grpo/per_token_avg)

# Memory and performance - Conservative for testing
memory_config:
  microbatch_size: 1                # For importance sampling (if needed)
  importance_microbatch_size: 1     # Conservative
  teacher_force_microbatch_size: 2  # G-microbatching for gradients
  amp: true                         # Use automatic mixed precision
  dtype: "bfloat16"                 # Match training configuration

# Variance estimation (for compatibility)
stats_config:
  compute_plugin_se: true           # Plug-in standard error estimate
  compute_jackknife_se: true        # Jackknife standard error estimate

# Importance sampling (for compatibility)
importance_sampling:
  use_snis: true                    # Self-normalized importance sampling
  use_psis: false                   # Skip PSIS for now
  ess_threshold: 0.5                # Effective sample size threshold
  resample_on_low_ess: false        # Don't resample for testing

# Checkpoint paths - using extracted RL training checkpoint
checkpoint:
  checkpoint_path: "/home/ubuntu/localfs/rl_training_runs/training_state/step_latest/model"
  optimizer_path: "/home/ubuntu/localfs/rl_training_runs/training_state/step_latest/optimizer.pt"
  model_config_path: "Qwen/Qwen2.5-1.5B"

# Generation settings - moderate for testing
generation:
  max_new_tokens: 100               # Reduced for faster testing
  temperature: 0.7                  # Match training temperature
  top_p: 1.0                       # Full distribution
  do_sample: true                  # Enable sampling
  pad_token_id: null               # Auto-detect from tokenizer

# Output and logging
output:
  save_results: true               # Save detailed results
  results_path: "mixed_probe_stage1_results.json"
  log_level: "INFO"                # Standard logging
  save_samples: false              # Don't save samples (saves space)

# Distributed settings - single GPU for Stage 1
distributed:
  find_unused_parameters: true     # Required for DDP compatibility
  reduce_dtype: "float32"          # Standard precision for reductions

# Advanced options - minimal for Stage 1 testing
advanced:
  force_recompute: true            # Don't use cached computations
  validation_mode: false           # Skip extra validation checks for speed
  profile_memory: true             # Monitor memory usage
  profile_timing: true             # Profile timing components