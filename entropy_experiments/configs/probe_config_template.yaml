# Offline Entropy Probe Configuration Template
# Based on offline_entropy_probe_strategy.txt

# Core sampling parameters
batch_config:
  B: 32                          # Number of prompts per batch
  G: 8                           # Number of responses per prompt  
  dataset_name: "gsm8k_r1_template"  # Dataset to sample prompts from (must match training)
  split: "train"                 # Dataset split to use
  
# Probe computation mode
probe_config:
  mode: "blocks"                 # "exact" for per-prompt, "blocks" for memory-efficient
  M: 8                          # Number of blocks (if mode="blocks") 
  pairs_per_block: "all"        # "all" for complete U-statistic, or integer for incomplete
  
# Memory and performance
memory_config:
  microbatch_size: 4            # Prompts per microbatch (memory constraint)
  amp: true                     # Use automatic mixed precision (match training)
  dtype: "bfloat16"             # AMP dtype, should match training
  
# Variance estimation
stats_config:
  compute_plugin_se: true       # Compute plug-in standard error estimate
  compute_jackknife_se: true    # Compute jackknife standard error estimate
  
# Importance sampling for actual Î”H  
importance_sampling:
  use_snis: true                # Use self-normalized importance sampling
  use_psis: false               # Use Pareto-smoothed importance sampling (optional)
  ess_threshold: 0.5            # Effective sample size threshold (resample if below)
  resample_on_low_ess: true     # Resample fresh responses if ESS too low
  
# Distributed settings
distributed:
  find_unused_parameters: true  # Required for DDP with unused parameters
  reduce_dtype: "float32"       # Dtype for all-reduce operations
  
# Checkpoint and model config
checkpoint:
  checkpoint_path: ""           # Path to model checkpoint (.pt file)
  optimizer_path: ""            # Path to optimizer state (if separate from checkpoint)
  model_config_path: ""         # Path to model configuration
  
# Generation settings (should match training)
generation:
  max_new_tokens: 200           # Maximum response length
  temperature: 0.7              # Sampling temperature
  top_p: 1.0                   # Nucleus sampling parameter
  do_sample: true              # Enable sampling
  pad_token_id: null           # Will be auto-detected from tokenizer
  
# Output and logging
output:
  save_results: true           # Save detailed results to file
  results_path: ""             # Path for results (auto-generated if empty)
  log_level: "INFO"            # Logging level
  save_samples: false          # Save the actual prompt/response samples (large files)
  
# Advanced options
advanced:
  cross_fitting: false         # Split G responses for X vs Y (reduces variance)
  force_recompute: false       # Don't use any cached computations
  validation_mode: false       # Extra checks and logging for validation
  profile_memory: false        # Profile memory usage (debugging)
  profile_timing: true         # Profile timing of different components