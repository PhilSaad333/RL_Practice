Summary
Route E and U sampling in offline_entropy_probe.py through SequenceProcessor, behind a config toggle, while keeping ProbeComponents compute unchanged. Preserve LoRA/QLoRA loading from Stage 1. Target single‑GPU in this stage; keep the surface DDP‑ready by isolating sampling and clearly separating adapter code.

Rationale
- Remove duplicate sampling logic in offline_entropy_probe.py and ProbeComponents.
- Ensure consistency by using the centralized SequenceProcessor for prompt sampling, generation, logprobs, RB entropies, and rewards.
- Keep a fallback to legacy sampling for parity checks and quick rollback.

Config (additive, backward‑compatible)
- probe_rework.use_sequence_processor_sampling: bool (default true)
  - When true, E/U sampling uses SequenceProcessor.

Minimal Diffs (copy‑pastable)

1) offline_entropy_probe.py: add SequenceProcessor imports (top of file)
Search:
    import torch
Add below:
    from sequence_processing.sequence_processor import (
        SequenceProcessor, GenerationConfig, BatchedSequences,
    )

2) offline_entropy_probe.py: add SequenceProcessor setup and adapters inside OfflineEntropyProbe
Insert these methods after _initialize_components() and its DDP block (same indentation as other methods in the class):

"""
    # --- SequenceProcessor setup and sampling helpers ---
    def _ensure_sequence_processor(self):
        if hasattr(self, "_sequence_processor") and self._sequence_processor is not None:
            return
        from transformers import AutoTokenizer
        backbone = self.config['checkpoint'].get('model_config_path', 'Qwen/Qwen2.5-1.5B')
        tok = AutoTokenizer.from_pretrained(backbone, trust_remote_code=True)
        tok.padding_side = "left"
        tok.pad_token = tok.eos_token

        gen_cfg = self.config.get('generation', {})
        sp_cfg = GenerationConfig(
            temperature=gen_cfg.get('temperature', 1.0),
            top_p=gen_cfg.get('top_p', 1.0),
            max_new_tokens=gen_cfg.get('max_new_tokens', 256),
            do_sample=True,
            num_return_sequences=self.config['batch_config']['G'],
            gen_batch_size=gen_cfg.get('gen_batch_size', 8),
            tf_batch_size=gen_cfg.get('tf_batch_size', 64),
            rb_requires_grad=gen_cfg.get('rb_requires_grad', False),
        )
        self._sequence_processor = SequenceProcessor(self.model, tok, sp_cfg)

    def _pack_E_from_sequences(self, seqs: BatchedSequences) -> dict:
        import torch as _torch
        B, G = seqs.sequences.shape[:2]
        assert G == 1, f"E-batch expected G=1, got G={G}"
        max_lengths = [max(lens) if len(lens) > 0 else 1 for lens in seqs.gen_lens]
        advantages = _torch.zeros((B, G), dtype=_torch.float32, device=seqs.sequences.device)
        return {
            'sequences': seqs.sequences,
            'attention_masks': seqs.attention_masks,
            'prompt_lens': seqs.prompt_lens,
            'advantages': advantages,
            'max_lengths': max_lengths,
            'num_prompts': B,
            'num_responses_per_prompt': 1,
        }

    def _pack_U_from_sequences(self, seqs: BatchedSequences, rewards: list[list[float]]) -> dict:
        import torch as _torch
        B, G = seqs.sequences.shape[:2]
        advantages = _torch.tensor(rewards, dtype=_torch.float32, device=seqs.sequences.device)
        advantages = advantages - advantages.mean(dim=1, keepdim=True)
        max_lengths = [max(lens) if len(lens) > 0 else 1 for lens in seqs.gen_lens]
        return {
            'sequences': seqs.sequences,
            'attention_masks': seqs.attention_masks,
            'prompt_lens': seqs.prompt_lens,
            'advantages': advantages,
            'max_lengths': max_lengths,
            'num_prompts': B,
            'num_responses_per_prompt': G,
        }

    def _sample_EU_via_sequence_processor(self, *, B_E: int, B_U: int, G_U: int) -> tuple[dict, dict]:
        self._ensure_sequence_processor()
        ds_name = self.config['batch_config']['dataset_name']
        split = self.config['batch_config']['split']

        # E: with replacement, G=1, compute RB for X if needed later
        E_sequences, _E_lp, _E_diag = self._sequence_processor.generate_with_replacement_sampling(
            total_sequences=B_E,
            dataset_name=ds_name,
            split=split,
            G=1,
            compute_rb=True,
        )
        E_batch = self._pack_E_from_sequences(E_sequences)

        # U: distinct prompts, G responses each (SequenceProcessor handles reward computation)
        U_sequences, U_lp, _U_diag = self._sequence_processor.generate_with_logprobs(
            prompts=None,
            G=G_U,
            dataset_name=ds_name,
            split=split,
            num_prompts=B_U,
            compute_rb=False,
            with_grad=False,
        )
        U_batch = self._pack_U_from_sequences(U_sequences, U_lp.rewards)
        return E_batch, U_batch
"""

3) offline_entropy_probe.py: switch run_mixed_probe Phase 0 to use SP path (with fallback)
Locate the Phase 0 sampling block under:
    self.logger.info("Phase 0: Sampling E and U batches")
Replace the block that builds E_batch and U_batch with:

"""
            G_U = self.config['batch_config']['G']
            if self.config.get('probe_rework', {}).get('use_sequence_processor_sampling', True):
                E_batch, U_batch = self._sample_EU_via_sequence_processor(
                    B_E=B_E, B_U=B_U, G_U=G_U
                )
            else:
                # Legacy fallback
                if is_dist:
                    E_total_sequences = B_E_local * 1
                    E_batch = self.probe_components.sample_E_batch_with_replacement(
                        E_total_sequences=E_total_sequences, G=1
                    )
                    U_batch = self.probe_components.sample_batch(
                        B=B_U_local, G=G_U, indices=U_indices_local
                    )
                else:
                    E_batch = self.probe_components.sample_E_batch_with_replacement(
                        E_total_sequences=B_E * 1, G=1
                    )
                    U_batch = self.probe_components.sample_batch(
                        B=B_U, G=G_U, indices=None
                    )
"""

Notes
- Keep all downstream compute intact (accumulate_sum_X/Y, dot, deltaH1).
- This change is single‑GPU oriented; DDP will pass explicit prompt indices in Stage 3.

Implementation Steps
1) Add the imports (Step 1) and commit.
2) Add the new helper methods into OfflineEntropyProbe (Step 2) and commit.
3) Replace Phase 0 sampling in run_mixed_probe (Step 3) and commit.
4) Ensure config contains probe_rework.use_sequence_processor_sampling=true (default if key missing).

Validation
- Parity check vs legacy sampling:
  - Run once with SP sampling (default true):
    set probe_rework.use_sequence_processor_sampling=true
    python -m entropy_experiments.offline_entropy_probe --config path/to/config.yaml
  - Run again with legacy sampling:
    set probe_rework.use_sequence_processor_sampling=false
    python -m entropy_experiments.offline_entropy_probe --config path/to/config.yaml
  - Expect deltaH1 within tolerance across 2–3 seeds; E uses with‑replacement (G=1), U uses distinct prompts with G responses.

Acceptance Criteria
- deltaH1 matches (within tolerance) legacy path on the same checkpoint and seeds.
- Logs report E batch with G=1 and U batch with configured G, and counts match requested B_E and B_U.
- No changes to ProbeComponents compute path; only sampling moved.

Rollback
- Flip probe_rework.use_sequence_processor_sampling=false to use legacy sampling immediately (no code revert necessary).

Forward‑Looking (DDP in Stage 3)
- Add an “index plan” broadcast for E/U indices from rank 0 and pass explicit prompts to SequenceProcessor to ensure identical sampling across ranks.
