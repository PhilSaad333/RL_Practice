# qwen2_5_15.yaml - Optimized for 2x H100 80GB GPUs (160GB total)
# ================================================================

# Model configuration
backbone: Qwen/Qwen2.5-1.5B
eval_backbone: qwen2_5_15

# -------------- Rollout Collection ---------------- #

# Batch sizes for generation
rollout_batch_size: 24    # Samples generated per collection round
num_generations: 8        # Number of completions per prompt

# VLLM disabled due to dependency conflicts - using standard rollout
use_vllm: false
vllm_reload_every: 5
vllm_gpu_memory_utilization: 0.25

# Teacher-forcing configuration
tf_micro_batch: 8         # Batch size for logprob recomputation

# Data saving
save_rollouts_every: 0    # 0 = never save rollouts to disk

# -------------- Training Configuration ---------------- #

# Buffer and batch sizes (optimized for ~90% GPU utilization on 2x H100)
buffer_size: 128          # Total samples in training buffer
microbatch_size: 2        # Samples per gradient step
# grad_accum_steps = buffer_size / (world_size * microbatch_size) = 128 / (2 * 2) = 32

# PPO configuration
ppo_epochs: 1             # Training epochs per rollout buffer
max_new_tokens: 200       # Max generation length (suitable for GSM8K)
temperature: 0.7          # Sampling temperature 

# Evaluation configuration
eval_frac: 0.01           # Fraction of validation set to evaluate
eval_batch_size: 64       # Batch size for evaluation
eval_temperature: 0.7     # Temperature for evaluation sampling

# Reward configuration
reward_var_thresh: 0.01   # Variance threshold for reward filtering
reject_all_zero: true     # Reject samples where all rewards are zero
reject_all_max: true      # Reject samples where all rewards are maximum

reward_fns:
  - tag_pref              # Primary reward function

# Scheduler configuration
scheduler:
  name: mix_passrate
  dataset_name: gsm8k_r1_template
  split: "train"
  ema_alpha: 0.01         # EMA smoothing factor
  boundaries: [0.2, 0.8]  # Difficulty boundaries
  sample_weights: [1, 1, 1]

# -------------- GRPO Trainer Configuration ---------------- #

# Learning rate and optimization
lr: 1.0e-6                # Learning rate
clip_eps: 0.2             # PPO clipping epsilon
clip_+: 0.2               # Asymmetric clipping parameter
cfg["kl_beta"]: 0.0       # KL penalty (0 = compute for logging only)
ent_coef: 0.0             # Entropy coefficient
grad_clip: 1.0            # Gradient clipping norm

# Training settings
bf16: true                # Use bfloat16 precision
gradient_checkpointing: true
lr_scheduler: "const"     # Constant learning rate

# Checkpoint configuration
save_every: 8             # Save checkpoint every N steps
total_steps: 64           # Total training steps

# -------------- Diagnostic Probes ---------------- #

# Entropy probe (currently disabled)
entropy_probe:
  every: 0                # 0 = disabled
  sketch_r: 0
  out_dir: entropy_probes

# Gradient Noise Scale probe
gns_probe:
  enabled: true           # Track gradient noise for batch size optimization
