# testconfig.yaml   –  starter settings for smoke-tests
# ----------------------------------------------------
# Anything in this file is passed verbatim to RolloutCollector via cfg.
# The GRPO trainer will later read the same file and use only what it needs.



# rl_configs/rl_qwen2.yaml
backbone: Qwen/Qwen2-0.5B
eval_backbone: qwen2
lora_ckpt: /content/drive/.../qwen2_gsm8k_lora      # << your SFT output dir


# -------------- rollout collector ---------------- #
rollout_batch_size: 8           # number of prompts per batch in rollout collection
rollout_entropy_mode: "full"    # "full", "lite" or "none"
tf_micro_batch: 2               # micro‑batch size for teacher forcing when lite/none
optimizer_entropy: "full"       # "full" or "none"


num_generations   : 4 #8   # G: number of generations per prompt

buffer_size       : 8 #2048   # total number of prompts in buffer (multiple of microbatch_size*grad_accum_steps)
microbatch_size   : 1   # B: prompts used for each gradient checkpointing steps
grad_accum_steps  : 8 #8   # ga_steps: total number of prompts per update batch = ga_step*B

# Above gives effective batch size 256 prompts, 8 gens per prompt, 8 steps per epoch

ppo_epochs        : 1 #3   # K: number of updates per rollout-collection step.
max_new_tokens    : 128
temperature       : 0.7
#top_p             : 0.95 #remove 

eval_every: 16        # ← run eval every 24 optimiser steps - after each rollout batch
eval_frac: 1.0        
eval_batch_size: 128
eval_temperature: 0.7


reward_var_thresh : 0.01
reject_all_zero   : true
reject_all_max    : true

reward_fns:
  - tag_pref
  #- tag_math_correct

scheduler:
  name          : mix_passrate
  dataset_name  : gsm8k_r1_template
  split         : "train"
  ema_alpha     : 0.01
  boundaries    : [0.2, 0.8]
  sample_weights: [1, 1, 1]

# ------------------ trainer (GRPO) --------------- #
lr               : 1.0e-6
clip_eps         : 0.2
clip_+           : 0.2 #asymmetric clipping

cfg["kl_beta"]   : 0.0 # If zero only computes KL with no grad for logging

ent_coef         : 0.0            # future entropy bonus knob
grad_clip        : 1.0
bf16             : true

gradient_checkpointing : true

lr_scheduler: "const" # 'const' or 'cosine'

save_every: 32
total_steps: 32

