# grpo_gsm8k_qwen2.yaml   –  starter settings for smoke-tests
# ----------------------------------------------------
# Anything in this file is passed verbatim to RolloutCollector via cfg.
# The GRPO trainer will later read the same file and use only what it needs.



# rl_configs/rl_qwen2.yaml
backbone: Qwen/Qwen2-0.5B
eval_backbone: qwen2
lora_ckpt: /content/drive/.../qwen2_gsm8k_lora      # << your SFT output dir


# -------------- rollout collector ---------------- #
rollout_batch_size: 12 # number of prompts per batch in rollout collection
num_generations   : 6 #8   # G: number of generations per prompt
buffer_size       : 32 #2048   # total number of prompts in buffer (multiple of microbatch_size*grad_accum_steps)
microbatch_size   : 2   # B: prompts used for each gradient checkpointing steps
grad_accum_steps  : 2 #8   # ga_steps: total number of prompts per update batch = ga_step*B
ppo_epochs        : 2 #3   # K: number of updates per rollout-collection step.
max_new_tokens    : 128
temperature       : 0.7
#top_p             : 0.95 #remove 

eval_every: 8        # ← run eval every 48 optimiser steps
eval_frac: 0.05
eval_cfg:             # (reserved for future – temperature, top-p, etc.)
  temperature: 0.7
  top_p: 1.0
  batch_size: 12
  subset_frac: 0.3


reward_var_thresh : 0.01
reject_all_zero   : true
reject_all_max    : true

reward_fns:
  - tag_pref
  #- tag_math_correct

scheduler:
  name          : mix_passrate
  dataset_name  : gsm8k_latex
  split         : "train"
  ema_alpha     : 0.05
  boundaries    : [0.2, 0.8]
  sample_weights: [4, 3, 1]

# ------------------ trainer (GRPO) --------------- #
lr               : 2.0e-4
clip_eps         : 0.2
clip_+           : 0.5 #asymmetric clipping
kl_beta          : 0.05
kl_target        : 0.1
max_kl           : 0.15
ent_coef         : 0.0            # future entropy bonus knob
grad_clip        : 1.0
bf16             : true

gradient_checkpointing : true

save_every: 5
total_steps: 10

